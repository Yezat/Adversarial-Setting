{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerics of Empirical Risk Minimisation - Adversarial Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import special\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('default')\n",
    "from matplotlib import rc, rcParams\n",
    "rcParams['font.size'] = 20\n",
    "import mpmath\n",
    "from data_model import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could we just use mpmath?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average difference between logsig_mpmath and logsig_naive: 2.0989291012813905e-17\n",
      "Standard deviation of difference between logsig_mpmath and logsig_naive: 1.4792252545044533e-16\n"
     ]
    }
   ],
   "source": [
    "def logsig_mpmath(x):\n",
    "  if x > 0:\n",
    "    return mpmath.log(1 / (1 + mpmath.exp(-x)))\n",
    "  else:\n",
    "    return x - mpmath.log(mpmath.exp(x) + 1)\n",
    "  \n",
    "def logsig_naive(t):\n",
    "    return mpmath.log(1 / (1 + mpmath.exp(-t)))\n",
    "\n",
    "n = 12\n",
    "nsteps = 100\n",
    "exps = np.concatenate([np.linspace(n, 0, nsteps), [0], np.linspace(0, n, nsteps)])\n",
    "xs = 2 ** exps\n",
    "xs[:nsteps] *= -1\n",
    "xs[nsteps] = 0\n",
    "\n",
    "# compute logsig using logsig_mpmath and logsig_naive\n",
    "logsig_mpmath_xs = np.zeros(len(xs))\n",
    "logsig_naive_xs = np.zeros(len(xs))\n",
    "for i, x in enumerate(xs):\n",
    "    logsig_mpmath_xs[i] = logsig_mpmath(x)\n",
    "    logsig_naive_xs[i] = logsig_naive(x)\n",
    "    # convert both to numpy float64\n",
    "    logsig_mpmath_xs[i] = np.float64(logsig_mpmath_xs[i])\n",
    "    logsig_naive_xs[i] = np.float64(logsig_naive_xs[i])\n",
    "    \n",
    "# compute the average difference between the two\n",
    "diff = np.abs(logsig_mpmath_xs - logsig_naive_xs)\n",
    "avg_diff = np.mean(diff)\n",
    "std_diff = np.std(diff)\n",
    "print(\"Average difference between logsig_mpmath and logsig_naive: {}\".format(avg_diff))\n",
    "print(\"Standard deviation of difference between logsig_mpmath and logsig_naive: {}\".format(std_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, it is possible to just use mpmath. However, at the moment it is unclear to me how well matrix operations are supported.\n",
    "Hence, we might depend on slow for-loops and moving to cython and doing mpmath there might fail as it still requires the GIL. So it is slow.\n",
    "This means that me might be only able to resort to mpmath in order to check out cython code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Cython Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cython_eval(z,e,y):\n",
    "\n",
    "    half = skloss.CyHalfBinomialLoss()\n",
    "    loss_out = np.empty_like(y)\n",
    "    gradient_out = np.empty_like(z)\n",
    "    epsilon_gradient_out = np.empty_like(z)\n",
    "    half.loss_gradient( y_true=y,\n",
    "        raw_prediction=z,    \n",
    "        adversarial_norm = e,\n",
    "        loss_out=loss_out,\n",
    "        gradient_out=gradient_out,\n",
    "        epsilon_gradient_out = epsilon_gradient_out,\n",
    "        n_threads=1,\n",
    "    )    \n",
    "\n",
    "    return loss_out, gradient_out, epsilon_gradient_out\n",
    "\n",
    "def mp_cython_eval(z,e,y):\n",
    "    loss_out = np.empty_like(y)\n",
    "    gradient_out = np.empty_like(z)\n",
    "    epsilon_gradient_out = np.empty_like(z)\n",
    "\n",
    "    skloss.mp_loss_gradient(y_true=y,\n",
    "        raw_prediction=z,    \n",
    "        adversarial_norm = e,\n",
    "        loss_out=loss_out,\n",
    "        gradient_out=gradient_out,\n",
    "        epsilon_gradient_out = epsilon_gradient_out\n",
    "    )\n",
    "\n",
    "    return loss_out, gradient_out, epsilon_gradient_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "$\\mathcal{L}=\\sum_{\\mu=1}^N-y_\\mu \\frac{X_\\mu^T \\boldsymbol{w}}{\\sqrt{d}}+y_\\mu \\frac{\\varepsilon_t \\boldsymbol{w}^T \\boldsymbol{\\Sigma}_\\delta \\boldsymbol{w}}{\\sqrt{d} \\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}}+\\left(1-y_\\mu\\right) \\log \\left(1+\\exp \\left(\\frac{X_\\mu^T \\boldsymbol{w}}{\\sqrt{d}}+\\frac{\\varepsilon_t \\boldsymbol{w}^T \\boldsymbol{\\Sigma}_\\delta \\boldsymbol{w}}{\\sqrt{d} \\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}}\\right)\\right)+y_\\mu \\log \\left(1+\\exp \\left(\\frac{X_\\mu^T \\boldsymbol{w}}{\\sqrt{d}}-\\frac{\\varepsilon_t \\boldsymbol{w}^T \\boldsymbol{\\Sigma}_\\delta \\boldsymbol{w}}{\\sqrt{d} \\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}}\\right)\\right)$\n",
    "\n",
    "Let $z_\\mu = \\frac{X_\\mu^T \\boldsymbol{w}}{\\sqrt{d}}$\n",
    "Let $e = \\frac{\\varepsilon_t \\boldsymbol{w}^T \\boldsymbol{\\Sigma}_\\delta \\boldsymbol{w}}{\\sqrt{d} \\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}}$\n",
    "\n",
    "Then\n",
    "$\\mathcal{L}=\\sum_{\\mu=1}^N-y_\\mu z_\\mu+y_\\mu e+\\left(1-y_\\mu\\right) \\log \\left(1+\\exp \\left(z_\\mu+e\\right)\\right)+y_\\mu \\log \\left(1+\\exp \\left(z_\\mu-e\\right)\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments of interest\n",
    "n = 12\n",
    "nsteps = 50\n",
    "exps = np.concatenate([np.linspace(n, 0, nsteps), [0], np.linspace(0, n, nsteps)])\n",
    "zs = 2 ** exps\n",
    "zs[:nsteps] *= -1\n",
    "zs[nsteps] = 0\n",
    "ys = np.array([0,1])\n",
    "es = zs.copy()\n",
    "\n",
    "# Create an array with all possible arguments z,e,y\n",
    "Z, E, Y = np.meshgrid(zs, es, ys)\n",
    "Z = Z.flatten()\n",
    "E = E.flatten()\n",
    "Y = Y.flatten()\n",
    "\n",
    "Y = Y.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the mpmath precision\n",
    "# mpmath.mp.dps = 10000 \n",
    "\n",
    "# The mpmath loss function for the baseline\n",
    "def mp_loss(z,e,y):\n",
    "    return -y*z + y*e + (1-y)*mpmath.log(1+mpmath.exp(z+e)) + y*mpmath.log(1+mpmath.exp(z-e))\n",
    "\n",
    "# our cython code\n",
    "def cython_loss(z,e,y):\n",
    "    loss_out, _, _ = cython_eval(z,e,y)\n",
    "    return loss_out\n",
    "\n",
    "def mp_cython_loss(z,e,y):\n",
    "    loss_out, _, _ = mp_cython_eval(z,e,y)\n",
    "    return loss_out\n",
    "\n",
    "\n",
    "def log1pexp(x):\n",
    "    out = np.zeros_like(x)\n",
    "    idx0 = x <= -37\n",
    "    out[idx0] = np.exp(x[idx0])\n",
    "    idx1 = (x > -37) & (x <= -2)\n",
    "    out[idx1] = np.log1p(np.exp(x[idx1]))\n",
    "    idx2 = (x > -2) & (x <= 18)\n",
    "    out[idx2] = np.log(1. + np.exp(x[idx2]))\n",
    "    idx3 = (x > 18) & (x <= 33.3)\n",
    "    out[idx3] = x[idx3] + np.exp(-x[idx3])\n",
    "    idx4 = x > 33.3\n",
    "    out[idx4] = x[idx4]\n",
    "    return out\n",
    "\n",
    "\n",
    "def stable_loss(z,e,y):\n",
    "    return -y*z + y*e + (1-y)*log1pexp(z+e) + y*log1pexp(z-e)\n",
    "\n",
    "\n",
    "def naive_loss(z,e,y):\n",
    "    return -y*z + y*e + (1-y)*np.log(1+np.exp(z+e)) + y*np.log(1+np.exp(z-e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mp = np.array([mp_loss(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "loss_mp = np.array([np.float64(x) for x in loss_mp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_cython_mp = np.array([mp_cython_loss(np.array([z]),e,np.array([y])) for z,e,y in zip(Z,E,Y)])\n",
    "# loss_cython_mp = loss_cython_mp.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_cython = np.array([cython_loss(np.array([z]),e,np.array([y])) for z,e,y in zip(Z,E,Y)])\n",
    "# loss_cython = loss_cython.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_stable = np.array([stable_loss(z,e,y) for z,e,y in zip(Z,E,Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/9b_m1n3130xfbjd06qfyyqgc0000gn/T/ipykernel_29327/3876734477.py:38: RuntimeWarning: overflow encountered in exp\n",
      "  return -y*z + y*e + (1-y)*np.log(1+np.exp(z+e)) + y*np.log(1+np.exp(z-e))\n",
      "/var/folders/4p/9b_m1n3130xfbjd06qfyyqgc0000gn/T/ipykernel_29327/3876734477.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -y*z + y*e + (1-y)*np.log(1+np.exp(z+e)) + y*np.log(1+np.exp(z-e))\n"
     ]
    }
   ],
   "source": [
    "loss_naive = np.array([naive_loss(z,e,y) for z,e,y in zip(Z,E,Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the absolute and relative errors\n",
    "# abs_error = np.abs(loss_mp - loss_cython)\n",
    "\n",
    "\n",
    "# # Compute the average and standard deviation of the relative error\n",
    "# avg_abs_error = np.mean(abs_error)\n",
    "# std_abs_error = np.std(abs_error)\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Comparison between MP and Cython\")\n",
    "# print(\"Average absolute error: %.2e\" % avg_abs_error)\n",
    "# print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the absolute and relative errors\n",
    "# abs_error = np.abs(loss_mp - loss_cython_mp)\n",
    "\n",
    "\n",
    "\n",
    "# # Compute the average and standard deviation of the relative error\n",
    "# avg_abs_error = np.mean(abs_error)\n",
    "# std_abs_error = np.std(abs_error)\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Comparison between MP and Cython MP\")\n",
    "# print(\"Average absolute error: %.2e\" % avg_abs_error)\n",
    "# print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between MP and Stable\n",
      "Average absolute error: 4.99e-18\n",
      "Standard deviation of the absolute error: 7.99e-17\n"
     ]
    }
   ],
   "source": [
    "# Compute the absolute and relative errors\n",
    "abs_error = np.abs(loss_mp - loss_stable)\n",
    "\n",
    "\n",
    "# Compute the average and standard deviation of the relative error\n",
    "avg_abs_error = np.mean(abs_error)\n",
    "std_abs_error = np.std(abs_error)\n",
    "\n",
    "# Print the results\n",
    "print(\"Comparison between MP and Stable\")\n",
    "print(\"Average absolute error: %.2e\" % avg_abs_error)\n",
    "print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between MP and Naive\n",
      "Average absolute error: nan\n",
      "Standard deviation of the absolute error: nan\n"
     ]
    }
   ],
   "source": [
    "# Compute the absolute errors\n",
    "abs_error = np.abs(loss_mp - loss_naive)\n",
    "\n",
    "# Compute the average and standard deviation of the relative error\n",
    "avg_abs_error = np.mean(abs_error)\n",
    "std_abs_error = np.std(abs_error)\n",
    "\n",
    "# Print the results\n",
    "print(\"Comparison between MP and Naive\")\n",
    "print(\"Average absolute error: %.2e\" % avg_abs_error)\n",
    "print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/9b_m1n3130xfbjd06qfyyqgc0000gn/T/ipykernel_29327/2602079403.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arg_zip = np.array([[np.array([z]),e,np.array([y])] for z,e,y in zip(Z,E,Y)])\n"
     ]
    }
   ],
   "source": [
    "arg_zip = np.array([[np.array([z]),e,np.array([y])] for z,e,y in zip(Z,E,Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([-3456.51396913]), -4096.0, array([0.])], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_zip[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cython_loss(*arg_zip[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 6.93147181e-01, 0.00000000e+00, ...,\n",
       "       6.39486031e+02, 8.19200000e+03, 6.93147181e-01])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_cython_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 6.93147181e-01, 0.00000000e+00, ...,\n",
       "       6.39486031e+02, 8.19200000e+03, 6.93147181e-01])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} \\partial_w \\mathcal{L} & =\\sum_{\\mu=1}^N-y_\\mu \\frac{X_\\mu}{\\sqrt{d}}+y_\\mu H+\\frac{\\left(1-y_\\mu\\right)}{1+\\exp (-C)}\\left(\\frac{X_\\mu}{\\sqrt{d}}+H\\right)+\\frac{y_\\mu}{1+\\exp (-\\bar{C})}\\left(\\frac{X_\\mu}{\\sqrt{d}}-H\\right) \\\\ & =\\sum_{\\mu=1}^N\\left[H \\cdot\\left(\\frac{\\left(1-y_\\mu\\right)}{1+\\exp (-C)}+\\frac{y_\\mu \\exp (-\\bar{C})}{1+\\exp (-\\bar{C})}\\right)+\\frac{X_\\mu}{\\sqrt{d}} \\cdot\\left(\\frac{\\left(1-y_\\mu\\right)}{1+\\exp (-C)}-\\frac{y_\\mu \\exp (-\\bar{C})}{1+\\exp (-\\bar{C})}\\right)\\right]\\end{aligned}$\n",
    "\n",
    "We are only interested in evaluating the function that will be multiplied with the data $X_\\mu$ and the derivative of the optimal attack $H$.\n",
    "\n",
    "Note the definitions\n",
    "$C=\\frac{X_\\mu^T \\boldsymbol{w}}{\\sqrt{d}}+\\frac{\\varepsilon_t \\boldsymbol{w}^T \\boldsymbol{\\Sigma}_\\delta \\boldsymbol{w}}{\\sqrt{d} \\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}}$ and $\\bar{C}=\\frac{X_\\mu^T \\boldsymbol{w}}{\\sqrt{d}}-\\frac{\\varepsilon_t \\boldsymbol{w}^T \\boldsymbol{\\Sigma}_\\delta \\boldsymbol{w}}{\\sqrt{d} \\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing on Sigmoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the gradient expression, it looks like it should be sufficient to compute the sigmoids accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average difference between naive sigmoid and mpmath sigmoid: 1.24186408533529e-18\n",
      "Standard deviation of difference between naive sigmoid and mpmath sigmoid: 6.01214443155874e-18\n",
      "Average difference between mpmath sigmoid and stable sigmoid: 1.90530007013938e-334\n",
      "Standard deviation of difference between mpmath sigmoid and stable sigmoid: 2.69450119958153e-333\n",
      "Naive sigmoid has 0 infs and 0 nans\n",
      "Stable sigmoid has 0 infs and 0 nans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/9b_m1n3130xfbjd06qfyyqgc0000gn/T/ipykernel_29327/475322496.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "def naive_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def stable_sigmoid(x):\n",
    "    out = np.zeros_like(x)\n",
    "    idx = x <= 0\n",
    "    out[idx] = np.exp(x[idx]) / (1 + np.exp(x[idx]))\n",
    "    idx = x > 0\n",
    "    out[idx] = 1 / (1 + np.exp(-x[idx]))\n",
    "    return out\n",
    "\n",
    "def mpmath_sigmoid(x):\n",
    "    # return 1 / (1 + mpmath.exp(-x))\n",
    "    if x <= 0:\n",
    "        return mpmath.exp(x) / (1 + mpmath.exp(x))\n",
    "    else:\n",
    "        return 1 / (1 + mpmath.exp(-x))\n",
    "\n",
    "naives = np.array([naive_sigmoid(x) for x in xs])\n",
    "mpmaths = np.array([mpmath_sigmoid(x) for x in xs])\n",
    "stables = np.array([stable_sigmoid(x) for x in xs])\n",
    "\n",
    "# compute the abs difference between the two\n",
    "diff = np.abs(naives - mpmaths)\n",
    "avg_diff = np.mean(diff)\n",
    "std_diff = np.std(diff)\n",
    "print(\"Average difference between naive sigmoid and mpmath sigmoid: {}\".format(avg_diff))\n",
    "print(\"Standard deviation of difference between naive sigmoid and mpmath sigmoid: {}\".format(std_diff))\n",
    "\n",
    "# compute the abs difference between the two\n",
    "diff = np.abs(mpmaths - stables)\n",
    "avg_diff = np.mean(diff)\n",
    "std_diff = np.std(diff)\n",
    "print(\"Average difference between mpmath sigmoid and stable sigmoid: {}\".format(avg_diff))\n",
    "print(\"Standard deviation of difference between mpmath sigmoid and stable sigmoid: {}\".format(std_diff))\n",
    "\n",
    "# count infs and nans in the naive sigmoid\n",
    "naive_infs = np.isinf(naives)\n",
    "naive_nans = np.isnan(naives)\n",
    "naive_inf_count = np.sum(naive_infs)\n",
    "naive_nan_count = np.sum(naive_nans)\n",
    "print(\"Naive sigmoid has {} infs and {} nans\".format(naive_inf_count, naive_nan_count))\n",
    "\n",
    "# count infs and nans in the stable sigmoid\n",
    "stable_infs = np.isinf(stables)\n",
    "stable_nans = np.isnan(stables)\n",
    "stable_inf_count = np.sum(stable_infs)\n",
    "stable_nan_count = np.sum(stable_nans)\n",
    "print(\"Stable sigmoid has {} infs and {} nans\".format(stable_inf_count, stable_nan_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the gradient per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mp_gradient(z,e,y):\n",
    "    opt_attack_term = (1-y)*mpmath_sigmoid(z+e) + y*mpmath_sigmoid(-z+e)\n",
    "    data_term = (1-y)*mpmath_sigmoid(z+e) - y*mpmath_sigmoid(-z+e)\n",
    "    return opt_attack_term, data_term\n",
    "\n",
    "\n",
    "def cython_gradient(z,e,y):\n",
    "    _, gradient_out, attack_term = cython_eval(z,e,y)\n",
    "    return attack_term, gradient_out\n",
    "\n",
    "def mp_cython_gradient(z,e,y):\n",
    "    _, gradient_out, attack_term = mp_cython_eval(z,e,y)\n",
    "    return attack_term, gradient_out\n",
    "\n",
    "def stable_gradient(z,e,y):\n",
    "    opt_attack_term = (1-y)*stable_sigmoid(z+e) + y*stable_sigmoid(-z+e)\n",
    "    data_term = (1-y)*stable_sigmoid(z+e) - y*stable_sigmoid(-z+e)\n",
    "    return opt_attack_term, data_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_mp = np.array([mp_gradient(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "gradient_mp = np.array([np.float64(x) for x in gradient_mp])\n",
    "gradient_mp = gradient_mp.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_cython = np.array([cython_gradient(np.array([z]),e,np.array([y])) for z,e,y in zip(Z,E,Y)])\n",
    "# gradient_cython = gradient_cython.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_gradient_cython = np.array([mp_cython_gradient(np.array([z]),e,np.array([y])) for z,e,y in zip(Z,E,Y)])\n",
    "# mp_gradient_cython = mp_gradient_cython.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_stable = np.array([stable_gradient(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "gradient_stable = gradient_stable.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between MP and Cython\n",
      "Average absolute error: nan\n",
      "Standard deviation of the absolute error: nan\n",
      "Comparison between MP and Stable\n",
      "Average absolute error: 6.64e-25\n",
      "Standard deviation of the absolute error: 9.49e-23\n"
     ]
    }
   ],
   "source": [
    "# Compute the absolute and errors between gradient_mp and all other gradients\n",
    "# abs_error = np.abs(gradient_mp - gradient_cython)\n",
    "# abs_error_mp = np.abs(gradient_mp - mp_gradient_cython)\n",
    "abs_error_stable = np.abs(gradient_mp - gradient_stable)\n",
    "\n",
    "# Compute the average and standard deviation of the relative error\n",
    "avg_abs_error = np.mean(abs_error)\n",
    "std_abs_error = np.std(abs_error)\n",
    "\n",
    "# avg_abs_error_mp = np.mean(abs_error_mp)\n",
    "# std_abs_error_mp = np.std(abs_error_mp)\n",
    "\n",
    "avg_abs_error_stable = np.mean(abs_error_stable)\n",
    "std_abs_error_stable = np.std(abs_error_stable)\n",
    "\n",
    "# Print the results\n",
    "print(\"Comparison between MP and Cython\")\n",
    "print(\"Average absolute error: %.2e\" % avg_abs_error)\n",
    "print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error)\n",
    "\n",
    "# print(\"Comparison between MP and Cython MP\")\n",
    "# print(\"Average absolute error: %.2e\" % avg_abs_error_mp)\n",
    "# print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error_mp)\n",
    "\n",
    "print(\"Comparison between MP and Stable\")\n",
    "print(\"Average absolute error: %.2e\" % avg_abs_error_stable)\n",
    "print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of nan and inf values in the gradient_cython\n",
    "# cython_infs = np.isinf(gradient_cython)\n",
    "# cython_nans = np.isnan(gradient_cython)\n",
    "# cython_inf_count = np.sum(cython_infs)\n",
    "# cython_nan_count = np.sum(cython_nans)\n",
    "# print(\"Cython gradient has {} infs and {} nans\".format(cython_inf_count, cython_nan_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\partial_{\\boldsymbol{w}^2} \\mathcal{L} & =\\sum_{\\mu=1}^N y_\\mu \\partial_{\\boldsymbol{w}} H+\\frac{\\left(1-y_\\mu\\right)}{1+\\exp (-C)}\\left(\\partial_{\\boldsymbol{w}} H\\right)+\\frac{y_\\mu}{1+\\exp (-\\bar{C})}\\left(-\\partial_{\\boldsymbol{w}} H\\right) \\\\\n",
    "& +\\left(1-y_\\mu\\right)\\left(\\frac{X_\\mu}{\\sqrt{d}}+H\\right) \\partial_{\\boldsymbol{w}} \\frac{1}{1+\\exp (-C)}+y_\\mu\\left(\\frac{X_\\mu}{\\sqrt{d}}-H\\right) \\partial_{\\boldsymbol{w}} \\frac{1}{1+\\exp (-\\bar{C})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Let's compute the individual terms. For now, we simplify $\\mathrm{H}$ a bit We let $H=\\frac{\\varepsilon_t}{\\sqrt{d}} \\frac{w}{\\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}}$. Thus, the derivative is\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\partial_{\\boldsymbol{w}} H=\\frac{\\varepsilon_t}{\\sqrt{d} \\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}} \\mathbb{I}-\\frac{\\varepsilon_t}{\\sqrt{d}} \\frac{\\boldsymbol{w} \\boldsymbol{w}^T}{\\left(\\boldsymbol{w}^T \\boldsymbol{w}\\right)^{3 / 2}} \\\\\n",
    "\\partial_{\\boldsymbol{w}} C_\\mu=X_\\mu^T+\\frac{1}{2} \\varepsilon_t \\frac{\\boldsymbol{w}^T}{\\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}} \\quad \\partial_{\\boldsymbol{w}} \\bar{C}_\\mu=X_\\mu^T-\\frac{1}{2} \\varepsilon_t \\frac{\\boldsymbol{w}^T}{\\sqrt{\\boldsymbol{w}^T \\boldsymbol{w}}} \\\\\n",
    "\\partial_{\\boldsymbol{w}} \\frac{1}{1+\\exp (-C)}=\\frac{-\\partial_{\\boldsymbol{w}} C}{2 \\cosh C+2}\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to make sure to compute the sigmoids correctly and the cosh terms. The sigmoids we know already how to numerically compute, how about the coshs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosh Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_stable_cosh_plus(z,e,y):\n",
    "    if z+e < 0:\n",
    "        return mpmath.exp(z+e)/( 1 + mpmath.exp(2*z+2*e) + 2*mpmath.exp(z+e) )\n",
    "    else:\n",
    "        return mpmath.exp( -z-e )/( 1 + mpmath.exp(-2*z-2*e) + 2*mpmath.exp(-z-e) )\n",
    "    \n",
    "def mp_stable_cosh_minus(z,e,y):\n",
    "    if z-e < 0:\n",
    "        return mpmath.exp(z-e)/( 1 + mpmath.exp(2*z-2*e) + 2*mpmath.exp(z-e) )\n",
    "    else:\n",
    "        return mpmath.exp( -z+e )/( 1 + mpmath.exp(-2*z+2*e) + 2*mpmath.exp(-z+e) )\n",
    "\n",
    "def mp_cosh_plus(z,e,y):\n",
    "    return 1/(2*mpmath.cosh(z+e) + 2)\n",
    "\n",
    "def mp_cosh_minus(z,e,y):\n",
    "    return 1/(2*mpmath.cosh(z-e) + 2)\n",
    "\n",
    "def naive_cosh_plus(z,e,y):\n",
    "    return 1/(2*np.cosh(z+e) + 2)\n",
    "\n",
    "def naive_cosh_minus(z,e,y):\n",
    "    return 1/(2*np.cosh(z-e) + 2)\n",
    "\n",
    "def stable_cosh_plus(z,e,y):\n",
    "    if z+e < 0:\n",
    "        return np.exp( z+e )/( 1 + np.exp(2*z+2*e) + 2*np.exp(z+e) )\n",
    "    else:\n",
    "        return np.exp( -z-e )/( 1 + np.exp(-2*z-2*e) + 2*np.exp(-z-e) )\n",
    "    \n",
    "def stable_cosh_minus(z,e,y):\n",
    "    if z-e < 0:\n",
    "        return np.exp( z-e )/( 1 + np.exp(2*z-2*e) + 2*np.exp(z-e) )\n",
    "    else:\n",
    "        return np.exp( -z+e )/( 1 + np.exp(-2*z+2*e) + 2*np.exp(-z+e) )\n",
    "    \n",
    "def stable_cosh(x):\n",
    "    out = np.zeros_like(x)\n",
    "    idx = x <= 0\n",
    "    out[idx] = np.exp(x[idx]) / (1 + np.exp(2*x[idx]) + 2*np.exp(x[idx]))\n",
    "    idx = x > 0\n",
    "    out[idx] = np.exp(-x[idx]) / (1 + np.exp(-2*x[idx]) + 2*np.exp(-x[idx]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/9b_m1n3130xfbjd06qfyyqgc0000gn/T/ipykernel_29327/4029768107.py:20: RuntimeWarning: overflow encountered in cosh\n",
      "  return 1/(2*np.cosh(z+e) + 2)\n",
      "/var/folders/4p/9b_m1n3130xfbjd06qfyyqgc0000gn/T/ipykernel_29327/4029768107.py:23: RuntimeWarning: overflow encountered in cosh\n",
      "  return 1/(2*np.cosh(z-e) + 2)\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate all the functions\n",
    "cosh_plus_stable_mp = np.array([mp_stable_cosh_plus(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "cosh_plus_stable_mp = np.array([np.float64(x) for x in cosh_plus_stable_mp])\n",
    "cosh_plus_stable_mp = cosh_plus_stable_mp.flatten()\n",
    "\n",
    "cosh_minus_stable_mp = np.array([mp_stable_cosh_minus(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "cosh_minus_stable_mp = np.array([np.float64(x) for x in cosh_minus_stable_mp])\n",
    "cosh_minus_stable_mp = cosh_minus_stable_mp.flatten()\n",
    "\n",
    "cosh_plus_mp = np.array([mp_cosh_plus(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "cosh_plus_mp = np.array([np.float64(x) for x in cosh_plus_mp])\n",
    "cosh_plus_mp = cosh_plus_mp.flatten()\n",
    "\n",
    "cosh_minus_mp = np.array([mp_cosh_minus(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "cosh_minus_mp = np.array([np.float64(x) for x in cosh_minus_mp])\n",
    "cosh_minus_mp = cosh_minus_mp.flatten()\n",
    "\n",
    "cosh_plus_stable = np.array([stable_cosh_plus(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "cosh_plus_stable = cosh_plus_stable.flatten()\n",
    "\n",
    "cosh_minus_stable = np.array([stable_cosh_minus(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "cosh_minus_stable = cosh_minus_stable.flatten()\n",
    "\n",
    "cosh_plus_naive = np.array([naive_cosh_plus(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "cosh_plus_naive = cosh_plus_naive.flatten()\n",
    "\n",
    "cosh_minus_naive = np.array([naive_cosh_minus(z,e,y) for z,e,y in zip(Z,E,Y)])\n",
    "cosh_minus_naive = cosh_minus_naive.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between Stable MP and Stable\n",
      "Average absolute error: 2.66e-24\n",
      "Standard deviation of the absolute error: 1.90e-22\n",
      "Comparison between Stable MP and Naive\n",
      "Average absolute error: 3.82e-19\n",
      "Standard deviation of the absolute error: 3.33e-18\n",
      "Comparison between Stable MP and MP\n",
      "Average absolute error: 3.82e-19\n",
      "Standard deviation of the absolute error: 3.33e-18\n"
     ]
    }
   ],
   "source": [
    "# compte the abs error between cosh_plus_stable_mp and cosh_plus_stable, cosh_plus_naive\n",
    "abs_error = np.abs(cosh_plus_stable_mp - cosh_plus_stable)\n",
    "abs_error_naive = np.abs(cosh_plus_stable_mp - cosh_plus_naive)\n",
    "abs_error_mp = np.abs(cosh_plus_stable_mp - cosh_plus_mp)\n",
    "\n",
    "\n",
    "# compute the average and standard deviation of the abs error\n",
    "avg_abs_error = np.mean(abs_error)\n",
    "std_abs_error = np.std(abs_error)\n",
    "\n",
    "avg_abs_error_naive = np.mean(abs_error_naive)\n",
    "std_abs_error_naive = np.std(abs_error_naive)\n",
    "\n",
    "avg_abs_error_mp = np.mean(abs_error_mp)\n",
    "std_abs_error_mp = np.std(abs_error_mp)\n",
    "\n",
    "# print the results\n",
    "print(\"Comparison between Stable MP and Stable\")\n",
    "print(\"Average absolute error: %.2e\" % avg_abs_error)\n",
    "print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error)\n",
    "\n",
    "print(\"Comparison between Stable MP and Naive\")\n",
    "print(\"Average absolute error: %.2e\" % avg_abs_error_naive)\n",
    "print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error_naive)\n",
    "\n",
    "print(\"Comparison between Stable MP and MP\")\n",
    "print(\"Average absolute error: %.2e\" % avg_abs_error_mp)\n",
    "print(\"Standard deviation of the absolute error: %.2e\" % std_abs_error_mp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the full Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = np.arange(9.0).reshape((3, 3))\n",
    "# np.outer(x1[0],x1[0]) + np.outer(x1[1],x1[1]) + np.outer(x1[2],x1[2])\n",
    "# np.einsum(\"ij,ik->jk\",x1,x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hessian(X,y,theta,epsilon, lam, Sigma_w):\n",
    "    X = X / np.sqrt(X.shape[1])\n",
    "    raw_prediction = X.dot(theta)\n",
    "\n",
    "    # B - Optimal Attack ()\n",
    "    B = epsilon * np.linalg.norm(theta) / np.sqrt(X.shape[1])\n",
    "\n",
    "    # C and C_prime (n,)\n",
    "    C = raw_prediction + B\n",
    "    C_prime = raw_prediction - B\n",
    "\n",
    "    # H - Derivative of Optimal Attack (d,)\n",
    "    H = epsilon * theta / (np.linalg.norm(theta) * np.sqrt(X.shape[1]))\n",
    "\n",
    "    # dH - Hessian of Optimal Attack (d,d)\n",
    "    dH = np.eye(X.shape[1]) * epsilon / (np.linalg.norm(theta) * np.sqrt(X.shape[1])) - epsilon*np.outer(theta, theta) / (np.linalg.norm(theta) ** 3 * np.sqrt(X.shape[1]))\n",
    "\n",
    "    # dH term\n",
    "    vec = (1-y) * stable_sigmoid(C) + y * stable_sigmoid(-C_prime) # (n,)\n",
    "    hessian = vec.sum() * dH\n",
    "\n",
    "    # dC term and dC_prime term\n",
    "    vecC = (1-y) * stable_cosh(C) # (n,)\n",
    "    vecC_prime = y * stable_cosh(C_prime) # (n,)\n",
    "\n",
    "    # Shift X by H\n",
    "    X_plus = X + H\n",
    "    X_minus = X - H\n",
    "\n",
    "    # dC term\n",
    "    act = np.multiply(X_plus.T, vecC) # (d,n)\n",
    "    hessian += np.einsum('ij,ik->jk', X_plus, act.T) # (d,d)\n",
    "\n",
    "    # dC_prime term\n",
    "    act = np.multiply(X_minus.T, vecC_prime) # (d,n)\n",
    "    hessian += np.einsum('ij,ik->jk', X_minus, act.T) # (d,d)\n",
    "\n",
    "    # Regularization\n",
    "    hessian += lam/2 * (Sigma_w + Sigma_w.T)\n",
    "\n",
    "    return hessian\n",
    "\n",
    "\n",
    "\n",
    "def logistic_hessian(X,y,theta, lam, Sigma_w):\n",
    "    d = X.shape[1]\n",
    "    raw_activation = X.dot(theta) / np.sqrt(d)\n",
    "    B = np.multiply(X.T/np.sqrt(d),stable_cosh(raw_activation)) \n",
    "    a = np.einsum('ij,ik->jk', X/np.sqrt(d), B.T)\n",
    "    assert a.shape == (d,d)\n",
    "\n",
    "    # Regularization\n",
    "    a += lam/2 * (Sigma_w + Sigma_w.T)\n",
    "\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n",
      "stored self to pickle\n"
     ]
    }
   ],
   "source": [
    "# The datamodel needs a logger\n",
    "import logging\n",
    "from scipy.sparse.linalg import eigsh\n",
    "import numpy as np\n",
    "from data_model import *\n",
    "import time\n",
    "logger = logging.getLogger()\n",
    "# Make the logger log to console\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)\n",
    "d = 1000\n",
    "lam = 0.1\n",
    "eps = 0.7\n",
    "Sigma_w = np.eye(d)\n",
    "data_model = VanillaGaussianDataModel(d,logger,source_pickle_path=\"\",delete_existing=True)\n",
    "# data_model = SourceCapacityDataModel(d,logger,source_pickle_path=\"\",delete_existing=True)\n",
    "# data_model = RandomCovariateDataModel(d,logger,source_pickle_path=\"\",delete_existing=True)\n",
    "X,y,X_test, y_test, theta = data_model.generate_data(3000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_eps_0 = compute_hessian(X,y,theta,0.0,lam, Sigma_w)\n",
    "# log_hessian = logistic_hessian(X,y,theta, lam, Sigma_w)\n",
    "hessian_eps_0_5 = compute_hessian(X,y,theta,eps,lam, Sigma_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute eigenvalues: 0.8750619888305664\n",
      "Epsilon = 0\n",
      "Smallest eigenvalue: 0.20724463701230217\n",
      "Epsilon = 0.7\n",
      "Smallest eigenvalue: -0.11862007557873183\n",
      "Logistic Hessian\n",
      "Smallest eigenvalue: 0.20724463701230234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute the smallest eigenvalue using eigsh\n",
    "start = time.time()\n",
    "sa_eps_0 = eigsh(hessian_eps_0, k=1, which='SA')[0][0]\n",
    "sa_eps_0_5 = eigsh(hessian_eps_0_5, k=1, which='SA')[0][0]\n",
    "# sa_log_hessian = eigsh(log_hessian, k=1, which='SA')[0][0]\n",
    "end = time.time()\n",
    "print(\"Time to compute eigenvalues: {}\".format(end-start))\n",
    "\n",
    "# Print the results\n",
    "print(\"Epsilon = 0\")\n",
    "print(\"Smallest eigenvalue: {}\".format(sa_eps_0))\n",
    "\n",
    "print(f\"Epsilon = {eps}\")\n",
    "print(\"Smallest eigenvalue: {}\".format(sa_eps_0_5))\n",
    "\n",
    "# print(\"Logistic Hessian\")\n",
    "# print(\"Smallest eigenvalue: {}\".format(sa_log_hessian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute eigenvalues: 1.976466178894043\n",
      "Time to compute eigenvalues: 0.45595812797546387\n",
      "Time to compute eigenvalues: 1.8214631080627441\n",
      "Time to compute eigenvalues: 0.4981529712677002\n",
      "Time to compute eigenvalues: 0.19749093055725098\n",
      "Epsilon = 0\n",
      "Min eigenvalue: 0.6949215397894504\n",
      "Max eigenvalue: 3.092676120037477\n",
      "Epsilon = 0.5\n",
      "Min eigenvalue: -0.1672938195183254\n",
      "Max eigenvalue: 4.912550018138389\n",
      "Logistic Hessian\n",
      "Min eigenvalue: 0.6949215397894399\n",
      "Max eigenvalue: 3.092676120037423\n",
      "Epsilon = 0\n",
      "Min eigenvalue: 0.6949215397894418\n",
      "Max eigenvalue: 3.092676120037426\n",
      "Epsilon = 0.5\n",
      "Min eigenvalue: -0.167293819518322\n",
      "Max eigenvalue: 4.912550018138306\n",
      "Logistic Hessian\n",
      "Min eigenvalue: 0.6949215397894438\n",
      "Max eigenvalue: 3.0926761200374266\n",
      "Epsilon = 0\n",
      "Min eigenvalue: (0.6949215397894504+0j)\n",
      "Max eigenvalue: (3.092676120037477+0j)\n",
      "Epsilon = 0.5\n",
      "Min eigenvalue: (-0.1672938195183254+0j)\n",
      "Max eigenvalue: (4.912550018138389+0j)\n",
      "Logistic Hessian\n",
      "Min eigenvalue: (0.6949215397894399+0j)\n",
      "Max eigenvalue: (3.092676120037423+0j)\n",
      "Epsilon = 0\n",
      "Min eigenvalue: 0.6949215397894378\n",
      "Max eigenvalue: 3.0926761200374298\n",
      "Epsilon = 0.5\n",
      "Min eigenvalue: -0.16729381951832378\n",
      "Max eigenvalue: 4.912550018138296\n",
      "Logistic Hessian\n",
      "Min eigenvalue: 0.6949215397894425\n",
      "Max eigenvalue: 3.092676120037425\n",
      "Epsilon = 0\n",
      "Smallest eigenvalue: [0.69492154]\n",
      "Epsilon = 0.5\n",
      "Smallest eigenvalue: [-0.16729382]\n",
      "Logistic Hessian\n",
      "Smallest eigenvalue: [0.69492154]\n"
     ]
    }
   ],
   "source": [
    "# # Compute the hessians eigenvalues\n",
    "# # start a timer\n",
    "# import time\n",
    "# start = time.time()\n",
    "# eigvals_eps_0 = np.linalg.eigvals(hessian_eps_0)\n",
    "# eigvals_eps_0_5 = np.linalg.eigvals(hessian_eps_0_5)\n",
    "# eigvals_log_hess = np.linalg.eigvals(log_hessian)\n",
    "# end = time.time()\n",
    "# print(\"Time to compute eigenvalues: {}\".format(end-start))\n",
    "\n",
    "# start = time.time()\n",
    "# h_np_eigvals_eps_0 = np.linalg.eigvalsh(hessian_eps_0)\n",
    "# h_np_eigvals_eps_0_5 = np.linalg.eigvalsh(hessian_eps_0_5)\n",
    "# h_np_eigvals_log_hess = np.linalg.eigvalsh(log_hessian)\n",
    "# end = time.time()\n",
    "# print(\"Time to compute eigenvalues: {}\".format(end-start))\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# sp_eigvals_eps_0 = scipy.linalg.eigvals(hessian_eps_0)\n",
    "# sp_eigvals_eps_0_5 = scipy.linalg.eigvals(hessian_eps_0_5)\n",
    "# sp_eigvals_log_hess = scipy.linalg.eigvals(log_hessian)\n",
    "# end = time.time()\n",
    "# print(\"Time to compute eigenvalues: {}\".format(end-start))\n",
    "\n",
    "# start = time.time()\n",
    "# sp_h_eigvals_eps_0 = scipy.linalg.eigvalsh(hessian_eps_0)\n",
    "# sp_h_eigvals_eps_0_5 = scipy.linalg.eigvalsh(hessian_eps_0_5)\n",
    "# sp_h_eigvals_log_hess = scipy.linalg.eigvalsh(log_hessian)\n",
    "# end = time.time()\n",
    "# print(\"Time to compute eigenvalues: {}\".format(end-start))\n",
    "\n",
    "# # Compute the smallest eigenvalue using eigsh\n",
    "# start = time.time()\n",
    "# sa_eps_0 = eigsh(hessian_eps_0, k=1, which='SA')[0]\n",
    "# sa_eps_0_5 = eigsh(hessian_eps_0_5, k=1, which='SA')[0]\n",
    "# sa_log_hessian = eigsh(log_hessian, k=1, which='SA')[0]\n",
    "# end = time.time()\n",
    "# print(\"Time to compute eigenvalues: {}\".format(end-start))\n",
    "\n",
    "# # print min max\n",
    "# print(\"Epsilon = 0\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(eigvals_eps_0)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(eigvals_eps_0)))\n",
    "\n",
    "# print(f\"Epsilon = {eps}\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(eigvals_eps_0_5)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(eigvals_eps_0_5)))\n",
    "\n",
    "# print(\"Logistic Hessian\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(eigvals_log_hess)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(eigvals_log_hess)))\n",
    "\n",
    "# # print min max\n",
    "# print(\"Epsilon = 0\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(h_np_eigvals_eps_0)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(h_np_eigvals_eps_0)))\n",
    "\n",
    "# print(f\"Epsilon = {eps}\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(h_np_eigvals_eps_0_5)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(h_np_eigvals_eps_0_5)))\n",
    "\n",
    "# print(\"Logistic Hessian\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(h_np_eigvals_log_hess)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(h_np_eigvals_log_hess)))\n",
    "\n",
    "# # print min max\n",
    "# print(\"Epsilon = 0\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(sp_eigvals_eps_0)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(sp_eigvals_eps_0)))\n",
    "\n",
    "# print(f\"Epsilon = {eps}\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(sp_eigvals_eps_0_5)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(sp_eigvals_eps_0_5)))\n",
    "\n",
    "# print(\"Logistic Hessian\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(sp_eigvals_log_hess)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(sp_eigvals_log_hess)))\n",
    "\n",
    "# # print min max\n",
    "# print(\"Epsilon = 0\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(sp_h_eigvals_eps_0)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(sp_h_eigvals_eps_0)))\n",
    "\n",
    "# print(f\"Epsilon = {eps}\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(sp_h_eigvals_eps_0_5)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(sp_h_eigvals_eps_0_5)))\n",
    "\n",
    "# print(\"Logistic Hessian\")\n",
    "# print(\"Min eigenvalue: {}\".format(np.min(sp_h_eigvals_log_hess)))\n",
    "# print(\"Max eigenvalue: {}\".format(np.max(sp_h_eigvals_log_hess)))\n",
    "\n",
    "# from scipy.sparse.linalg import eigsh\n",
    "\n",
    "# # Define a large sparse matrix (replace this with your actual matrix)\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Epsilon = 0\")\n",
    "# print(\"Smallest eigenvalue: {}\".format(sa_eps_0))\n",
    "\n",
    "# print(f\"Epsilon = {eps}\")\n",
    "# print(\"Smallest eigenvalue: {}\".format(sa_eps_0_5))\n",
    "\n",
    "# print(\"Logistic Hessian\")\n",
    "# print(\"Smallest eigenvalue: {}\".format(sa_log_hessian))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
