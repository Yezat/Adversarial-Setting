{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import inspect\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "\n",
    "from experiment_information import *\n",
    "from data import *\n",
    "from helpers import *\n",
    "from _version import __version__\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import pprint\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "# mpl.rcParams['lines.linewidth'] = 1\n",
    "# mpl.rcParams['legend.fontsize'] = 13\n",
    "\n",
    "# mpl.rcParams['axes.titlesize'] = 15\n",
    "# mpl.rcParams['axes.labelsize'] = 13\n",
    "# mpl.rcParams['xtick.labelsize'] = 10\n",
    "# mpl.rcParams['ytick.labelsize'] = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the potting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current code version,  101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from _version import __version__\n",
    "df_experiments = None\n",
    "df_state_evolution = None\n",
    "df_erm = None\n",
    "logger = logging.getLogger()\n",
    "# with DatabaseHandler(logger,\"experiments/experiments.db\") as dbHandler:\n",
    "with DatabaseHandler(logger,\"../experiments/experiments.db\") as dbHandler:\n",
    "\n",
    "    df_experiments = dbHandler.get_experiments()\n",
    "    df_state_evolution = dbHandler.get_state_evolutions()\n",
    "    df_state_evolution[\"calibrations\"] = df_state_evolution[\"calibrations\"].apply(lambda x: json.loads(x))\n",
    "    df_erm = dbHandler.get_erms()\n",
    "    df_erm[\"analytical_calibrations\"] = df_erm[\"analytical_calibrations\"].apply(lambda x: json.loads(x))\n",
    "    df_erm[\"erm_calibrations\"] = df_erm[\"erm_calibrations\"].apply(lambda x: json.loads(x))\n",
    "    # delete incomplete experiments (bad, deletes running experiments...)\n",
    "    # dbHandler.delete_incomplete_experiments()\n",
    "\n",
    "# def explode_calibrations(df):\n",
    "#     a = df[\"calibrations\"].apply(pd.Series)\n",
    "#     # drop the original calibrations column\n",
    "#     df = df.drop(columns=[\"calibrations\"])\n",
    "#     # concat the original dataframe with the new dataframe containing the exploded calibrations column\n",
    "#     df = pd.concat([df,a],axis=1)\n",
    "#     # explode both calibrations and ps columns\n",
    "#     df = df.explode([\"calibrations\",\"ps\"])\n",
    "#     # rename the exploded columns\n",
    "#     df = df.rename(columns={\"calibrations\":\"calibration\",\"ps\":\"p_calibration\"})\n",
    "#     return df\n",
    "# df_state_evolution = explode_calibrations(df_state_evolution)\n",
    "# def explode_erm_calibrations(df):\n",
    "#     a = df[\"erm_calibrations\"].apply(pd.Series)\n",
    "#     # drop the original calibrations column\n",
    "#     df = df.drop(columns=[\"erm_calibrations\"])\n",
    "#     b = df[\"analytical_calibrations\"].apply(pd.Series)\n",
    "#     # drop the original calibrations column\n",
    "#     df = df.drop(columns=[\"analytical_calibrations\"])\n",
    "#     # drop the dp and ps columns from b\n",
    "#     b = b.drop(columns=[\"dp\",\"ps\"])\n",
    "#     # rename the columns of b\n",
    "#     b = b.rename(columns={\"calibrations\":\"analytical_calibration\"})\n",
    "#     # rename the columns of a\n",
    "#     a = a.rename(columns={\"calibrations\":\"erm_calibration\"})\n",
    "#     # concat the original dataframe with the new dataframe containing the exploded calibrations column\n",
    "#     df = pd.concat([df,a],axis=1)\n",
    "#     df = pd.concat([df,b],axis=1)\n",
    "#     # explode both calibrations and ps columns\n",
    "#     df = df.explode([\"erm_calibration\",\"analytical_calibration\",\"ps\"])\n",
    "#     # rename the exploded columns\n",
    "#     df = df.rename(columns={\"ps\":\"p_calibration\"})\n",
    "#     return df\n",
    "# df_erm = explode_erm_calibrations(df_erm)\n",
    "\n",
    "def explode_measures(df, new_columns, columns):\n",
    "    for column in columns:\n",
    "        def transform(column):\n",
    "            # replace NaN in the column string by 0\n",
    "            column = column.replace(\"NaN\",\"0\")\n",
    "            # replace null in the column string by 0\n",
    "            column = column.replace(\"null\",\"0\")\n",
    "            # replace Infinity in the column string by np.inf\n",
    "            column = column.replace(\"Infinity\",\"np.inf\")\n",
    "            return eval(column)\n",
    "        df[column] = df[column].apply(transform)\n",
    "\n",
    "    exploded = df.explode(columns).reset_index(drop=True)\n",
    "\n",
    "    for new_column, column in zip(new_columns, columns):\n",
    "        if len(exploded[column].tolist()) > 0:\n",
    "            exploded[[\"attack_epsilon\",new_column]] = pd.DataFrame(exploded[column].tolist(), index=exploded.index)\n",
    "        else:\n",
    "            exploded[new_column] = np.nan\n",
    "            # set attack_epsilon\n",
    "            exploded[\"attack_epsilon\"] = np.nan\n",
    "\n",
    "    exploded = exploded.drop(columns=columns)\n",
    "    return exploded\n",
    "\n",
    "\n",
    "def explode_erm_measures(df):\n",
    "    columns = [\"adversarial_generalization_errors\",\"adversarial_generalization_errors_teacher\",\"adversarial_generalization_errors_overlap\",\"fair_adversarial_errors\",\"test_losses\",\"boundary_loss_test_es\"]\n",
    "    new_columns = [\"adversarial_generalization_error\",\"adversarial_generalization_error_teacher\",\"adversarial_generalization_error_overlap\",\"fair_adversarial_error\",\"test_loss\",\"boundary_loss_test\"]\n",
    "    return explode_measures(df, new_columns, columns)\n",
    "\n",
    "def explode_state_evolution_measures(df):\n",
    "    columns = [\"adversarial_generalization_errors\",\"adversarial_generalization_errors_teacher\",\"fair_adversarial_errors\",\"first_term_fair_errors\",\"second_term_fair_errors\",\"third_term_fair_errors\",\"test_losses\",\"data_model_adversarial_generalization_errors\",\"gamma_robustness_es\",\"boundary_loss_test_es\"]\n",
    "    new_columns = [\"adversarial_generalization_error\",\"adversarial_generalization_error_teacher\",\"fair_adversarial_error\",\"first_term_fair_error\",\"second_term_fair_error\",\"third_term_fair_error\",\"test_loss\",\"data_model_adversarial_generalization_error\",\"gamma_robustness\",\"boundary_loss_test\"] #\n",
    "    # columns = [\"adversarial_generalization_errors\",\"adversarial_generalization_errors_teacher\",\"fair_adversarial_errors\",\"second_term_fair_errors\",\"test_losses\",\"data_model_adversarial_generalization_errors\"]\n",
    "    # new_columns = [\"adversarial_generalization_error\",\"adversarial_generalization_error_teacher\",\"fair_adversarial_error\",\"second_term_fair_error\",\"test_loss\",\"data_model_adversarial_generalization_error\"] #\n",
    "    return explode_measures(df, new_columns, columns)\n",
    "    \n",
    "\n",
    "df_erm = explode_erm_measures(df_erm)\n",
    "\n",
    "df_state_evolution = explode_state_evolution_measures(df_state_evolution)\n",
    "    \n",
    "\n",
    "print(\"Current code version, \", __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_eigenvalue(row, column):\n",
    "    array = row[column]\n",
    "\n",
    "    # load json string to array\n",
    "    array = json.loads(array)\n",
    "    return np.array([float(array[0])])\n",
    "def extract_second_eigenvalue(row, column):\n",
    "    array = row[column]\n",
    "\n",
    "    # load json string to array\n",
    "    array = json.loads(array)\n",
    "    return np.array([float(array[-1])])\n",
    "def extract_trace(row, column):\n",
    "    array = row[column]\n",
    "\n",
    "    # load json string to array\n",
    "    array = json.loads(array)\n",
    "    return np.array([float(sum(array))])\n",
    "\n",
    "df2 = df_state_evolution.reset_index()\n",
    "\n",
    "\n",
    "df2[\"sigmax_first_ev\"] = df2.apply(lambda row: extract_first_eigenvalue(row, \"sigmax_eigenvalues\"), axis=1)\n",
    "df2[\"sigmax_second_ev\"] = df2.apply(lambda row: extract_second_eigenvalue(row, \"sigmax_eigenvalues\"), axis=1)\n",
    "df2[\"sigmax_trace\"] = df2.apply(lambda row: extract_trace(row, \"sigmax_eigenvalues\"), axis=1)\n",
    "\n",
    "df2[\"sigmatheta_first_ev\"] = df2.apply(lambda row: extract_first_eigenvalue(row, \"sigmatheta_eigenvalues\"), axis=1)\n",
    "df2[\"sigmatheta_second_ev\"] = df2.apply(lambda row: extract_second_eigenvalue(row, \"sigmatheta_eigenvalues\"), axis=1)\n",
    "df2[\"sigmatheta_trace\"] = df2.apply(lambda row: extract_trace(row, \"sigmatheta_eigenvalues\"), axis=1)\n",
    "\n",
    "df2[\"xtheta_first_ev\"] = df2.apply(lambda row: extract_first_eigenvalue(row, \"xtheta_eigenvalues\"), axis=1)\n",
    "df2[\"xtheta_second_ev\"] = df2.apply(lambda row: extract_second_eigenvalue(row, \"xtheta_eigenvalues\"), axis=1)\n",
    "df2[\"xtheta_trace\"] = df2.apply(lambda row: extract_trace(row, \"xtheta_eigenvalues\"), axis=1)\n",
    "\n",
    "# columns = [\"sigmax_first_ev\",\"sigmax_second_ev\",\"sigmax_trace\",\"sigmatheta_first_ev\",\"sigmatheta_second_ev\",\"sigmatheta_trace\",\"xtheta_first_ev\",\"xtheta_second_ev\",\"xtheta_trace\"]\n",
    "\n",
    "# for column in columns:\n",
    "#     # apply mean and std to the new column\n",
    "#     df_result = df2[column].apply(lambda x: pd.Series({\"mean\":x, \"std\":0}))\n",
    "#     multiindex = pd.MultiIndex.from_product([[column], ['mean', 'std']], names=['', ''])\n",
    "#     df_result.columns = multiindex\n",
    "#     # Concatenate the original DataFrame with the computed values DataFrame\n",
    "#     df2 = pd.concat([df2, df_result], axis=1)\n",
    "\n",
    "# drop the original eigenvalues columns\n",
    "df2 = df2.drop(columns=[\"sigmax_eigenvalues\",\"sigmatheta_eigenvalues\",\"xtheta_eigenvalues\"])\n",
    "\n",
    "df_state_evolution = df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>duration</th>\n",
       "      <th>problem_types</th>\n",
       "      <th>code_version</th>\n",
       "      <th>date</th>\n",
       "      <th>state_evolution_repetitions</th>\n",
       "      <th>erm_repetitions</th>\n",
       "      <th>alphas</th>\n",
       "      <th>epsilons</th>\n",
       "      <th>...</th>\n",
       "      <th>taus</th>\n",
       "      <th>ps</th>\n",
       "      <th>dp</th>\n",
       "      <th>d</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>completed</th>\n",
       "      <th>data_model_types</th>\n",
       "      <th>data_model_names</th>\n",
       "      <th>data_model_descriptions</th>\n",
       "      <th>gamma_fair_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>PowerLawBetaSweep</td>\n",
       "      <td>66.651880</td>\n",
       "      <td>[\"Logistic\"]</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-01-29 11:11:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1000000.0]</td>\n",
       "      <td>[0.0, 0.2]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>null</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>Sweep</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...</td>\n",
       "      <td>[\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...</td>\n",
       "      <td>[\"2 Features, Theta Identity, Sigma_upsilon Id...</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>14de8cb0-38e1-412f-ba8d-8372934d8d52</td>\n",
       "      <td>PowerLawBetaSweep</td>\n",
       "      <td>42.126652</td>\n",
       "      <td>[\"Logistic\"]</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-01-29 11:01:17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1000000.0]</td>\n",
       "      <td>[0.0, 0.2]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>null</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>Sweep</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...</td>\n",
       "      <td>[\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...</td>\n",
       "      <td>[\"2 Features, Theta Identity, Sigma_upsilon Id...</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>09cd19fa-14a1-4099-a415-3ff2d61ba666</td>\n",
       "      <td>PowerLawBetaSweep</td>\n",
       "      <td>48.764761</td>\n",
       "      <td>[\"Logistic\"]</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-01-29 10:54:40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[100000.0]</td>\n",
       "      <td>[0.0, 0.2]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>null</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>Sweep</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...</td>\n",
       "      <td>[\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...</td>\n",
       "      <td>[\"2 Features, Theta Identity, Sigma_upsilon Id...</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2812efcb-ece9-42f5-a200-5b5f1d6662ef</td>\n",
       "      <td>PowerLawBetaSweep</td>\n",
       "      <td>24.958861</td>\n",
       "      <td>[\"Logistic\"]</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-01-29 10:46:12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[10000.0]</td>\n",
       "      <td>[0.0, 0.2]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>null</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>Sweep</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...</td>\n",
       "      <td>[\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...</td>\n",
       "      <td>[\"2 Features, Theta Identity, Sigma_upsilon Id...</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>cadea355-37f1-4cd4-a048-45ae8fc86ca3</td>\n",
       "      <td>PowerLawBetaSweep</td>\n",
       "      <td>320.390882</td>\n",
       "      <td>[\"Logistic\"]</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-01-29 10:36:18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[10000000.0]</td>\n",
       "      <td>[0.0, 0.2]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>null</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>Sweep</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...</td>\n",
       "      <td>[\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...</td>\n",
       "      <td>[\"2 Features, Theta Identity, Sigma_upsilon Id...</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           experiment_id    experiment_name    duration  \\\n",
       "54  62213053-2758-45e7-8ee9-05cf0dd72e0f  PowerLawBetaSweep   66.651880   \n",
       "53  14de8cb0-38e1-412f-ba8d-8372934d8d52  PowerLawBetaSweep   42.126652   \n",
       "52  09cd19fa-14a1-4099-a415-3ff2d61ba666  PowerLawBetaSweep   48.764761   \n",
       "51  2812efcb-ece9-42f5-a200-5b5f1d6662ef  PowerLawBetaSweep   24.958861   \n",
       "50  cadea355-37f1-4cd4-a048-45ae8fc86ca3  PowerLawBetaSweep  320.390882   \n",
       "\n",
       "   problem_types code_version                 date  \\\n",
       "54  [\"Logistic\"]          100  2024-01-29 11:11:31   \n",
       "53  [\"Logistic\"]          100  2024-01-29 11:01:17   \n",
       "52  [\"Logistic\"]          100  2024-01-29 10:54:40   \n",
       "51  [\"Logistic\"]          100  2024-01-29 10:46:12   \n",
       "50  [\"Logistic\"]          100  2024-01-29 10:36:18   \n",
       "\n",
       "    state_evolution_repetitions  erm_repetitions        alphas    epsilons  \\\n",
       "54                            1                0   [1000000.0]  [0.0, 0.2]   \n",
       "53                            1                0   [1000000.0]  [0.0, 0.2]   \n",
       "52                            1                0    [100000.0]  [0.0, 0.2]   \n",
       "51                            1                0     [10000.0]  [0.0, 0.2]   \n",
       "50                            1                0  [10000000.0]  [0.0, 0.2]   \n",
       "\n",
       "    ...    taus    ps    dp    d  experiment_type  completed  \\\n",
       "54  ...  [0.05]  null  0.01  500            Sweep          1   \n",
       "53  ...  [0.05]  null  0.01  500            Sweep          1   \n",
       "52  ...  [0.05]  null  0.01  500            Sweep          1   \n",
       "51  ...  [0.05]  null  0.01  500            Sweep          1   \n",
       "50  ...  [0.05]  null  0.01  500            Sweep          1   \n",
       "\n",
       "                                     data_model_types  \\\n",
       "54  [\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...   \n",
       "53  [\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...   \n",
       "52  [\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...   \n",
       "51  [\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...   \n",
       "50  [\"KFeaturesModel\", \"KFeaturesModel\", \"KFeature...   \n",
       "\n",
       "                                     data_model_names  \\\n",
       "54  [\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...   \n",
       "53  [\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...   \n",
       "52  [\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...   \n",
       "51  [\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...   \n",
       "50  [\"KFeaturesModel_PowerLaw_Coefficient_1.01___P...   \n",
       "\n",
       "                              data_model_descriptions gamma_fair_error  \n",
       "54  [\"2 Features, Theta Identity, Sigma_upsilon Id...           0.0001  \n",
       "53  [\"2 Features, Theta Identity, Sigma_upsilon Id...           0.0001  \n",
       "52  [\"2 Features, Theta Identity, Sigma_upsilon Id...           0.0001  \n",
       "51  [\"2 Features, Theta Identity, Sigma_upsilon Id...           0.0001  \n",
       "50  [\"2 Features, Theta Identity, Sigma_upsilon Id...           0.0001  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the completed experiments with the current code version ordered by date\n",
    "version_choice = __version__\n",
    "version_choice = \"100\"\n",
    "df_experiments = df_experiments[(df_experiments[\"completed\"]==True) & (df_experiments[\"code_version\"]==version_choice)]\n",
    "df_experiments = df_experiments.sort_values(by=\"date\",ascending=False)\n",
    "df_experiments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state_evolution[\"estimation_error\"] = 1 + df_state_evolution[\"q\"] - 2 * df_state_evolution[\"m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62213053-2758-45e7-8ee9-05cf0dd72e0f\n",
      "Sweep\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "KFeaturesModel\n",
      "['KFeaturesModel_PowerLaw_Coefficient_1.01___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.0786___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.1472___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.2159___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.2845___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.3531___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.4217___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.4903___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.559___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.6276___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.6962___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.7648___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.8334___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.9021___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_1.9707___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.0393___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.1079___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.1766___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.2452___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.3138___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.3824___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.451___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.5197___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.5883___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.6569___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.7255___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.7941___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.8628___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_2.9314___PowerLawBetaSweep', 'KFeaturesModel_PowerLaw_Coefficient_3.0___PowerLawBetaSweep']\n",
      "[\"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\", \"2 Features, Theta Identity, Sigma_upsilon Identity, Sigma_delta Identity\"]\n",
      "PowerLawBetaSweep\n",
      "['Logistic']\n"
     ]
    }
   ],
   "source": [
    "experiment_loc = 0\n",
    "\n",
    "# extract and print the top experiment_id\n",
    "experiment_id = df_experiments.iloc[experiment_loc][\"experiment_id\"]\n",
    "print(experiment_id)\n",
    "\n",
    "# extract and print the experiment type\n",
    "experiment_type = df_experiments.iloc[experiment_loc][\"experiment_type\"]\n",
    "print(experiment_type)\n",
    "\n",
    "# extract and print the data model type and data_model name used\n",
    "data_model_types = df_experiments.iloc[experiment_loc][\"data_model_types\"]\n",
    "# convert it to the enum\n",
    "data_model_types = [DataModelType[data_model_type] for data_model_type in json.loads(data_model_types)]\n",
    "data_model_names = [name for name in json.loads(df_experiments.iloc[experiment_loc][\"data_model_names\"])]\n",
    "data_model_descriptions = df_experiments.iloc[experiment_loc][\"data_model_descriptions\"]\n",
    "for data_model_type in data_model_types:\n",
    "    print(data_model_type.name)\n",
    "print(data_model_names)\n",
    "print(data_model_descriptions)\n",
    "\n",
    "# print the experiment name\n",
    "experiment_name = df_experiments.iloc[experiment_loc][\"experiment_name\"]\n",
    "print(experiment_name)\n",
    "\n",
    "# print the experiment problem types\n",
    "experiment_problem_types = df_experiments.iloc[experiment_loc][\"problem_types\"]\n",
    "experiment_problem_types = json.loads(experiment_problem_types)\n",
    "print(experiment_problem_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_name_dict = {}\n",
    "data_model_name_dict[\"VanillaGaussian\"] = \"Vanilla Gaussian\"\n",
    "data_model_name_dict[\"2_VanillaGaussian\"] = \"Vanilla Gaussian\"\n",
    "data_model_name_dict[\"VanillaGaussianThetaFirst\"] = \"Vanilla Gaussian - Teacher 10:1\"\n",
    "data_model_name_dict[\"VanillaGaussianTimes10\"] = \"Vanilla Gaussian x10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[1 1]\"] = \"Strong Weak 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1 1]\"] = \"Strong Weak 5:1\"\n",
    "data_model_name_dict[\"2_KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1 1]\"] = \"Strong Weak 5:1\"\n",
    "data_model_name_dict[\"2_KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[1 1]\"] = \"Strong Weak 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[10  1]\"] = \"Strong Weak 5:1 - Teacher 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[10  1]\"] = \"Strong Weak 10:1 - Teacher 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[ 1 10]\"] = \"Strong Weak 5:1 - Teacher 1:10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[ 1 10]\"] = \"Strong Weak 10:1 - Teacher 1:10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[1 1]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Non-Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[5 5]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[5 5]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Non-Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[1 1]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[1 1]_SD_1_1_SU_1_1\"] = \"Non-Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[5 5]_SD_1_1_SU_1_1\"] = \"Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[5 5]_SD_1_1_SU_1_1\"] = \"Non-Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[1 1]_SD_1_1_SU_1_1\"] = \"Robust Non-Useful\"\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[2 2]_[2 2]_SD_1_1_SU_1_1\"] = \"Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[4 4]_SD_1_1_SU_1_1\"] = \"Non-Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[2 2]_[0.5 0.5]_SD_1_1_SU_1_1\"] = \"Robust Non-Useful\"\n",
    "\n",
    "\n",
    "data_model_name_dict['KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[0.5 0.5]_[2 2]_SD_1_1_SU_1_1'] = \"Non-Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[0.5 0.5]_[8 8]_SD_1_1_SU_1_1\"] = \"Non-Robust Useful\"\n",
    "\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[2.  0.5]_[1 1]_SD_1_1_SU_1_1\"] = \"Invariant Defence\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingFirstStronger_AttackingIdentity_[0.5 0.5]_[2.  0.5]_[1 1]_SD_2_1_SU_1_1\"] = \"Protecting Robust\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[2.  0.5]_[1 1]_SD_1_2_SU_1_1\"] = \"Protecting Non-Robust\"\n",
    "\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[1 1]_SD_1_1_SU_1_1\"] = \"Invariant Defence\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingFirstStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[1 1]_SD_2_1_SU_1_1\"] = \"Protecting Robust\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[1 1]_SD_1_2_SU_1_1\"] = \"Protecting Non-Robust\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/9b_m1n3130xfbjd06qfyyqgc0000gn/T/ipykernel_9475/3541688095.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: str(x))\n",
      "/var/folders/4p/9b_m1n3130xfbjd06qfyyqgc0000gn/T/ipykernel_9475/3541688095.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: json.loads(x))\n"
     ]
    }
   ],
   "source": [
    "state_evolution = df_state_evolution[df_state_evolution[\"experiment_id\" ] == experiment_id]\n",
    "gd = df_erm[df_erm[\"experiment_id\" ] == experiment_id]\n",
    "# make the column subspace_overlaps to string\n",
    "state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: str(x))\n",
    "gd[\"subspace_overlaps\"] = gd[\"subspace_overlaps\"].apply(lambda x: str(x))\n",
    "\n",
    "# create a json colum\n",
    "state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: json.loads(x))\n",
    "gd[\"subspace_overlaps\"] = gd[\"subspace_overlaps\"].apply(lambda x: json.loads(x))\n",
    "from pandas import json_normalize\n",
    "# normalize the json column\n",
    "normalized = json_normalize(state_evolution[\"subspace_overlaps\"])\n",
    "normalized_gd = json_normalize(gd[\"subspace_overlaps\"])\n",
    "\n",
    "\n",
    "\n",
    "def explode_array_column(row, col):\n",
    "    return pd.Series(row[col])\n",
    "\n",
    "# reset the index of the original dataframe\n",
    "state_evolution = state_evolution.reset_index(drop=True)\n",
    "gd = gd.reset_index(drop=True)\n",
    "\n",
    "for col in normalized.columns:\n",
    "    expanded_cols = normalized.apply(lambda x: explode_array_column(x,col), axis=1)\n",
    "    col = col[:-1]\n",
    "    expanded_cols.columns = [col+'_{}'.format(i) for i in range(expanded_cols.shape[1])]\n",
    "    # reset the index of the expanded columns\n",
    "    expanded_cols = expanded_cols.reset_index(drop=True)\n",
    "    state_evolution = pd.concat([state_evolution, expanded_cols], axis=1)\n",
    "\n",
    "for col in normalized_gd.columns:\n",
    "    expanded_cols = normalized_gd.apply(lambda x: explode_array_column(x,col), axis=1)\n",
    "    col = col[:-1]\n",
    "    expanded_cols.columns = [col+'_{}'.format(i) for i in range(expanded_cols.shape[1])]\n",
    "    # reset the index of the expanded columns\n",
    "    expanded_cols = expanded_cols.reset_index(drop=True)\n",
    "\n",
    "    gd = pd.concat([gd, expanded_cols], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_evolution[\"subspace_overlaps_ratio\"] = state_evolution[\"subspace_overlaps_ratio\"].apply(lambda x: str(x))\n",
    "state_evolution[\"subspace_overlaps_ratio\"] = state_evolution[\"subspace_overlaps_ratio\"].apply(lambda x: json.loads(x))\n",
    "normalized = json_normalize(state_evolution[\"subspace_overlaps_ratio\"])\n",
    "# rename the columns of the normalized dataframe\n",
    "for column in normalized.columns:\n",
    "    normalized = normalized.rename(columns={column:column+\"_ratio\"})\n",
    "# merge the normalized dataframe with the original dataframe\n",
    "state_evolution = pd.concat([state_evolution, normalized], axis=1)\n",
    "# drop the original subspace_overlaps column\n",
    "state_evolution = state_evolution.drop(columns=[\"subspace_overlaps_ratio\"])\n",
    "\n",
    "gd[\"subspace_overlaps_ratio\"] = gd[\"subspace_overlaps_ratio\"].apply(lambda x: str(x))\n",
    "gd[\"subspace_overlaps_ratio\"] = gd[\"subspace_overlaps_ratio\"].apply(lambda x: json.loads(x))\n",
    "normalized = json_normalize(gd[\"subspace_overlaps_ratio\"])\n",
    "# rename the columns of the normalized dataframe\n",
    "for column in normalized.columns:\n",
    "    normalized = normalized.rename(columns={column:column+\"_ratio\"})\n",
    "# merge the normalized dataframe with the original dataframe\n",
    "gd = pd.concat([gd, normalized], axis=1)\n",
    "# drop the original subspace_overlaps column\n",
    "gd = gd.drop(columns=[\"subspace_overlaps_ratio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mu_usefulness(row):\n",
    "#     rho = row[\"rho\"]\n",
    "#     tau = row[\"tau\"]\n",
    "\n",
    "#     rho = float(rho)\n",
    "#     tau = float(tau)\n",
    "\n",
    "#     return np.sqrt(2 / np.pi) * rho / np.sqrt( rho + tau**2 )\n",
    "\n",
    "# def compute_gamma_robustness(row):\n",
    "#     rho = row[\"rho\"]\n",
    "#     tau = row[\"tau\"]\n",
    "\n",
    "\n",
    "#     rho = float(rho)\n",
    "#     tau = float(tau)\n",
    "\n",
    "#     return np.sqrt(2 / np.pi) * tau / np.sqrt( rho + tau**2 )\n",
    "\n",
    "# def compute_mu_usefulness_ratio(row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the difference between the adversarial_generalization_error and the generalization_error for both the state evolution and the erm\n",
    "state_evolution[\"difference_adv_gen\"] = state_evolution[\"adversarial_generalization_error\"] - state_evolution[\"generalization_error\"]\n",
    "gd[\"difference_adv_gen\"] = gd[\"adversarial_generalization_error\"] - gd[\"generalization_error_erm\"]\n",
    "\n",
    "\n",
    "state_evolution[\"ratio_adv_gen\"] = state_evolution[\"adversarial_generalization_error\"] / state_evolution[\"generalization_error\"]\n",
    "gd[\"ratio_adv_gen\"] = gd[\"adversarial_generalization_error\"] / gd[\"generalization_error_erm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the noise contribution\n",
    "def noise_contribution(rho: float, tau: float) -> float:\n",
    "    if tau == 0:\n",
    "        tau = 1e-10\n",
    "    return 0.5 - np.arctan( np.sqrt( rho / tau**2 ) ) / np.pi\n",
    "\n",
    "state_evolution[\"noise_contribution\"] = state_evolution.apply(lambda x: noise_contribution(x[\"rho\"], x[\"tau\"]), axis=1)\n",
    "gd[\"noise_contribution\"] = gd.apply(lambda x: noise_contribution(x[\"rho\"], x[\"tau\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the noiseless generalization error without the noise contribution\n",
    "state_evolution[\"noiseless_generalization_error\"] = state_evolution[\"generalization_error\"] - state_evolution[\"noise_contribution\"]\n",
    "gd[\"noiseless_generalization_error_erm\"] = gd[\"generalization_error_erm\"] - gd[\"noise_contribution\"]\n",
    "\n",
    "# create a column for the adversarial noiseless generalization error without the noise contribution\n",
    "state_evolution[\"noiseless_adversarial_generalization_error\"] = state_evolution[\"adversarial_generalization_error\"] - state_evolution[\"noise_contribution\"]\n",
    "gd[\"noiseless_adversarial_generalization_error\"] = gd[\"adversarial_generalization_error\"] - gd[\"noise_contribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseless_angle_to_generalisation(angle):\n",
    "    return np.arccos(angle) / np.pi\n",
    "\n",
    "state_evolution[\"noiseless_angle_to_generalisation\"] = state_evolution.apply(lambda x: noiseless_angle_to_generalisation(x[\"angle\"]), axis=1)\n",
    "gd[\"noiseless_angle_to_generalisation\"] = gd.apply(lambda x: noiseless_angle_to_generalisation(x[\"angle\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the A/sqrt(q*N) for both the state evolution and the erm\n",
    "state_evolution[\"A_over_sqrt_qN\"] = state_evolution[\"A\"] / np.sqrt(state_evolution[\"q\"] * state_evolution[\"N\"])\n",
    "gd[\"A_over_sqrt_qN\"] = gd[\"A\"] / np.sqrt(gd[\"q\"] * gd[\"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for m/sqrt( rho*q - m**2 ) vs A/sqrt(q*N) for both the state evolution and the erm\n",
    "state_evolution[\"m_over_sqrt_rhoq_minus_m2\"] = state_evolution[\"m\"] / np.sqrt(state_evolution[\"rho\"] * state_evolution[\"q\"] - state_evolution[\"m\"]**2)\n",
    "gd[\"m_over_sqrt_rhoq_minus_m2\"] = gd[\"m\"] / np.sqrt(gd[\"rho\"] * gd[\"q\"] - gd[\"m\"]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the data_model_name if there is a sequence ___text at the end\n",
    "def strip_data_model_name(data_model_name):\n",
    "    return re.sub(r\"___.*\",\"\",data_model_name)\n",
    "state_evolution[\"data_model_name\"] = state_evolution[\"data_model_name\"].apply(strip_data_model_name)\n",
    "gd[\"data_model_name\"] = gd[\"data_model_name\"].apply(strip_data_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the beta coefficient from the data_model_name\n",
    "def extract_beta(row):\n",
    "    data_model_name = row[\"data_model_name\"]\n",
    "    split = data_model_name.split(\"_\")\n",
    "    # extract the coefficient\n",
    "    beta = split[-1]\n",
    "    # convert the beta to a float\n",
    "    beta = float(beta)\n",
    "    return beta\n",
    "\n",
    "state_evolution[\"beta\"] = state_evolution.apply(lambda x: extract_beta(x), axis=1)\n",
    "if len(gd) > 0:\n",
    "    gd[\"beta\"] = gd.apply(lambda x: extract_beta(x), axis=1)\n",
    "else:\n",
    "    gd[\"beta\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>code_version</th>\n",
       "      <th>duration</th>\n",
       "      <th>problem_type</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>generalization_error</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_error</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>F_ratio</th>\n",
       "      <th>difference_adv_gen</th>\n",
       "      <th>ratio_adv_gen</th>\n",
       "      <th>noise_contribution</th>\n",
       "      <th>noiseless_generalization_error</th>\n",
       "      <th>noiseless_adversarial_generalization_error</th>\n",
       "      <th>noiseless_angle_to_generalisation</th>\n",
       "      <th>A_over_sqrt_qN</th>\n",
       "      <th>m_over_sqrt_rhoq_minus_m2</th>\n",
       "      <th>beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2583</td>\n",
       "      <td>b263cbb7-6299-4996-aa7c-a6f87d4a4013</td>\n",
       "      <td>100</td>\n",
       "      <td>4.147121</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>0.090698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-01-29 11:10:29</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.043794</td>\n",
       "      <td>7.349270</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.045479</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.317827</td>\n",
       "      <td>70.460018</td>\n",
       "      <td>1.1472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2584</td>\n",
       "      <td>696080b0-ca8b-478a-b36b-200c134b57c1</td>\n",
       "      <td>100</td>\n",
       "      <td>7.071348</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.021897</td>\n",
       "      <td>0.104044</td>\n",
       "      <td>0.021897</td>\n",
       "      <td>2024-01-29 11:10:31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.029317</td>\n",
       "      <td>2.338888</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.013790</td>\n",
       "      <td>0.043107</td>\n",
       "      <td>0.020343</td>\n",
       "      <td>0.280977</td>\n",
       "      <td>15.625906</td>\n",
       "      <td>1.7648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2585</td>\n",
       "      <td>c50c43d9-6775-4a24-8351-518ca510b8c0</td>\n",
       "      <td>100</td>\n",
       "      <td>6.995848</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.021135</td>\n",
       "      <td>0.107668</td>\n",
       "      <td>0.021135</td>\n",
       "      <td>2024-01-29 11:10:32</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032496</td>\n",
       "      <td>2.537534</td>\n",
       "      <td>0.007872</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>0.045759</td>\n",
       "      <td>0.019616</td>\n",
       "      <td>0.303874</td>\n",
       "      <td>16.206164</td>\n",
       "      <td>1.6962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2586</td>\n",
       "      <td>73a95b55-f46b-4712-a8d7-ae4961028b08</td>\n",
       "      <td>100</td>\n",
       "      <td>7.182110</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.019719</td>\n",
       "      <td>0.110293</td>\n",
       "      <td>0.019719</td>\n",
       "      <td>2024-01-29 11:10:32</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036098</td>\n",
       "      <td>2.830672</td>\n",
       "      <td>0.007617</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.018190</td>\n",
       "      <td>0.326823</td>\n",
       "      <td>17.480267</td>\n",
       "      <td>1.6276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2587</td>\n",
       "      <td>b3a67f50-e703-4195-8bd7-ef078ecd3a8f</td>\n",
       "      <td>100</td>\n",
       "      <td>7.170546</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.099814</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>2024-01-29 11:10:32</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026649</td>\n",
       "      <td>2.207582</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.013745</td>\n",
       "      <td>0.040395</td>\n",
       "      <td>0.020441</td>\n",
       "      <td>0.259954</td>\n",
       "      <td>15.550682</td>\n",
       "      <td>1.8334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2588</td>\n",
       "      <td>612b45fa-f8af-4b32-a014-fad14c9d12d7</td>\n",
       "      <td>100</td>\n",
       "      <td>7.391037</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>0.111559</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>2024-01-29 11:10:32</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.039826</td>\n",
       "      <td>3.244347</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>0.010405</td>\n",
       "      <td>0.050231</td>\n",
       "      <td>0.016157</td>\n",
       "      <td>0.346788</td>\n",
       "      <td>19.683918</td>\n",
       "      <td>1.5590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2589</td>\n",
       "      <td>04c58d50-af5f-4547-8d24-3bacc6d4c3b0</td>\n",
       "      <td>100</td>\n",
       "      <td>7.768277</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.015463</td>\n",
       "      <td>0.111247</td>\n",
       "      <td>0.015463</td>\n",
       "      <td>2024-01-29 11:10:32</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>3.794043</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>0.051626</td>\n",
       "      <td>0.013768</td>\n",
       "      <td>0.360583</td>\n",
       "      <td>23.104841</td>\n",
       "      <td>1.4903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2590</td>\n",
       "      <td>a5d556de-9611-4f6f-ae09-cd7674302e9f</td>\n",
       "      <td>100</td>\n",
       "      <td>7.813201</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.021798</td>\n",
       "      <td>0.095307</td>\n",
       "      <td>0.021798</td>\n",
       "      <td>2024-01-29 11:10:32</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>2.121512</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>0.013276</td>\n",
       "      <td>0.037723</td>\n",
       "      <td>0.020066</td>\n",
       "      <td>0.241403</td>\n",
       "      <td>15.842277</td>\n",
       "      <td>1.9021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2591</td>\n",
       "      <td>533c8eab-d1f0-47ad-b370-ebf8fe868c11</td>\n",
       "      <td>100</td>\n",
       "      <td>8.254530</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>0.090785</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>2024-01-29 11:10:33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022630</td>\n",
       "      <td>2.065254</td>\n",
       "      <td>0.008704</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.035169</td>\n",
       "      <td>0.019381</td>\n",
       "      <td>0.225317</td>\n",
       "      <td>16.403687</td>\n",
       "      <td>1.9707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2592</td>\n",
       "      <td>e795f929-ea95-48df-b3d0-63d521d2c5fc</td>\n",
       "      <td>100</td>\n",
       "      <td>8.536574</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>0.086405</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>2024-01-29 11:10:33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021109</td>\n",
       "      <td>2.028093</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.011659</td>\n",
       "      <td>0.032768</td>\n",
       "      <td>0.018518</td>\n",
       "      <td>0.211352</td>\n",
       "      <td>17.169670</td>\n",
       "      <td>2.0393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2593</td>\n",
       "      <td>e7aa3ec3-5e10-409c-8531-4a34ba840767</td>\n",
       "      <td>100</td>\n",
       "      <td>9.363878</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>0.109340</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>2024-01-29 11:10:34</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045731</td>\n",
       "      <td>4.472846</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>0.006451</td>\n",
       "      <td>0.052182</td>\n",
       "      <td>0.011327</td>\n",
       "      <td>0.366247</td>\n",
       "      <td>28.090594</td>\n",
       "      <td>1.4217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2594</td>\n",
       "      <td>737bbca9-546b-4293-ae92-c49701d3d81b</td>\n",
       "      <td>100</td>\n",
       "      <td>9.868379</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.105987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-01-29 11:10:34</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.047094</td>\n",
       "      <td>5.253875</td>\n",
       "      <td>0.006372</td>\n",
       "      <td>0.004699</td>\n",
       "      <td>0.051793</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.363661</td>\n",
       "      <td>35.146961</td>\n",
       "      <td>1.3531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2595</td>\n",
       "      <td>46ac4dee-5a06-43d8-8d08-39ac2efe4985</td>\n",
       "      <td>100</td>\n",
       "      <td>10.050043</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>0.082266</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>2024-01-29 11:10:34</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019814</td>\n",
       "      <td>2.003042</td>\n",
       "      <td>0.009027</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.030541</td>\n",
       "      <td>0.017573</td>\n",
       "      <td>0.199152</td>\n",
       "      <td>18.095207</td>\n",
       "      <td>2.1079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2596</td>\n",
       "      <td>f17ed572-12e5-46b8-b68f-ce16a4d0e3e4</td>\n",
       "      <td>100</td>\n",
       "      <td>10.518581</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>0.078414</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>2024-01-29 11:10:35</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018692</td>\n",
       "      <td>1.985346</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.009799</td>\n",
       "      <td>0.028491</td>\n",
       "      <td>0.016608</td>\n",
       "      <td>0.188384</td>\n",
       "      <td>19.148750</td>\n",
       "      <td>2.1766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2597</td>\n",
       "      <td>89b3511b-0777-4ed9-94f4-fa47098e6630</td>\n",
       "      <td>100</td>\n",
       "      <td>11.264466</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>0.101425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-01-29 11:10:36</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.047209</td>\n",
       "      <td>6.090208</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.050479</td>\n",
       "      <td>0.007069</td>\n",
       "      <td>0.353887</td>\n",
       "      <td>45.020017</td>\n",
       "      <td>1.2845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2598</td>\n",
       "      <td>26728b19-6745-4850-aae9-c13aa61ebacb</td>\n",
       "      <td>100</td>\n",
       "      <td>11.349099</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.018218</td>\n",
       "      <td>0.074880</td>\n",
       "      <td>0.018218</td>\n",
       "      <td>2024-01-29 11:10:36</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017706</td>\n",
       "      <td>1.971905</td>\n",
       "      <td>0.009303</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>0.026621</td>\n",
       "      <td>0.015666</td>\n",
       "      <td>0.178823</td>\n",
       "      <td>20.302308</td>\n",
       "      <td>2.2452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2599</td>\n",
       "      <td>0a712c8a-e8c9-43bc-8750-f1c81428bf63</td>\n",
       "      <td>100</td>\n",
       "      <td>14.358366</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.007798</td>\n",
       "      <td>0.095910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-01-29 11:10:39</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.046185</td>\n",
       "      <td>6.922862</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.048366</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>0.338441</td>\n",
       "      <td>58.836881</td>\n",
       "      <td>1.2159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2600</td>\n",
       "      <td>faa1d018-56cd-425d-80ed-1d0b225196a2</td>\n",
       "      <td>100</td>\n",
       "      <td>4.265552</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.014524</td>\n",
       "      <td>0.056714</td>\n",
       "      <td>0.014524</td>\n",
       "      <td>2024-01-29 11:10:40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012199</td>\n",
       "      <td>1.839919</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.016654</td>\n",
       "      <td>0.010469</td>\n",
       "      <td>0.125893</td>\n",
       "      <td>30.393144</td>\n",
       "      <td>2.7941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2601</td>\n",
       "      <td>6b353ee3-05b5-4e08-a248-371ff75e5050</td>\n",
       "      <td>100</td>\n",
       "      <td>11.283769</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.017518</td>\n",
       "      <td>0.071658</td>\n",
       "      <td>0.017518</td>\n",
       "      <td>2024-01-29 11:10:40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016827</td>\n",
       "      <td>1.960567</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.008093</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>0.014769</td>\n",
       "      <td>0.170251</td>\n",
       "      <td>21.537308</td>\n",
       "      <td>2.3138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2602</td>\n",
       "      <td>77441b52-8311-49da-aa3d-38e16b320a1b</td>\n",
       "      <td>100</td>\n",
       "      <td>7.322058</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>0.057957</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>2024-01-29 11:10:42</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012825</td>\n",
       "      <td>1.871774</td>\n",
       "      <td>0.009995</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.017542</td>\n",
       "      <td>0.010797</td>\n",
       "      <td>0.131648</td>\n",
       "      <td>29.469476</td>\n",
       "      <td>2.7255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2603</td>\n",
       "      <td>01cd2429-4589-44aa-bce4-49b7f399e9da</td>\n",
       "      <td>100</td>\n",
       "      <td>3.499993</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>0.055839</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>2024-01-29 11:10:43</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>1.808902</td>\n",
       "      <td>0.010137</td>\n",
       "      <td>0.004229</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.010181</td>\n",
       "      <td>0.120568</td>\n",
       "      <td>31.254592</td>\n",
       "      <td>2.8628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2604</td>\n",
       "      <td>5b1d7cd2-882e-48ab-a5f8-5ee205dd6e4b</td>\n",
       "      <td>100</td>\n",
       "      <td>12.280480</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.016880</td>\n",
       "      <td>0.068736</td>\n",
       "      <td>0.016880</td>\n",
       "      <td>2024-01-29 11:10:44</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>1.949970</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>0.023378</td>\n",
       "      <td>0.013929</td>\n",
       "      <td>0.162512</td>\n",
       "      <td>22.837602</td>\n",
       "      <td>2.3824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2605</td>\n",
       "      <td>078bff15-a120-4638-add6-2fe0d7e58ac3</td>\n",
       "      <td>100</td>\n",
       "      <td>19.449587</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.005659</td>\n",
       "      <td>0.082966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-01-29 11:10:44</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.041605</td>\n",
       "      <td>8.352173</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.042466</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.296349</td>\n",
       "      <td>106.099530</td>\n",
       "      <td>1.0786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2606</td>\n",
       "      <td>bb4d0540-7d06-4f37-b6a5-6c34dd760dbe</td>\n",
       "      <td>100</td>\n",
       "      <td>13.278244</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.016307</td>\n",
       "      <td>0.066093</td>\n",
       "      <td>0.016307</td>\n",
       "      <td>2024-01-29 11:10:45</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>1.939226</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.006663</td>\n",
       "      <td>0.021979</td>\n",
       "      <td>0.013152</td>\n",
       "      <td>0.155482</td>\n",
       "      <td>24.189008</td>\n",
       "      <td>2.4510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2607</td>\n",
       "      <td>658dd4f8-b96d-405f-9fe2-723c572892ed</td>\n",
       "      <td>100</td>\n",
       "      <td>14.263302</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.015796</td>\n",
       "      <td>0.063704</td>\n",
       "      <td>0.015796</td>\n",
       "      <td>2024-01-29 11:10:47</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014656</td>\n",
       "      <td>1.927835</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>0.006055</td>\n",
       "      <td>0.020710</td>\n",
       "      <td>0.012436</td>\n",
       "      <td>0.149054</td>\n",
       "      <td>25.581916</td>\n",
       "      <td>2.5197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2608</td>\n",
       "      <td>da31a8d4-d07b-4f64-9bb6-85c2a6abce4a</td>\n",
       "      <td>100</td>\n",
       "      <td>15.193065</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>0.061552</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>2024-01-29 11:10:48</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014044</td>\n",
       "      <td>1.914937</td>\n",
       "      <td>0.009832</td>\n",
       "      <td>0.005517</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.011789</td>\n",
       "      <td>0.143126</td>\n",
       "      <td>26.988547</td>\n",
       "      <td>2.5883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2609</td>\n",
       "      <td>d0b6bc08-eb62-4365-aff8-e159575e694f</td>\n",
       "      <td>100</td>\n",
       "      <td>14.107495</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.014972</td>\n",
       "      <td>0.059613</td>\n",
       "      <td>0.014972</td>\n",
       "      <td>2024-01-29 11:10:48</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>1.898829</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.005056</td>\n",
       "      <td>0.018513</td>\n",
       "      <td>0.011219</td>\n",
       "      <td>0.137514</td>\n",
       "      <td>28.359946</td>\n",
       "      <td>2.6569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2610</td>\n",
       "      <td>e499e565-2df4-40b8-9666-0c22e7d055cd</td>\n",
       "      <td>100</td>\n",
       "      <td>23.561209</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>0.075974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-01-29 11:10:48</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.038522</td>\n",
       "      <td>8.876989</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.039034</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.272179</td>\n",
       "      <td>146.166859</td>\n",
       "      <td>1.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2611</td>\n",
       "      <td>0397b0e1-da47-4efc-a93b-f9c7cff815a0</td>\n",
       "      <td>100</td>\n",
       "      <td>19.740512</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>62213053-2758-45e7-8ee9-05cf0dd72e0f</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>0.053507</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>2024-01-29 11:11:00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>1.842737</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>0.015208</td>\n",
       "      <td>0.009279</td>\n",
       "      <td>0.119863</td>\n",
       "      <td>34.294745</td>\n",
       "      <td>2.9314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                    id code_version   duration  \\\n",
       "0    2583  b263cbb7-6299-4996-aa7c-a6f87d4a4013          100   4.147121   \n",
       "1    2584  696080b0-ca8b-478a-b36b-200c134b57c1          100   7.071348   \n",
       "2    2585  c50c43d9-6775-4a24-8351-518ca510b8c0          100   6.995848   \n",
       "3    2586  73a95b55-f46b-4712-a8d7-ae4961028b08          100   7.182110   \n",
       "4    2587  b3a67f50-e703-4195-8bd7-ef078ecd3a8f          100   7.170546   \n",
       "5    2588  612b45fa-f8af-4b32-a014-fad14c9d12d7          100   7.391037   \n",
       "6    2589  04c58d50-af5f-4547-8d24-3bacc6d4c3b0          100   7.768277   \n",
       "7    2590  a5d556de-9611-4f6f-ae09-cd7674302e9f          100   7.813201   \n",
       "8    2591  533c8eab-d1f0-47ad-b370-ebf8fe868c11          100   8.254530   \n",
       "9    2592  e795f929-ea95-48df-b3d0-63d521d2c5fc          100   8.536574   \n",
       "10   2593  e7aa3ec3-5e10-409c-8531-4a34ba840767          100   9.363878   \n",
       "11   2594  737bbca9-546b-4293-ae92-c49701d3d81b          100   9.868379   \n",
       "12   2595  46ac4dee-5a06-43d8-8d08-39ac2efe4985          100  10.050043   \n",
       "13   2596  f17ed572-12e5-46b8-b68f-ce16a4d0e3e4          100  10.518581   \n",
       "14   2597  89b3511b-0777-4ed9-94f4-fa47098e6630          100  11.264466   \n",
       "15   2598  26728b19-6745-4850-aae9-c13aa61ebacb          100  11.349099   \n",
       "16   2599  0a712c8a-e8c9-43bc-8750-f1c81428bf63          100  14.358366   \n",
       "17   2600  faa1d018-56cd-425d-80ed-1d0b225196a2          100   4.265552   \n",
       "18   2601  6b353ee3-05b5-4e08-a248-371ff75e5050          100  11.283769   \n",
       "19   2602  77441b52-8311-49da-aa3d-38e16b320a1b          100   7.322058   \n",
       "20   2603  01cd2429-4589-44aa-bce4-49b7f399e9da          100   3.499993   \n",
       "21   2604  5b1d7cd2-882e-48ab-a5f8-5ee205dd6e4b          100  12.280480   \n",
       "22   2605  078bff15-a120-4638-add6-2fe0d7e58ac3          100  19.449587   \n",
       "23   2606  bb4d0540-7d06-4f37-b6a5-6c34dd760dbe          100  13.278244   \n",
       "24   2607  658dd4f8-b96d-405f-9fe2-723c572892ed          100  14.263302   \n",
       "25   2608  da31a8d4-d07b-4f64-9bb6-85c2a6abce4a          100  15.193065   \n",
       "26   2609  d0b6bc08-eb62-4365-aff8-e159575e694f          100  14.107495   \n",
       "27   2610  e499e565-2df4-40b8-9666-0c22e7d055cd          100  23.561209   \n",
       "28   2611  0397b0e1-da47-4efc-a93b-f9c7cff815a0          100  19.740512   \n",
       "\n",
       "   problem_type                         experiment_id  generalization_error  \\\n",
       "0      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.006898   \n",
       "1      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.021897   \n",
       "2      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.021135   \n",
       "3      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.019719   \n",
       "4      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.022068   \n",
       "5      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.017745   \n",
       "6      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.015463   \n",
       "7      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.021798   \n",
       "8      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.021243   \n",
       "9      Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.020532   \n",
       "10     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.013168   \n",
       "11     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.011071   \n",
       "12     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.019754   \n",
       "13     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.018970   \n",
       "14     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.009274   \n",
       "15     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.018218   \n",
       "16     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.007798   \n",
       "17     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.014524   \n",
       "18     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.017518   \n",
       "19     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.014712   \n",
       "20     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.014366   \n",
       "21     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.016880   \n",
       "22     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.005659   \n",
       "23     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.016307   \n",
       "24     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.015796   \n",
       "25     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.015349   \n",
       "26     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.014972   \n",
       "27     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.004890   \n",
       "28     Logistic  62213053-2758-45e7-8ee9-05cf0dd72e0f              0.013789   \n",
       "\n",
       "    training_loss  training_error                 date  ...  F_ratio  \\\n",
       "0        0.090698        0.000000  2024-01-29 11:10:29  ...      1.0   \n",
       "1        0.104044        0.021897  2024-01-29 11:10:31  ...      1.0   \n",
       "2        0.107668        0.021135  2024-01-29 11:10:32  ...      1.0   \n",
       "3        0.110293        0.019719  2024-01-29 11:10:32  ...      1.0   \n",
       "4        0.099814        0.022068  2024-01-29 11:10:32  ...      1.0   \n",
       "5        0.111559        0.017745  2024-01-29 11:10:32  ...      1.0   \n",
       "6        0.111247        0.015463  2024-01-29 11:10:32  ...      1.0   \n",
       "7        0.095307        0.021798  2024-01-29 11:10:32  ...      1.0   \n",
       "8        0.090785        0.021243  2024-01-29 11:10:33  ...      1.0   \n",
       "9        0.086405        0.020532  2024-01-29 11:10:33  ...      1.0   \n",
       "10       0.109340        0.013168  2024-01-29 11:10:34  ...      1.0   \n",
       "11       0.105987        0.000000  2024-01-29 11:10:34  ...      1.0   \n",
       "12       0.082266        0.019754  2024-01-29 11:10:34  ...      1.0   \n",
       "13       0.078414        0.018970  2024-01-29 11:10:35  ...      1.0   \n",
       "14       0.101425        0.000000  2024-01-29 11:10:36  ...      1.0   \n",
       "15       0.074880        0.018218  2024-01-29 11:10:36  ...      1.0   \n",
       "16       0.095910        0.000000  2024-01-29 11:10:39  ...      1.0   \n",
       "17       0.056714        0.014524  2024-01-29 11:10:40  ...      1.0   \n",
       "18       0.071658        0.017518  2024-01-29 11:10:40  ...      1.0   \n",
       "19       0.057957        0.014712  2024-01-29 11:10:42  ...      1.0   \n",
       "20       0.055839        0.014366  2024-01-29 11:10:43  ...      1.0   \n",
       "21       0.068736        0.016880  2024-01-29 11:10:44  ...      1.0   \n",
       "22       0.082966        0.000000  2024-01-29 11:10:44  ...      1.0   \n",
       "23       0.066093        0.016307  2024-01-29 11:10:45  ...      1.0   \n",
       "24       0.063704        0.015796  2024-01-29 11:10:47  ...      1.0   \n",
       "25       0.061552        0.015349  2024-01-29 11:10:48  ...      1.0   \n",
       "26       0.059613        0.014972  2024-01-29 11:10:48  ...      1.0   \n",
       "27       0.075974        0.000000  2024-01-29 11:10:48  ...      1.0   \n",
       "28       0.053507        0.013789  2024-01-29 11:11:00  ...      1.0   \n",
       "\n",
       "    difference_adv_gen  ratio_adv_gen  noise_contribution  \\\n",
       "0             0.043794       7.349270            0.005213   \n",
       "1             0.029317       2.338888            0.008107   \n",
       "2             0.032496       2.537534            0.007872   \n",
       "3             0.036098       2.830672            0.007617   \n",
       "4             0.026649       2.207582            0.008323   \n",
       "5             0.039826       3.244347            0.007340   \n",
       "6             0.043204       3.794043            0.007040   \n",
       "7             0.024447       2.121512            0.008522   \n",
       "8             0.022630       2.065254            0.008704   \n",
       "9             0.021109       2.028093            0.008872   \n",
       "10            0.045731       4.472846            0.006717   \n",
       "11            0.047094       5.253875            0.006372   \n",
       "12            0.019814       2.003042            0.009027   \n",
       "13            0.018692       1.985346            0.009171   \n",
       "14            0.047209       6.090208            0.006004   \n",
       "15            0.017706       1.971905            0.009303   \n",
       "16            0.046185       6.922862            0.005617   \n",
       "17            0.012199       1.839919            0.010068   \n",
       "18            0.016827       1.960567            0.009425   \n",
       "19            0.012825       1.871774            0.009995   \n",
       "20            0.011621       1.808902            0.010137   \n",
       "21            0.016036       1.949970            0.009538   \n",
       "22            0.041605       8.352173            0.004798   \n",
       "23            0.015316       1.939226            0.009643   \n",
       "24            0.014656       1.927835            0.009741   \n",
       "25            0.014044       1.914937            0.009832   \n",
       "26            0.013457       1.898829            0.009916   \n",
       "27            0.038522       8.876989            0.004379   \n",
       "28            0.011620       1.842737            0.010201   \n",
       "\n",
       "    noiseless_generalization_error noiseless_adversarial_generalization_error  \\\n",
       "0                         0.001685                                   0.045479   \n",
       "1                         0.013790                                   0.043107   \n",
       "2                         0.013263                                   0.045759   \n",
       "3                         0.012102                                   0.048200   \n",
       "4                         0.013745                                   0.040395   \n",
       "5                         0.010405                                   0.050231   \n",
       "6                         0.008423                                   0.051626   \n",
       "7                         0.013276                                   0.037723   \n",
       "8                         0.012539                                   0.035169   \n",
       "9                         0.011659                                   0.032768   \n",
       "10                        0.006451                                   0.052182   \n",
       "11                        0.004699                                   0.051793   \n",
       "12                        0.010727                                   0.030541   \n",
       "13                        0.009799                                   0.028491   \n",
       "14                        0.003271                                   0.050479   \n",
       "15                        0.008915                                   0.026621   \n",
       "16                        0.002181                                   0.048366   \n",
       "17                        0.004455                                   0.016654   \n",
       "18                        0.008093                                   0.024921   \n",
       "19                        0.004717                                   0.017542   \n",
       "20                        0.004229                                   0.015849   \n",
       "21                        0.007342                                   0.023378   \n",
       "22                        0.000861                                   0.042466   \n",
       "23                        0.006663                                   0.021979   \n",
       "24                        0.006055                                   0.020710   \n",
       "25                        0.005517                                   0.019561   \n",
       "26                        0.005056                                   0.018513   \n",
       "27                        0.000512                                   0.039034   \n",
       "28                        0.003588                                   0.015208   \n",
       "\n",
       "    noiseless_angle_to_generalisation  A_over_sqrt_qN  \\\n",
       "0                            0.004517        0.317827   \n",
       "1                            0.020343        0.280977   \n",
       "2                            0.019616        0.303874   \n",
       "3                            0.018190        0.326823   \n",
       "4                            0.020441        0.259954   \n",
       "5                            0.016157        0.346788   \n",
       "6                            0.013768        0.360583   \n",
       "7                            0.020066        0.241403   \n",
       "8                            0.019381        0.225317   \n",
       "9                            0.018518        0.211352   \n",
       "10                           0.011327        0.366247   \n",
       "11                           0.009054        0.363661   \n",
       "12                           0.017573        0.199152   \n",
       "13                           0.016608        0.188384   \n",
       "14                           0.007069        0.353887   \n",
       "15                           0.015666        0.178823   \n",
       "16                           0.005410        0.338441   \n",
       "17                           0.010469        0.125893   \n",
       "18                           0.014769        0.170251   \n",
       "19                           0.010797        0.131648   \n",
       "20                           0.010181        0.120568   \n",
       "21                           0.013929        0.162512   \n",
       "22                           0.003000        0.296349   \n",
       "23                           0.013152        0.155482   \n",
       "24                           0.012436        0.149054   \n",
       "25                           0.011789        0.143126   \n",
       "26                           0.011219        0.137514   \n",
       "27                           0.002178        0.272179   \n",
       "28                           0.009279        0.119863   \n",
       "\n",
       "    m_over_sqrt_rhoq_minus_m2    beta  \n",
       "0                   70.460018  1.1472  \n",
       "1                   15.625906  1.7648  \n",
       "2                   16.206164  1.6962  \n",
       "3                   17.480267  1.6276  \n",
       "4                   15.550682  1.8334  \n",
       "5                   19.683918  1.5590  \n",
       "6                   23.104841  1.4903  \n",
       "7                   15.842277  1.9021  \n",
       "8                   16.403687  1.9707  \n",
       "9                   17.169670  2.0393  \n",
       "10                  28.090594  1.4217  \n",
       "11                  35.146961  1.3531  \n",
       "12                  18.095207  2.1079  \n",
       "13                  19.148750  2.1766  \n",
       "14                  45.020017  1.2845  \n",
       "15                  20.302308  2.2452  \n",
       "16                  58.836881  1.2159  \n",
       "17                  30.393144  2.7941  \n",
       "18                  21.537308  2.3138  \n",
       "19                  29.469476  2.7255  \n",
       "20                  31.254592  2.8628  \n",
       "21                  22.837602  2.3824  \n",
       "22                 106.099530  1.0786  \n",
       "23                  24.189008  2.4510  \n",
       "24                  25.581916  2.5197  \n",
       "25                  26.988547  2.5883  \n",
       "26                  28.359946  2.6569  \n",
       "27                 146.166859  1.0100  \n",
       "28                  34.294745  2.9314  \n",
       "\n",
       "[29 rows x 100 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">index_state_evolution</th>\n",
       "      <th colspan=\"2\" halign=\"left\">duration_state_evolution</th>\n",
       "      <th colspan=\"2\" halign=\"left\">generalization_error_state_evolution</th>\n",
       "      <th colspan=\"2\" halign=\"left\">training_loss_state_evolution</th>\n",
       "      <th colspan=\"2\" halign=\"left\">training_error_state_evolution</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">noiseless_generalization_error_erm_erm</th>\n",
       "      <th colspan=\"2\" halign=\"left\">noiseless_adversarial_generalization_error_erm</th>\n",
       "      <th colspan=\"2\" halign=\"left\">noiseless_angle_to_generalisation_erm</th>\n",
       "      <th colspan=\"2\" halign=\"left\">A_over_sqrt_qN_erm</th>\n",
       "      <th colspan=\"2\" halign=\"left\">m_over_sqrt_rhoq_minus_m2_erm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>...</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>tau</th>\n",
       "      <th>lam</th>\n",
       "      <th>problem_type</th>\n",
       "      <th>attack_epsilon</th>\n",
       "      <th>beta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"29\" valign=\"top\">1000000.0</th>\n",
       "      <th rowspan=\"29\" valign=\"top\">0.2</th>\n",
       "      <th rowspan=\"29\" valign=\"top\">0.05</th>\n",
       "      <th rowspan=\"29\" valign=\"top\">0.001</th>\n",
       "      <th rowspan=\"29\" valign=\"top\">Logistic</th>\n",
       "      <th rowspan=\"29\" valign=\"top\">0.2</th>\n",
       "      <th>1.0100</th>\n",
       "      <td>2610.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.561209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0786</th>\n",
       "      <td>2605.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.449587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.1472</th>\n",
       "      <td>2583.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.147121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.090698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2159</th>\n",
       "      <td>2599.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.358366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2845</th>\n",
       "      <td>2597.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.264466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.101425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3531</th>\n",
       "      <td>2594.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.868379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.105987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.4217</th>\n",
       "      <td>2593.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.363878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.4903</th>\n",
       "      <td>2589.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.768277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5590</th>\n",
       "      <td>2588.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.391037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.6276</th>\n",
       "      <td>2586.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.182110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.110293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.6962</th>\n",
       "      <td>2585.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.995848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.107668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.7648</th>\n",
       "      <td>2584.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.071348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.8334</th>\n",
       "      <td>2587.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.170546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.099814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.9021</th>\n",
       "      <td>2590.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.813201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.9707</th>\n",
       "      <td>2591.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.254530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.090785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0393</th>\n",
       "      <td>2592.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.536574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1079</th>\n",
       "      <td>2595.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.050043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1766</th>\n",
       "      <td>2596.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.518581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.2452</th>\n",
       "      <td>2598.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.349099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.3138</th>\n",
       "      <td>2601.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.283769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.3824</th>\n",
       "      <td>2604.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.280480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.4510</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.278244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5197</th>\n",
       "      <td>2607.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.263302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5883</th>\n",
       "      <td>2608.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.193065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.061552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.6569</th>\n",
       "      <td>2609.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.107495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.7255</th>\n",
       "      <td>2602.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.322058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.7941</th>\n",
       "      <td>2600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.265552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.8628</th>\n",
       "      <td>2603.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.499993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.9314</th>\n",
       "      <td>2611.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.740512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.053507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                index_state_evolution  \\\n",
       "                                                                                 mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                           \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                2610.0   \n",
       "                                                         1.0786                2605.0   \n",
       "                                                         1.1472                2583.0   \n",
       "                                                         1.2159                2599.0   \n",
       "                                                         1.2845                2597.0   \n",
       "                                                         1.3531                2594.0   \n",
       "                                                         1.4217                2593.0   \n",
       "                                                         1.4903                2589.0   \n",
       "                                                         1.5590                2588.0   \n",
       "                                                         1.6276                2586.0   \n",
       "                                                         1.6962                2585.0   \n",
       "                                                         1.7648                2584.0   \n",
       "                                                         1.8334                2587.0   \n",
       "                                                         1.9021                2590.0   \n",
       "                                                         1.9707                2591.0   \n",
       "                                                         2.0393                2592.0   \n",
       "                                                         2.1079                2595.0   \n",
       "                                                         2.1766                2596.0   \n",
       "                                                         2.2452                2598.0   \n",
       "                                                         2.3138                2601.0   \n",
       "                                                         2.3824                2604.0   \n",
       "                                                         2.4510                2606.0   \n",
       "                                                         2.5197                2607.0   \n",
       "                                                         2.5883                2608.0   \n",
       "                                                         2.6569                2609.0   \n",
       "                                                         2.7255                2602.0   \n",
       "                                                         2.7941                2600.0   \n",
       "                                                         2.8628                2603.0   \n",
       "                                                         2.9314                2611.0   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta         \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN   \n",
       "                                                         1.0786 NaN   \n",
       "                                                         1.1472 NaN   \n",
       "                                                         1.2159 NaN   \n",
       "                                                         1.2845 NaN   \n",
       "                                                         1.3531 NaN   \n",
       "                                                         1.4217 NaN   \n",
       "                                                         1.4903 NaN   \n",
       "                                                         1.5590 NaN   \n",
       "                                                         1.6276 NaN   \n",
       "                                                         1.6962 NaN   \n",
       "                                                         1.7648 NaN   \n",
       "                                                         1.8334 NaN   \n",
       "                                                         1.9021 NaN   \n",
       "                                                         1.9707 NaN   \n",
       "                                                         2.0393 NaN   \n",
       "                                                         2.1079 NaN   \n",
       "                                                         2.1766 NaN   \n",
       "                                                         2.2452 NaN   \n",
       "                                                         2.3138 NaN   \n",
       "                                                         2.3824 NaN   \n",
       "                                                         2.4510 NaN   \n",
       "                                                         2.5197 NaN   \n",
       "                                                         2.5883 NaN   \n",
       "                                                         2.6569 NaN   \n",
       "                                                         2.7255 NaN   \n",
       "                                                         2.7941 NaN   \n",
       "                                                         2.8628 NaN   \n",
       "                                                         2.9314 NaN   \n",
       "\n",
       "                                                                duration_state_evolution  \\\n",
       "                                                                                    mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                              \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                23.561209   \n",
       "                                                         1.0786                19.449587   \n",
       "                                                         1.1472                 4.147121   \n",
       "                                                         1.2159                14.358366   \n",
       "                                                         1.2845                11.264466   \n",
       "                                                         1.3531                 9.868379   \n",
       "                                                         1.4217                 9.363878   \n",
       "                                                         1.4903                 7.768277   \n",
       "                                                         1.5590                 7.391037   \n",
       "                                                         1.6276                 7.182110   \n",
       "                                                         1.6962                 6.995848   \n",
       "                                                         1.7648                 7.071348   \n",
       "                                                         1.8334                 7.170546   \n",
       "                                                         1.9021                 7.813201   \n",
       "                                                         1.9707                 8.254530   \n",
       "                                                         2.0393                 8.536574   \n",
       "                                                         2.1079                10.050043   \n",
       "                                                         2.1766                10.518581   \n",
       "                                                         2.2452                11.349099   \n",
       "                                                         2.3138                11.283769   \n",
       "                                                         2.3824                12.280480   \n",
       "                                                         2.4510                13.278244   \n",
       "                                                         2.5197                14.263302   \n",
       "                                                         2.5883                15.193065   \n",
       "                                                         2.6569                14.107495   \n",
       "                                                         2.7255                 7.322058   \n",
       "                                                         2.7941                 4.265552   \n",
       "                                                         2.8628                 3.499993   \n",
       "                                                         2.9314                19.740512   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta         \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN   \n",
       "                                                         1.0786 NaN   \n",
       "                                                         1.1472 NaN   \n",
       "                                                         1.2159 NaN   \n",
       "                                                         1.2845 NaN   \n",
       "                                                         1.3531 NaN   \n",
       "                                                         1.4217 NaN   \n",
       "                                                         1.4903 NaN   \n",
       "                                                         1.5590 NaN   \n",
       "                                                         1.6276 NaN   \n",
       "                                                         1.6962 NaN   \n",
       "                                                         1.7648 NaN   \n",
       "                                                         1.8334 NaN   \n",
       "                                                         1.9021 NaN   \n",
       "                                                         1.9707 NaN   \n",
       "                                                         2.0393 NaN   \n",
       "                                                         2.1079 NaN   \n",
       "                                                         2.1766 NaN   \n",
       "                                                         2.2452 NaN   \n",
       "                                                         2.3138 NaN   \n",
       "                                                         2.3824 NaN   \n",
       "                                                         2.4510 NaN   \n",
       "                                                         2.5197 NaN   \n",
       "                                                         2.5883 NaN   \n",
       "                                                         2.6569 NaN   \n",
       "                                                         2.7255 NaN   \n",
       "                                                         2.7941 NaN   \n",
       "                                                         2.8628 NaN   \n",
       "                                                         2.9314 NaN   \n",
       "\n",
       "                                                                generalization_error_state_evolution  \\\n",
       "                                                                                                mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                                          \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                             0.004890   \n",
       "                                                         1.0786                             0.005659   \n",
       "                                                         1.1472                             0.006898   \n",
       "                                                         1.2159                             0.007798   \n",
       "                                                         1.2845                             0.009274   \n",
       "                                                         1.3531                             0.011071   \n",
       "                                                         1.4217                             0.013168   \n",
       "                                                         1.4903                             0.015463   \n",
       "                                                         1.5590                             0.017745   \n",
       "                                                         1.6276                             0.019719   \n",
       "                                                         1.6962                             0.021135   \n",
       "                                                         1.7648                             0.021897   \n",
       "                                                         1.8334                             0.022068   \n",
       "                                                         1.9021                             0.021798   \n",
       "                                                         1.9707                             0.021243   \n",
       "                                                         2.0393                             0.020532   \n",
       "                                                         2.1079                             0.019754   \n",
       "                                                         2.1766                             0.018970   \n",
       "                                                         2.2452                             0.018218   \n",
       "                                                         2.3138                             0.017518   \n",
       "                                                         2.3824                             0.016880   \n",
       "                                                         2.4510                             0.016307   \n",
       "                                                         2.5197                             0.015796   \n",
       "                                                         2.5883                             0.015349   \n",
       "                                                         2.6569                             0.014972   \n",
       "                                                         2.7255                             0.014712   \n",
       "                                                         2.7941                             0.014524   \n",
       "                                                         2.8628                             0.014366   \n",
       "                                                         2.9314                             0.013789   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta         \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN   \n",
       "                                                         1.0786 NaN   \n",
       "                                                         1.1472 NaN   \n",
       "                                                         1.2159 NaN   \n",
       "                                                         1.2845 NaN   \n",
       "                                                         1.3531 NaN   \n",
       "                                                         1.4217 NaN   \n",
       "                                                         1.4903 NaN   \n",
       "                                                         1.5590 NaN   \n",
       "                                                         1.6276 NaN   \n",
       "                                                         1.6962 NaN   \n",
       "                                                         1.7648 NaN   \n",
       "                                                         1.8334 NaN   \n",
       "                                                         1.9021 NaN   \n",
       "                                                         1.9707 NaN   \n",
       "                                                         2.0393 NaN   \n",
       "                                                         2.1079 NaN   \n",
       "                                                         2.1766 NaN   \n",
       "                                                         2.2452 NaN   \n",
       "                                                         2.3138 NaN   \n",
       "                                                         2.3824 NaN   \n",
       "                                                         2.4510 NaN   \n",
       "                                                         2.5197 NaN   \n",
       "                                                         2.5883 NaN   \n",
       "                                                         2.6569 NaN   \n",
       "                                                         2.7255 NaN   \n",
       "                                                         2.7941 NaN   \n",
       "                                                         2.8628 NaN   \n",
       "                                                         2.9314 NaN   \n",
       "\n",
       "                                                                training_loss_state_evolution  \\\n",
       "                                                                                         mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                                   \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                      0.075974   \n",
       "                                                         1.0786                      0.082966   \n",
       "                                                         1.1472                      0.090698   \n",
       "                                                         1.2159                      0.095910   \n",
       "                                                         1.2845                      0.101425   \n",
       "                                                         1.3531                      0.105987   \n",
       "                                                         1.4217                      0.109340   \n",
       "                                                         1.4903                      0.111247   \n",
       "                                                         1.5590                      0.111559   \n",
       "                                                         1.6276                      0.110293   \n",
       "                                                         1.6962                      0.107668   \n",
       "                                                         1.7648                      0.104044   \n",
       "                                                         1.8334                      0.099814   \n",
       "                                                         1.9021                      0.095307   \n",
       "                                                         1.9707                      0.090785   \n",
       "                                                         2.0393                      0.086405   \n",
       "                                                         2.1079                      0.082266   \n",
       "                                                         2.1766                      0.078414   \n",
       "                                                         2.2452                      0.074880   \n",
       "                                                         2.3138                      0.071658   \n",
       "                                                         2.3824                      0.068736   \n",
       "                                                         2.4510                      0.066093   \n",
       "                                                         2.5197                      0.063704   \n",
       "                                                         2.5883                      0.061552   \n",
       "                                                         2.6569                      0.059613   \n",
       "                                                         2.7255                      0.057957   \n",
       "                                                         2.7941                      0.056714   \n",
       "                                                         2.8628                      0.055839   \n",
       "                                                         2.9314                      0.053507   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta         \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN   \n",
       "                                                         1.0786 NaN   \n",
       "                                                         1.1472 NaN   \n",
       "                                                         1.2159 NaN   \n",
       "                                                         1.2845 NaN   \n",
       "                                                         1.3531 NaN   \n",
       "                                                         1.4217 NaN   \n",
       "                                                         1.4903 NaN   \n",
       "                                                         1.5590 NaN   \n",
       "                                                         1.6276 NaN   \n",
       "                                                         1.6962 NaN   \n",
       "                                                         1.7648 NaN   \n",
       "                                                         1.8334 NaN   \n",
       "                                                         1.9021 NaN   \n",
       "                                                         1.9707 NaN   \n",
       "                                                         2.0393 NaN   \n",
       "                                                         2.1079 NaN   \n",
       "                                                         2.1766 NaN   \n",
       "                                                         2.2452 NaN   \n",
       "                                                         2.3138 NaN   \n",
       "                                                         2.3824 NaN   \n",
       "                                                         2.4510 NaN   \n",
       "                                                         2.5197 NaN   \n",
       "                                                         2.5883 NaN   \n",
       "                                                         2.6569 NaN   \n",
       "                                                         2.7255 NaN   \n",
       "                                                         2.7941 NaN   \n",
       "                                                         2.8628 NaN   \n",
       "                                                         2.9314 NaN   \n",
       "\n",
       "                                                                training_error_state_evolution  \\\n",
       "                                                                                          mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                                    \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                       0.000000   \n",
       "                                                         1.0786                       0.000000   \n",
       "                                                         1.1472                       0.000000   \n",
       "                                                         1.2159                       0.000000   \n",
       "                                                         1.2845                       0.000000   \n",
       "                                                         1.3531                       0.000000   \n",
       "                                                         1.4217                       0.013168   \n",
       "                                                         1.4903                       0.015463   \n",
       "                                                         1.5590                       0.017745   \n",
       "                                                         1.6276                       0.019719   \n",
       "                                                         1.6962                       0.021135   \n",
       "                                                         1.7648                       0.021897   \n",
       "                                                         1.8334                       0.022068   \n",
       "                                                         1.9021                       0.021798   \n",
       "                                                         1.9707                       0.021243   \n",
       "                                                         2.0393                       0.020532   \n",
       "                                                         2.1079                       0.019754   \n",
       "                                                         2.1766                       0.018970   \n",
       "                                                         2.2452                       0.018218   \n",
       "                                                         2.3138                       0.017518   \n",
       "                                                         2.3824                       0.016880   \n",
       "                                                         2.4510                       0.016307   \n",
       "                                                         2.5197                       0.015796   \n",
       "                                                         2.5883                       0.015349   \n",
       "                                                         2.6569                       0.014972   \n",
       "                                                         2.7255                       0.014712   \n",
       "                                                         2.7941                       0.014524   \n",
       "                                                         2.8628                       0.014366   \n",
       "                                                         2.9314                       0.013789   \n",
       "\n",
       "                                                                     ...  \\\n",
       "                                                                std  ...   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta        ...   \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN  ...   \n",
       "                                                         1.0786 NaN  ...   \n",
       "                                                         1.1472 NaN  ...   \n",
       "                                                         1.2159 NaN  ...   \n",
       "                                                         1.2845 NaN  ...   \n",
       "                                                         1.3531 NaN  ...   \n",
       "                                                         1.4217 NaN  ...   \n",
       "                                                         1.4903 NaN  ...   \n",
       "                                                         1.5590 NaN  ...   \n",
       "                                                         1.6276 NaN  ...   \n",
       "                                                         1.6962 NaN  ...   \n",
       "                                                         1.7648 NaN  ...   \n",
       "                                                         1.8334 NaN  ...   \n",
       "                                                         1.9021 NaN  ...   \n",
       "                                                         1.9707 NaN  ...   \n",
       "                                                         2.0393 NaN  ...   \n",
       "                                                         2.1079 NaN  ...   \n",
       "                                                         2.1766 NaN  ...   \n",
       "                                                         2.2452 NaN  ...   \n",
       "                                                         2.3138 NaN  ...   \n",
       "                                                         2.3824 NaN  ...   \n",
       "                                                         2.4510 NaN  ...   \n",
       "                                                         2.5197 NaN  ...   \n",
       "                                                         2.5883 NaN  ...   \n",
       "                                                         2.6569 NaN  ...   \n",
       "                                                         2.7255 NaN  ...   \n",
       "                                                         2.7941 NaN  ...   \n",
       "                                                         2.8628 NaN  ...   \n",
       "                                                         2.9314 NaN  ...   \n",
       "\n",
       "                                                                noiseless_generalization_error_erm_erm  \\\n",
       "                                                                                                  mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                                            \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                                    NaN   \n",
       "                                                         1.0786                                    NaN   \n",
       "                                                         1.1472                                    NaN   \n",
       "                                                         1.2159                                    NaN   \n",
       "                                                         1.2845                                    NaN   \n",
       "                                                         1.3531                                    NaN   \n",
       "                                                         1.4217                                    NaN   \n",
       "                                                         1.4903                                    NaN   \n",
       "                                                         1.5590                                    NaN   \n",
       "                                                         1.6276                                    NaN   \n",
       "                                                         1.6962                                    NaN   \n",
       "                                                         1.7648                                    NaN   \n",
       "                                                         1.8334                                    NaN   \n",
       "                                                         1.9021                                    NaN   \n",
       "                                                         1.9707                                    NaN   \n",
       "                                                         2.0393                                    NaN   \n",
       "                                                         2.1079                                    NaN   \n",
       "                                                         2.1766                                    NaN   \n",
       "                                                         2.2452                                    NaN   \n",
       "                                                         2.3138                                    NaN   \n",
       "                                                         2.3824                                    NaN   \n",
       "                                                         2.4510                                    NaN   \n",
       "                                                         2.5197                                    NaN   \n",
       "                                                         2.5883                                    NaN   \n",
       "                                                         2.6569                                    NaN   \n",
       "                                                         2.7255                                    NaN   \n",
       "                                                         2.7941                                    NaN   \n",
       "                                                         2.8628                                    NaN   \n",
       "                                                         2.9314                                    NaN   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta         \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN   \n",
       "                                                         1.0786 NaN   \n",
       "                                                         1.1472 NaN   \n",
       "                                                         1.2159 NaN   \n",
       "                                                         1.2845 NaN   \n",
       "                                                         1.3531 NaN   \n",
       "                                                         1.4217 NaN   \n",
       "                                                         1.4903 NaN   \n",
       "                                                         1.5590 NaN   \n",
       "                                                         1.6276 NaN   \n",
       "                                                         1.6962 NaN   \n",
       "                                                         1.7648 NaN   \n",
       "                                                         1.8334 NaN   \n",
       "                                                         1.9021 NaN   \n",
       "                                                         1.9707 NaN   \n",
       "                                                         2.0393 NaN   \n",
       "                                                         2.1079 NaN   \n",
       "                                                         2.1766 NaN   \n",
       "                                                         2.2452 NaN   \n",
       "                                                         2.3138 NaN   \n",
       "                                                         2.3824 NaN   \n",
       "                                                         2.4510 NaN   \n",
       "                                                         2.5197 NaN   \n",
       "                                                         2.5883 NaN   \n",
       "                                                         2.6569 NaN   \n",
       "                                                         2.7255 NaN   \n",
       "                                                         2.7941 NaN   \n",
       "                                                         2.8628 NaN   \n",
       "                                                         2.9314 NaN   \n",
       "\n",
       "                                                                noiseless_adversarial_generalization_error_erm  \\\n",
       "                                                                                                          mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                                                    \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                                            NaN   \n",
       "                                                         1.0786                                            NaN   \n",
       "                                                         1.1472                                            NaN   \n",
       "                                                         1.2159                                            NaN   \n",
       "                                                         1.2845                                            NaN   \n",
       "                                                         1.3531                                            NaN   \n",
       "                                                         1.4217                                            NaN   \n",
       "                                                         1.4903                                            NaN   \n",
       "                                                         1.5590                                            NaN   \n",
       "                                                         1.6276                                            NaN   \n",
       "                                                         1.6962                                            NaN   \n",
       "                                                         1.7648                                            NaN   \n",
       "                                                         1.8334                                            NaN   \n",
       "                                                         1.9021                                            NaN   \n",
       "                                                         1.9707                                            NaN   \n",
       "                                                         2.0393                                            NaN   \n",
       "                                                         2.1079                                            NaN   \n",
       "                                                         2.1766                                            NaN   \n",
       "                                                         2.2452                                            NaN   \n",
       "                                                         2.3138                                            NaN   \n",
       "                                                         2.3824                                            NaN   \n",
       "                                                         2.4510                                            NaN   \n",
       "                                                         2.5197                                            NaN   \n",
       "                                                         2.5883                                            NaN   \n",
       "                                                         2.6569                                            NaN   \n",
       "                                                         2.7255                                            NaN   \n",
       "                                                         2.7941                                            NaN   \n",
       "                                                         2.8628                                            NaN   \n",
       "                                                         2.9314                                            NaN   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta         \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN   \n",
       "                                                         1.0786 NaN   \n",
       "                                                         1.1472 NaN   \n",
       "                                                         1.2159 NaN   \n",
       "                                                         1.2845 NaN   \n",
       "                                                         1.3531 NaN   \n",
       "                                                         1.4217 NaN   \n",
       "                                                         1.4903 NaN   \n",
       "                                                         1.5590 NaN   \n",
       "                                                         1.6276 NaN   \n",
       "                                                         1.6962 NaN   \n",
       "                                                         1.7648 NaN   \n",
       "                                                         1.8334 NaN   \n",
       "                                                         1.9021 NaN   \n",
       "                                                         1.9707 NaN   \n",
       "                                                         2.0393 NaN   \n",
       "                                                         2.1079 NaN   \n",
       "                                                         2.1766 NaN   \n",
       "                                                         2.2452 NaN   \n",
       "                                                         2.3138 NaN   \n",
       "                                                         2.3824 NaN   \n",
       "                                                         2.4510 NaN   \n",
       "                                                         2.5197 NaN   \n",
       "                                                         2.5883 NaN   \n",
       "                                                         2.6569 NaN   \n",
       "                                                         2.7255 NaN   \n",
       "                                                         2.7941 NaN   \n",
       "                                                         2.8628 NaN   \n",
       "                                                         2.9314 NaN   \n",
       "\n",
       "                                                                noiseless_angle_to_generalisation_erm  \\\n",
       "                                                                                                 mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                                           \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                                   NaN   \n",
       "                                                         1.0786                                   NaN   \n",
       "                                                         1.1472                                   NaN   \n",
       "                                                         1.2159                                   NaN   \n",
       "                                                         1.2845                                   NaN   \n",
       "                                                         1.3531                                   NaN   \n",
       "                                                         1.4217                                   NaN   \n",
       "                                                         1.4903                                   NaN   \n",
       "                                                         1.5590                                   NaN   \n",
       "                                                         1.6276                                   NaN   \n",
       "                                                         1.6962                                   NaN   \n",
       "                                                         1.7648                                   NaN   \n",
       "                                                         1.8334                                   NaN   \n",
       "                                                         1.9021                                   NaN   \n",
       "                                                         1.9707                                   NaN   \n",
       "                                                         2.0393                                   NaN   \n",
       "                                                         2.1079                                   NaN   \n",
       "                                                         2.1766                                   NaN   \n",
       "                                                         2.2452                                   NaN   \n",
       "                                                         2.3138                                   NaN   \n",
       "                                                         2.3824                                   NaN   \n",
       "                                                         2.4510                                   NaN   \n",
       "                                                         2.5197                                   NaN   \n",
       "                                                         2.5883                                   NaN   \n",
       "                                                         2.6569                                   NaN   \n",
       "                                                         2.7255                                   NaN   \n",
       "                                                         2.7941                                   NaN   \n",
       "                                                         2.8628                                   NaN   \n",
       "                                                         2.9314                                   NaN   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta         \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN   \n",
       "                                                         1.0786 NaN   \n",
       "                                                         1.1472 NaN   \n",
       "                                                         1.2159 NaN   \n",
       "                                                         1.2845 NaN   \n",
       "                                                         1.3531 NaN   \n",
       "                                                         1.4217 NaN   \n",
       "                                                         1.4903 NaN   \n",
       "                                                         1.5590 NaN   \n",
       "                                                         1.6276 NaN   \n",
       "                                                         1.6962 NaN   \n",
       "                                                         1.7648 NaN   \n",
       "                                                         1.8334 NaN   \n",
       "                                                         1.9021 NaN   \n",
       "                                                         1.9707 NaN   \n",
       "                                                         2.0393 NaN   \n",
       "                                                         2.1079 NaN   \n",
       "                                                         2.1766 NaN   \n",
       "                                                         2.2452 NaN   \n",
       "                                                         2.3138 NaN   \n",
       "                                                         2.3824 NaN   \n",
       "                                                         2.4510 NaN   \n",
       "                                                         2.5197 NaN   \n",
       "                                                         2.5883 NaN   \n",
       "                                                         2.6569 NaN   \n",
       "                                                         2.7255 NaN   \n",
       "                                                         2.7941 NaN   \n",
       "                                                         2.8628 NaN   \n",
       "                                                         2.9314 NaN   \n",
       "\n",
       "                                                                A_over_sqrt_qN_erm  \\\n",
       "                                                                              mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                        \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                NaN   \n",
       "                                                         1.0786                NaN   \n",
       "                                                         1.1472                NaN   \n",
       "                                                         1.2159                NaN   \n",
       "                                                         1.2845                NaN   \n",
       "                                                         1.3531                NaN   \n",
       "                                                         1.4217                NaN   \n",
       "                                                         1.4903                NaN   \n",
       "                                                         1.5590                NaN   \n",
       "                                                         1.6276                NaN   \n",
       "                                                         1.6962                NaN   \n",
       "                                                         1.7648                NaN   \n",
       "                                                         1.8334                NaN   \n",
       "                                                         1.9021                NaN   \n",
       "                                                         1.9707                NaN   \n",
       "                                                         2.0393                NaN   \n",
       "                                                         2.1079                NaN   \n",
       "                                                         2.1766                NaN   \n",
       "                                                         2.2452                NaN   \n",
       "                                                         2.3138                NaN   \n",
       "                                                         2.3824                NaN   \n",
       "                                                         2.4510                NaN   \n",
       "                                                         2.5197                NaN   \n",
       "                                                         2.5883                NaN   \n",
       "                                                         2.6569                NaN   \n",
       "                                                         2.7255                NaN   \n",
       "                                                         2.7941                NaN   \n",
       "                                                         2.8628                NaN   \n",
       "                                                         2.9314                NaN   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta         \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN   \n",
       "                                                         1.0786 NaN   \n",
       "                                                         1.1472 NaN   \n",
       "                                                         1.2159 NaN   \n",
       "                                                         1.2845 NaN   \n",
       "                                                         1.3531 NaN   \n",
       "                                                         1.4217 NaN   \n",
       "                                                         1.4903 NaN   \n",
       "                                                         1.5590 NaN   \n",
       "                                                         1.6276 NaN   \n",
       "                                                         1.6962 NaN   \n",
       "                                                         1.7648 NaN   \n",
       "                                                         1.8334 NaN   \n",
       "                                                         1.9021 NaN   \n",
       "                                                         1.9707 NaN   \n",
       "                                                         2.0393 NaN   \n",
       "                                                         2.1079 NaN   \n",
       "                                                         2.1766 NaN   \n",
       "                                                         2.2452 NaN   \n",
       "                                                         2.3138 NaN   \n",
       "                                                         2.3824 NaN   \n",
       "                                                         2.4510 NaN   \n",
       "                                                         2.5197 NaN   \n",
       "                                                         2.5883 NaN   \n",
       "                                                         2.6569 NaN   \n",
       "                                                         2.7255 NaN   \n",
       "                                                         2.7941 NaN   \n",
       "                                                         2.8628 NaN   \n",
       "                                                         2.9314 NaN   \n",
       "\n",
       "                                                                m_over_sqrt_rhoq_minus_m2_erm  \\\n",
       "                                                                                         mean   \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta                                   \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100                           NaN   \n",
       "                                                         1.0786                           NaN   \n",
       "                                                         1.1472                           NaN   \n",
       "                                                         1.2159                           NaN   \n",
       "                                                         1.2845                           NaN   \n",
       "                                                         1.3531                           NaN   \n",
       "                                                         1.4217                           NaN   \n",
       "                                                         1.4903                           NaN   \n",
       "                                                         1.5590                           NaN   \n",
       "                                                         1.6276                           NaN   \n",
       "                                                         1.6962                           NaN   \n",
       "                                                         1.7648                           NaN   \n",
       "                                                         1.8334                           NaN   \n",
       "                                                         1.9021                           NaN   \n",
       "                                                         1.9707                           NaN   \n",
       "                                                         2.0393                           NaN   \n",
       "                                                         2.1079                           NaN   \n",
       "                                                         2.1766                           NaN   \n",
       "                                                         2.2452                           NaN   \n",
       "                                                         2.3138                           NaN   \n",
       "                                                         2.3824                           NaN   \n",
       "                                                         2.4510                           NaN   \n",
       "                                                         2.5197                           NaN   \n",
       "                                                         2.5883                           NaN   \n",
       "                                                         2.6569                           NaN   \n",
       "                                                         2.7255                           NaN   \n",
       "                                                         2.7941                           NaN   \n",
       "                                                         2.8628                           NaN   \n",
       "                                                         2.9314                           NaN   \n",
       "\n",
       "                                                                     \n",
       "                                                                std  \n",
       "alpha     epsilon tau  lam   problem_type attack_epsilon beta        \n",
       "1000000.0 0.2     0.05 0.001 Logistic     0.2            1.0100 NaN  \n",
       "                                                         1.0786 NaN  \n",
       "                                                         1.1472 NaN  \n",
       "                                                         1.2159 NaN  \n",
       "                                                         1.2845 NaN  \n",
       "                                                         1.3531 NaN  \n",
       "                                                         1.4217 NaN  \n",
       "                                                         1.4903 NaN  \n",
       "                                                         1.5590 NaN  \n",
       "                                                         1.6276 NaN  \n",
       "                                                         1.6962 NaN  \n",
       "                                                         1.7648 NaN  \n",
       "                                                         1.8334 NaN  \n",
       "                                                         1.9021 NaN  \n",
       "                                                         1.9707 NaN  \n",
       "                                                         2.0393 NaN  \n",
       "                                                         2.1079 NaN  \n",
       "                                                         2.1766 NaN  \n",
       "                                                         2.2452 NaN  \n",
       "                                                         2.3138 NaN  \n",
       "                                                         2.3824 NaN  \n",
       "                                                         2.4510 NaN  \n",
       "                                                         2.5197 NaN  \n",
       "                                                         2.5883 NaN  \n",
       "                                                         2.6569 NaN  \n",
       "                                                         2.7255 NaN  \n",
       "                                                         2.7941 NaN  \n",
       "                                                         2.8628 NaN  \n",
       "                                                         2.9314 NaN  \n",
       "\n",
       "[29 rows x 214 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd = gd.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"beta\"]) #,\"p_calibration\"\n",
    "state_evolution = state_evolution.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"beta\"]) #,\"p_calibration\"\n",
    "\n",
    "# drop id, code_version, experiment_id, date, initial_condition, test_against_epsilons, calibrations, abs_tol, min_iter,max_iter,blend_fpe,int_lims,subspace_overlaps\n",
    "state_evolution = state_evolution.drop(columns=[\"id\",\"code_version\",\"experiment_id\",\"date\",\"initial_condition\",\"test_against_epsilons\",\"calibrations\",\"abs_tol\",\"min_iter\",\"max_iter\",\"blend_fpe\",\"int_lims\",\"subspace_overlaps\",\"data_model_type\",\"data_model_description\",\"data_model_name\"])\n",
    "# drop id, code_version, experiment_id, test_against_epsilons, date, subspace_overlaps, analytical_calibrations, erm_calibrations, \n",
    "gd = gd.drop(columns=[\"id\",\"code_version\",\"experiment_id\",\"test_against_epsilons\",\"date\",\"subspace_overlaps\",\"analytical_calibrations\",\"erm_calibrations\",\"data_model_type\",\"data_model_description\",\"data_model_name\"])\n",
    "\n",
    "state_evolution.columns = [col+\"_state_evolution\" for col in state_evolution.columns]\n",
    "gd.columns = [col+\"_erm\" for col in gd.columns]\n",
    "\n",
    "state_evolution = state_evolution.groupby(level=[0,1,2,3,4,5,6]).agg([\"mean\",\"std\"]) #,4\n",
    "gd = gd.groupby(level=[0,1,2,3,4,5,6]).agg([\"mean\",\"std\"]) #,4\n",
    "df = state_evolution.join(gd, how=\"outer\")\n",
    "df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataframe as a pickle file\n",
    "if not os.path.exists(\"Pickles\"):\n",
    "    os.makedirs(\"Pickles\")\n",
    "if not os.path.exists(\"Pickles/powerlaw_beta.pkl\"):\n",
    "    df.to_pickle(\"Pickles/powerlaw_beta.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle\n",
    "df = pd.read_pickle(\"Pickles/powerlaw_beta.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique epsilons\n",
    "epsilons = df.index.get_level_values(\"epsilon\").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = df.index.get_level_values(\"alpha\").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of dataframes for each data_model_name\n",
    "df_dict = {}\n",
    "for epsilon in epsilons:\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    eps_df = df.xs(epsilon, level=\"epsilon\")\n",
    "\n",
    "\n",
    "\n",
    "    # for each data_model_name, create a dictionary\n",
    "    eps_dict = {}\n",
    "\n",
    "\n",
    "    betas = eps_df.index.get_level_values(\"beta\").unique()\n",
    "    adversarial_error_0 = eps_df[\"adversarial_generalization_error_state_evolution\"][\"mean\"].values\n",
    "    generalization_error_0 = eps_df[\"generalization_error_state_evolution\"][\"mean\"].values\n",
    "    boundary_error_0 = eps_df[\"difference_adv_gen_state_evolution\"][\"mean\"].values\n",
    "    class_preserving = eps_df[\"fair_adversarial_error_state_evolution\"][\"mean\"].values\n",
    "\n",
    "    adversarial_error_erm = eps_df[\"adversarial_generalization_error_erm\"][\"mean\"].values\n",
    "    generalization_error_erm = eps_df[\"generalization_error_erm_erm\"][\"mean\"].values\n",
    "    boundary_error_erm = eps_df[\"difference_adv_gen_erm\"][\"mean\"].values\n",
    "    class_preserving_erm = eps_df[\"fair_adversarial_error_erm\"][\"mean\"].values\n",
    "\n",
    "    adversarial_error_erm_std = eps_df[\"adversarial_generalization_error_erm\"][\"std\"].values\n",
    "    generalization_error_erm_std = eps_df[\"generalization_error_erm_erm\"][\"std\"].values\n",
    "    boundary_error_erm_std = eps_df[\"difference_adv_gen_erm\"][\"std\"].values\n",
    "    class_preserving_erm_std = eps_df[\"fair_adversarial_error_erm\"][\"std\"].values\n",
    "\n",
    "    betas = np.array(betas)\n",
    "    adversarial_error_0 = np.array(adversarial_error_0)\n",
    "    generalization_error_0 = np.array(generalization_error_0)\n",
    "    boundary_error_0 = np.array(boundary_error_0)\n",
    "    class_preserving = np.array(class_preserving)\n",
    "\n",
    "    adversarial_error_erm = np.array(adversarial_error_erm)\n",
    "    generalization_error_erm = np.array(generalization_error_erm)\n",
    "    boundary_error_erm = np.array(boundary_error_erm)\n",
    "    class_preserving_erm = np.array(class_preserving_erm)\n",
    "\n",
    "    adversarial_error_erm_std = np.array(adversarial_error_erm_std)\n",
    "    generalization_error_erm_std = np.array(generalization_error_erm_std)\n",
    "    boundary_error_erm_std = np.array(boundary_error_erm_std)\n",
    "    class_preserving_erm_std = np.array(class_preserving_erm_std)\n",
    "\n",
    "    eps_0_dict = {}\n",
    "    eps_0_dict[\"betas\"] = betas\n",
    "    eps_0_dict[\"adversarial_error\"] = adversarial_error_0\n",
    "    eps_0_dict[\"generalization_error\"] = generalization_error_0\n",
    "    eps_0_dict[\"boundary_error\"] = boundary_error_0\n",
    "    eps_0_dict[\"class_preserving\"] = class_preserving\n",
    "\n",
    "    eps_0_dict[\"adversarial_error_erm\"] = adversarial_error_erm\n",
    "    eps_0_dict[\"generalization_error_erm\"] = generalization_error_erm\n",
    "    eps_0_dict[\"boundary_error_erm\"] = boundary_error_erm\n",
    "    eps_0_dict[\"class_preserving_erm\"] = class_preserving_erm\n",
    "\n",
    "    eps_0_dict[\"adversarial_error_erm_std\"] = adversarial_error_erm_std\n",
    "    eps_0_dict[\"generalization_error_erm_std\"] = generalization_error_erm_std\n",
    "    eps_0_dict[\"boundary_error_erm_std\"] = boundary_error_erm_std\n",
    "    eps_0_dict[\"class_preserving_erm_std\"] = class_preserving_erm_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_dict[epsilon] = eps_0_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 425x270 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "IMG_DIRECTORY = \"./Assets/powerlawBeta\"\n",
    "if not os.path.exists(IMG_DIRECTORY):\n",
    "    os.makedirs(IMG_DIRECTORY)\n",
    "\n",
    "\n",
    "def save_plot(fig, name, formats=[\"pdf\",\"jpg\"], date=False):\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for f in formats:        \n",
    "        path = \"{}\".format(name) + \"_{}\".format(current_date) + \".\" + f\n",
    "        if not date:\n",
    "            path = \"{}\".format(name) + \".\" + f\n",
    "        fig.savefig(            \n",
    "            os.path.join(IMG_DIRECTORY, path),\n",
    "            format=f,\n",
    "        )\n",
    "\n",
    "\n",
    "def set_size(width, fraction=1, subplots=(1, 1)):\n",
    "    if width == \"thesis\":\n",
    "        width_pt = 426.79135\n",
    "    elif width == \"beamer\":\n",
    "        width_pt = 307.28987\n",
    "    else:\n",
    "        width_pt = width\n",
    "\n",
    "    fig_width_pt = width_pt * fraction\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    golden_ratio = (5**0.5 - 1) / 2\n",
    "\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    fig_height_in = fig_width_in * (golden_ratio) * (subplots[0] / subplots[1])\n",
    "\n",
    "    return (fig_width_in, fig_height_in)\n",
    "\n",
    "\n",
    "width = 1.25 * 458.63788\n",
    "\n",
    "plt.style.use(\"../latex_ready.mplstyle\")\n",
    "\n",
    "tuple_size = set_size(width, fraction=0.50)\n",
    "\n",
    "tuple_size = (4.25,2.7)\n",
    "\n",
    "multiplier = 1.2\n",
    "second_multiplier = 0.7\n",
    "\n",
    "\n",
    "# import Line2D for custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=1,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    figsize=(tuple_size[0], tuple_size[1]),\n",
    "    gridspec_kw={\"hspace\": 0,\"wspace\": 0},\n",
    ")\n",
    "\n",
    "\n",
    "# ICML adjustments\n",
    "fig.subplots_adjust(left=0.11)\n",
    "fig.subplots_adjust(bottom=0.12)\n",
    "fig.subplots_adjust(top=0.83)\n",
    "fig.subplots_adjust(right=0.97)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a custom legend\n",
    "custom_legend = []\n",
    "\n",
    "linestyles = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for df_idx, (epsilon, value) in enumerate(df_dict.items()):\n",
    "\n",
    "    eps_dict = value\n",
    "\n",
    "    \n",
    "\n",
    "    betas = eps_dict[\"betas\"]\n",
    "    adversarial_error = eps_dict[\"adversarial_error\"]\n",
    "    generalization_error = eps_dict[\"generalization_error\"]\n",
    "    boundary_error = eps_dict[\"boundary_error\"]\n",
    "    class_preserving = eps_dict[\"class_preserving\"]\n",
    "\n",
    "    adversarial_error_erm = eps_dict[\"adversarial_error_erm\"]\n",
    "    generalization_error_erm = eps_dict[\"generalization_error_erm\"]\n",
    "    boundary_error_erm = eps_dict[\"boundary_error_erm\"]\n",
    "    class_preserving_erm = eps_dict[\"class_preserving_erm\"]\n",
    "\n",
    "    adversarial_error_erm_std = eps_dict[\"adversarial_error_erm_std\"]\n",
    "    generalization_error_erm_std = eps_dict[\"generalization_error_erm_std\"]\n",
    "    boundary_error_erm_std = eps_dict[\"boundary_error_erm_std\"]\n",
    "    class_preserving_erm_std = eps_dict[\"class_preserving_erm_std\"]\n",
    "\n",
    "\n",
    "    adversarial_lines = axes.plot(betas, adversarial_error, linestyle=linestyles[df_idx],color=\"C0\")\n",
    "    axes.plot(betas, generalization_error, linestyle=linestyles[df_idx],color=\"C1\")\n",
    "    axes.plot(betas, boundary_error,linestyle=linestyles[df_idx], color=\"C2\")\n",
    "    # axes.plot(betas, class_preserving,linestyle=linestyles[df_idx], color=\"C3\")\n",
    "\n",
    "    # custom_legend.append(Line2D([0],[0],color=\"C0\", linestyle=linestyles[df_idx], label=r\"$E_{{\\mathrm{{adv}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "    # custom_legend.append(Line2D([0],[0],color=\"C1\", linestyle=linestyles[df_idx], label=r\"$E_{{\\mathrm{{gen}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "    # custom_legend.append(Line2D([0],[0],color=\"C2\", linestyle=linestyles[df_idx], label=r\"$E_{{\\mathrm{{bound}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "    # custom_legend.append(Line2D([0],[0],color=\"C3\", linestyle=linestyles[df_idx], label=r\"$E_{{\\mathrm{{CP}}}}, \\beta={}$\".format(beta)))\n",
    "\n",
    "    # axs.errorbar(\n",
    "    #     betas,\n",
    "    #     adversarial_error_erm,\n",
    "    #     yerr=adversarial_error_erm_std,\n",
    "    #     fmt=\".\",\n",
    "    #     markersize=1,\n",
    "    #     color=\"C0\"\n",
    "    # )\n",
    "    # axs.errorbar(\n",
    "    #     betas,\n",
    "    #     generalization_error_erm,\n",
    "    #     yerr=generalization_error_erm_std,\n",
    "    #     fmt=\".\",\n",
    "    #     markersize=1,\n",
    "    #     color=\"C1\"\n",
    "    # )\n",
    "    # axs.errorbar(\n",
    "    #     betas,\n",
    "    #     boundary_error_erm,\n",
    "    #     yerr=boundary_error_erm_std,\n",
    "    #     fmt=\".\",\n",
    "    #     markersize=1,\n",
    "    #     color=\"C2\"\n",
    "    # )\n",
    "    # axs.errorbar(\n",
    "    #     betas,\n",
    "    #     class_preserving_erm,\n",
    "    #     yerr=class_preserving_erm_std,\n",
    "    #     fmt=\".\",\n",
    "    #     markersize=1,\n",
    "    #     color=\"C3\"\n",
    "    # )\n",
    "\n",
    "\n",
    "# axes.set_xscale(\"log\")\n",
    "axes.set_yscale(\"log\")\n",
    "\n",
    "\n",
    "\n",
    "axes.set_ylabel(r\"$E$\", labelpad=2.0)\n",
    "\n",
    "\n",
    "axes.set_xlabel(r\"$\\beta$\", labelpad=2.0)\n",
    "axes.grid(which=\"both\", axis=\"both\", alpha=0.5)\n",
    "# Set the major ticks to face inwards\n",
    "axes.tick_params(axis='both', which='major', direction='in')\n",
    "\n",
    "# Set the minor ticks to face inwards\n",
    "axes.tick_params(axis='both', which='minor', direction='in')\n",
    "\n",
    "error_legend = []\n",
    "\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{adv}}$\",color=\"C0\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{gen}}$\",color=\"C1\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{bound}}$\",color=\"C2\"))\n",
    "\n",
    "epsilon_legend = []\n",
    "\n",
    "for idx, epsilon in enumerate(epsilons):\n",
    "    epsilon_legend.append(Line2D([0],[0],color=\"black\", linestyle=linestyles[idx], label=r\"$\\varepsilon_t={}$\".format(epsilons[idx]))) \n",
    "\n",
    "custom_legend = []\n",
    "\n",
    "for idx in range(len(error_legend)-1):\n",
    "    custom_legend.append(error_legend[idx])\n",
    "    custom_legend.append(epsilon_legend[idx])\n",
    "\n",
    "custom_legend.append(error_legend[-1])\n",
    "\n",
    "\n",
    "# Place the legend at the bottom of the figure\n",
    "fig.legend(handles=custom_legend, loc='upper center', ncol=3)\n",
    "\n",
    "save = True\n",
    "if save:\n",
    "    save_plot(\n",
    "        fig,\n",
    "        f\"powerlawbeta\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
