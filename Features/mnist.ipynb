{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import sklearn_optimize\n",
    "from data import sample_weights\n",
    "import numpy as np\n",
    "# Let's see how well we do on the test set\n",
    "from theoretical import predict_erm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_model import CustomSpectra\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a subset of the data and learn to distinguish between two classes\n",
    "# Let's pick t-shirt 0 from sneaker 7\n",
    "X_train = X_train[np.logical_or(y_train == 0, y_train == 7)]\n",
    "y_train = y_train[np.logical_or(y_train == 0, y_train == 7)]\n",
    "X_test = X_test[np.logical_or(y_test == 0, y_test == 7)]\n",
    "y_test = y_test[np.logical_or(y_test == 0, y_test == 7)]\n",
    "\n",
    "# change the datatype to int64\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "y_test = y_test.astype(np.float64)\n",
    "y_train = y_train.astype(np.float64)\n",
    "\n",
    "\n",
    "# Compute the standard deviation and replace 0 with 1\n",
    "std = X_train.std(axis=0)\n",
    "std[std == 0] = 1\n",
    "std_test = X_test.std(axis=0)\n",
    "std_test[std_test == 0] = 1\n",
    "\n",
    "# Subtract the mean and divide by the standard deviation\n",
    "X_train = (X_train - X_train.mean(axis=0)) / std\n",
    "X_test = (X_test - X_test.mean(axis=0)) / std_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 7.]), array([6000, 6000]))\n",
      "(array([0., 7.]), array([1000, 1000]))\n"
     ]
    }
   ],
   "source": [
    "# Do we have the same number of examples for each class?\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True))\n",
    "# Yes, we do!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make the labels binary\n",
    "y_train[y_train == 0] = -1\n",
    "y_train[y_train == 7] = 1\n",
    "y_test[y_test == 0] = -1\n",
    "y_test[y_test == 7] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put the data into a json object and pickle it\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "data = {\n",
    "    'X_train': X_train.tolist(),\n",
    "    'y_train': y_train.tolist(),\n",
    "    'X_test': X_test.tolist(),\n",
    "    'y_test': y_test.tolist()\n",
    "}\n",
    "\n",
    "with open('data/fashion_mnist.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# Let's pickle the data\n",
    "with open('data/fashion_mnist.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the pickle file\n",
    "with open('data/fashion_mnist.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the json file\n",
    "with open('data/fashion_mnist.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntot = 12000\n",
    "Omega = X_train.T @ X_train / ntot # student-student\n",
    "rho = y_train.dot(y_train) / ntot\n",
    "spec_Omega, U = np.linalg.eigh(Omega)\n",
    "diagUtPhiPhitU = np.diag(1/ntot * U.T @ X_train.T @ y_train.reshape(ntot,1) @ \n",
    "                    y_train.reshape(1,ntot) @ X_train @ U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add Omega, rho, spec_Omega and diagUtPhiPhitU to the data\n",
    "data['Omega'] = Omega.tolist()\n",
    "data['rho'] = rho.tolist()\n",
    "data['spec_Omega'] = spec_Omega.tolist()\n",
    "data['diagUtPhiPhitU'] = diagUtPhiPhitU.tolist()\n",
    "\n",
    "# Let's pickle the data\n",
    "with open('data/fashion_mnist.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  1.]), array([6000, 6000]))"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get nunique for y_train\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of weights: 2.600423\n",
      "Accuracy: 0.997000\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs',max_iter=1000,C=1/lam).fit(X_train, y_train)\n",
    "w_lr = clf.coef_.flatten()\n",
    "\n",
    "print(\"Norm of weights: %f\" % np.linalg.norm(w_lr))\n",
    "y_pred = predict_erm(X_test, w_lr)\n",
    "print(\"Accuracy: %f\" % accuracy_score(y_test, y_pred))\n",
    "\n",
    "# print the regularization parameter\n",
    "print(clf.C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of the weights: 2.752518\n"
     ]
    }
   ],
   "source": [
    "w_gd = sklearn_optimize(sample_weights(784),X_train,y_train,lam,0)\n",
    "print(\"Norm of the weights: %f\" % np.linalg.norm(w_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of the weights: 28.939971\n"
     ]
    }
   ],
   "source": [
    "epsilon = 10\n",
    "w_gd_adv = sklearn_optimize(sample_weights(784),X_train,y_train,lam,epsilon)\n",
    "print(\"Norm of the weights: %f\" % np.linalg.norm(w_gd_adv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set with gradient descent: 0.9995\n",
      "Accuracy on test set with adversarial gradient descent: 0.9995\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_pred = predict_erm(X_test, w_gd)\n",
    "print(\"Accuracy on test set with gradient descent: {}\".format(accuracy_score(y_test,y_pred)))\n",
    "\n",
    "y_pred = predict_erm(X_test, w_gd_adv)\n",
    "print(\"Accuracy on test set with adversarial gradient descent: {}\".format(accuracy_score(y_test,y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compute adversarial examples for the test set\n",
    "# The best attack is given by -y*epsilon* w / ||w||_2\n",
    "# Let's compute it\n",
    "epsilon = 3\n",
    "w_lr_norm = np.linalg.norm(w_lr)\n",
    "# Let's compute the adversarial examples\n",
    "X_test_adv = X_test - epsilon * y_test[:, np.newaxis] * w_lr / w_lr_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set with adversarial examples: 0.9735\n",
      "Accuracy on test set with adversarial examples: 0.994\n",
      "Accuracy on test set with adversarial examples: 0.9935\n"
     ]
    }
   ],
   "source": [
    "# Let's see how well we do on the test set\n",
    "y_pred = predict_erm(X_test_adv, w_lr)\n",
    "print(\"Accuracy on test set with adversarial examples: {}\".format(accuracy_score(y_test,y_pred)))\n",
    "# using gradient descent\n",
    "y_pred = predict_erm(X_test_adv, w_gd)\n",
    "print(\"Accuracy on test set with adversarial examples: {}\".format(accuracy_score(y_test,y_pred)))\n",
    "# using adversarial gradient descent\n",
    "y_pred = predict_erm(X_test_adv, w_gd_adv)\n",
    "print(\"Accuracy on test set with adversarial examples: {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher features\n",
    "p = 12000\n",
    "gamma = 12000/784\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing covariances\n",
      "Computing rho\n",
      "Diagonalising the student-student covariance\n",
      "Projecting teacher-student correlation on student-student basis\n"
     ]
    }
   ],
   "source": [
    "# Student-student covariance\n",
    "print('Computing covariances')\n",
    "Omega = X_train.T @ X_train / p # student-student\n",
    "\n",
    "print('Computing rho')\n",
    "rho = y_train.dot(y_train) / p\n",
    "\n",
    "print('Diagonalising the student-student covariance')\n",
    "spec_Omega, U = np.linalg.eigh(Omega)\n",
    "\n",
    "print('Projecting teacher-student correlation on student-student basis')\n",
    "diagUtPhiPhitU = np.diag(1/p * U.T @ X_train.T @ y_train.reshape(p,1) @ \n",
    "                         y_train.reshape(1,p) @ X_train @ U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = CustomSpectra(gamma = gamma,\n",
    "                           rho = rho, \n",
    "                           spec_Omega = spec_Omega, \n",
    "                           diagonal_term = diagUtPhiPhitU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Scattering as in Loureiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    '''\n",
    "    Preprocess MNIST data set: \n",
    "    center, divide by global variance and assign labels +1 for even, -1 for odd digits.\n",
    "    \n",
    "    Args:\n",
    "        - pyTorch dataset\n",
    "    \n",
    "    Return:\n",
    "        - tuple (X,y) where both X and y are numpy arrays.\n",
    "    '''\n",
    "    n_samples, _, _ = dataset.data.shape\n",
    "    X = torch.clone(dataset.data).float()\n",
    "    y = torch.clone(dataset.targets).view(n_samples,).float()\n",
    "    \n",
    "    data, labels = [], []\n",
    "    # Extract digits and create labels\n",
    "    for k,label in enumerate(y):\n",
    "        if label in [0, 2, 4, 6, 8]:\n",
    "            data.append(X[k].numpy())\n",
    "            labels.append(1)\n",
    "        elif label in [1, 3, 5, 7, 9]:\n",
    "            data.append(X[k].numpy())\n",
    "            labels.append(-1)\n",
    "            \n",
    "    data = np.array(data)\n",
    "    data -= data.mean(axis=0)\n",
    "    data /= data.std()\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 17327133.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 29119157.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 12369088.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 15941865.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST \n",
    "mnist = datasets.MNIST(root='data', train=True, download=True, transform=None)\n",
    "\n",
    "# Pre-process\n",
    "C, y = preprocess_data(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kymatio.sklearn import Scattering2D\n",
    "\n",
    "ntot, dx, dy = C.shape\n",
    "S = Scattering2D(shape=(dx,dy), J=3, L=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature space dimension: 1953\n"
     ]
    }
   ],
   "source": [
    "# Data in feature spac\n",
    "V = S(C).reshape(ntot, -1)\n",
    "\n",
    "_, d = V.shape\n",
    "\n",
    "print('Feature space dimension: {}'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher features\n",
    "p = ntot\n",
    "gamma = p/d\n",
    "\n",
    "# l2 regularisation parameter\n",
    "lamb = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing covariances\n"
     ]
    }
   ],
   "source": [
    "# Student-student covariance\n",
    "print('Computing covariances')\n",
    "Omega = V.T @ V / ntot # student-student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rho\n",
      "Diagonalising the student-student covariance\n",
      "Projecting teacher-student correlation on student-student basis\n"
     ]
    }
   ],
   "source": [
    "print('Computing rho')\n",
    "rho = y.dot(y) / ntot\n",
    "\n",
    "print('Diagonalising the student-student covariance')\n",
    "spec_Omega, U = np.linalg.eigh(Omega)\n",
    "\n",
    "print('Projecting teacher-student correlation on student-student basis')\n",
    "diagUtPhiPhitU = np.diag(1/ntot * U.T @ V.T @ y.reshape(p,1) @ \n",
    "                         y.reshape(1,p) @ V @ U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data json object\n",
    "data = {\n",
    "    'X_train': V.tolist(),\n",
    "    'y_train': y.tolist(),\n",
    "    'Omega': Omega.tolist(),\n",
    "    'rho': rho.tolist(),\n",
    "    'spec_Omega': spec_Omega.tolist(),\n",
    "    'diagUtPhiPhitU': diagUtPhiPhitU.tolist()\n",
    "}\n",
    "\n",
    "# Let's pickle the data\n",
    "with open('data/mnist.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
