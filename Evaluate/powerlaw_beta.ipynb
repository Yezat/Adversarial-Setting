{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"../experiments\")\n",
    "\n",
    "\n",
    "from experiment_information import *\n",
    "from experiment_setup import *\n",
    "from data import *\n",
    "from helpers import *\n",
    "from data_loading import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code assumes that you have defined and run an experiment before using `define_experiment.ipynb` in the `experiments` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiments, df_state_evolution, df_erm = obtain_dataframes(logger)\n",
    "df_experiments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_loc = 0\n",
    "\n",
    "# extract and print the top experiment_id\n",
    "experiment_id = df_experiments.iloc[experiment_loc][\"experiment_id\"]\n",
    "print(experiment_id)\n",
    "\n",
    "# extract and print the experiment type\n",
    "experiment_type = df_experiments.iloc[experiment_loc][\"experiment_type\"]\n",
    "print(experiment_type)\n",
    "\n",
    "# extract and print the data model type and data_model name used\n",
    "data_model_types = df_experiments.iloc[experiment_loc][\"data_model_types\"]\n",
    "# convert it to the enum\n",
    "data_model_types = [DataModelType[data_model_type] for data_model_type in json.loads(data_model_types)]\n",
    "data_model_names = [name for name in json.loads(df_experiments.iloc[experiment_loc][\"data_model_names\"])]\n",
    "data_model_descriptions = df_experiments.iloc[experiment_loc][\"data_model_descriptions\"]\n",
    "for data_model_type in data_model_types:\n",
    "    print(data_model_type.name)\n",
    "print(data_model_names)\n",
    "print(data_model_descriptions)\n",
    "\n",
    "# print the experiment name\n",
    "experiment_name = df_experiments.iloc[experiment_loc][\"experiment_name\"]\n",
    "print(experiment_name)\n",
    "\n",
    "# print the experiment problem types\n",
    "experiment_problem_types = df_experiments.iloc[experiment_loc][\"problem_types\"]\n",
    "experiment_problem_types = json.loads(experiment_problem_types)\n",
    "print(experiment_problem_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_name_dict = {}\n",
    "data_model_name_dict[\"VanillaGaussian\"] = \"Vanilla Gaussian\"\n",
    "data_model_name_dict[\"2_VanillaGaussian\"] = \"Vanilla Gaussian\"\n",
    "data_model_name_dict[\"VanillaGaussianThetaFirst\"] = \"Vanilla Gaussian - Teacher 10:1\"\n",
    "data_model_name_dict[\"VanillaGaussianTimes10\"] = \"Vanilla Gaussian x10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[1 1]\"] = \"Strong Weak 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1 1]\"] = \"Strong Weak 5:1\"\n",
    "data_model_name_dict[\"2_KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1 1]\"] = \"Strong Weak 5:1\"\n",
    "data_model_name_dict[\"2_KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[1 1]\"] = \"Strong Weak 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[10  1]\"] = \"Strong Weak 5:1 - Teacher 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[10  1]\"] = \"Strong Weak 10:1 - Teacher 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[ 1 10]\"] = \"Strong Weak 5:1 - Teacher 1:10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[ 1 10]\"] = \"Strong Weak 10:1 - Teacher 1:10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[1 1]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Non-Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[5 5]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[5 5]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Non-Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[1 1]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[1 1]_SD_1_1_SU_1_1\"] = \"Non-Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[5 5]_SD_1_1_SU_1_1\"] = \"Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[5 5]_SD_1_1_SU_1_1\"] = \"Non-Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[1 1]_SD_1_1_SU_1_1\"] = \"Robust Non-Useful\"\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[2 2]_[2 2]_SD_1_1_SU_1_1\"] = \"Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[4 4]_SD_1_1_SU_1_1\"] = \"Non-Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[2 2]_[0.5 0.5]_SD_1_1_SU_1_1\"] = \"Robust Non-Useful\"\n",
    "\n",
    "\n",
    "data_model_name_dict['KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[0.5 0.5]_[2 2]_SD_1_1_SU_1_1'] = \"Non-Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[0.5 0.5]_[8 8]_SD_1_1_SU_1_1\"] = \"Non-Robust Useful\"\n",
    "\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[2.  0.5]_[1 1]_SD_1_1_SU_1_1\"] = \"Invariant Defence\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingFirstStronger_AttackingIdentity_[0.5 0.5]_[2.  0.5]_[1 1]_SD_2_1_SU_1_1\"] = \"Protecting Robust\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[2.  0.5]_[1 1]_SD_1_2_SU_1_1\"] = \"Protecting Non-Robust\"\n",
    "\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[1 1]_SD_1_1_SU_1_1\"] = \"Invariant Defence\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingFirstStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[1 1]_SD_2_1_SU_1_1\"] = \"Protecting Robust\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[1 1]_SD_1_2_SU_1_1\"] = \"Protecting Non-Robust\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_evolution = df_state_evolution[df_state_evolution[\"experiment_id\" ] == experiment_id]\n",
    "gd = df_erm[df_erm[\"experiment_id\" ] == experiment_id]\n",
    "# make the column subspace_overlaps to string\n",
    "state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: str(x))\n",
    "gd[\"subspace_overlaps\"] = gd[\"subspace_overlaps\"].apply(lambda x: str(x))\n",
    "\n",
    "# create a json colum\n",
    "state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: json.loads(x))\n",
    "gd[\"subspace_overlaps\"] = gd[\"subspace_overlaps\"].apply(lambda x: json.loads(x))\n",
    "from pandas import json_normalize\n",
    "# normalize the json column\n",
    "normalized = json_normalize(state_evolution[\"subspace_overlaps\"])\n",
    "normalized_gd = json_normalize(gd[\"subspace_overlaps\"])\n",
    "\n",
    "\n",
    "\n",
    "def explode_array_column(row, col):\n",
    "    return pd.Series(row[col])\n",
    "\n",
    "# reset the index of the original dataframe\n",
    "state_evolution = state_evolution.reset_index(drop=True)\n",
    "gd = gd.reset_index(drop=True)\n",
    "\n",
    "for col in normalized.columns:\n",
    "    expanded_cols = normalized.apply(lambda x: explode_array_column(x,col), axis=1)\n",
    "    col = col[:-1]\n",
    "    expanded_cols.columns = [col+'_{}'.format(i) for i in range(expanded_cols.shape[1])]\n",
    "    # reset the index of the expanded columns\n",
    "    expanded_cols = expanded_cols.reset_index(drop=True)\n",
    "    state_evolution = pd.concat([state_evolution, expanded_cols], axis=1)\n",
    "\n",
    "for col in normalized_gd.columns:\n",
    "    expanded_cols = normalized_gd.apply(lambda x: explode_array_column(x,col), axis=1)\n",
    "    col = col[:-1]\n",
    "    expanded_cols.columns = [col+'_{}'.format(i) for i in range(expanded_cols.shape[1])]\n",
    "    # reset the index of the expanded columns\n",
    "    expanded_cols = expanded_cols.reset_index(drop=True)\n",
    "\n",
    "    gd = pd.concat([gd, expanded_cols], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_evolution[\"subspace_overlaps_ratio\"] = state_evolution[\"subspace_overlaps_ratio\"].apply(lambda x: str(x))\n",
    "state_evolution[\"subspace_overlaps_ratio\"] = state_evolution[\"subspace_overlaps_ratio\"].apply(lambda x: json.loads(x))\n",
    "normalized = json_normalize(state_evolution[\"subspace_overlaps_ratio\"])\n",
    "# rename the columns of the normalized dataframe\n",
    "for column in normalized.columns:\n",
    "    normalized = normalized.rename(columns={column:column+\"_ratio\"})\n",
    "# merge the normalized dataframe with the original dataframe\n",
    "state_evolution = pd.concat([state_evolution, normalized], axis=1)\n",
    "# drop the original subspace_overlaps column\n",
    "state_evolution = state_evolution.drop(columns=[\"subspace_overlaps_ratio\"])\n",
    "\n",
    "gd[\"subspace_overlaps_ratio\"] = gd[\"subspace_overlaps_ratio\"].apply(lambda x: str(x))\n",
    "gd[\"subspace_overlaps_ratio\"] = gd[\"subspace_overlaps_ratio\"].apply(lambda x: json.loads(x))\n",
    "normalized = json_normalize(gd[\"subspace_overlaps_ratio\"])\n",
    "# rename the columns of the normalized dataframe\n",
    "for column in normalized.columns:\n",
    "    normalized = normalized.rename(columns={column:column+\"_ratio\"})\n",
    "# merge the normalized dataframe with the original dataframe\n",
    "gd = pd.concat([gd, normalized], axis=1)\n",
    "# drop the original subspace_overlaps column\n",
    "gd = gd.drop(columns=[\"subspace_overlaps_ratio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mu_usefulness(row):\n",
    "#     rho = row[\"rho\"]\n",
    "#     tau = row[\"tau\"]\n",
    "\n",
    "#     rho = float(rho)\n",
    "#     tau = float(tau)\n",
    "\n",
    "#     return np.sqrt(2 / np.pi) * rho / np.sqrt( rho + tau**2 )\n",
    "\n",
    "# def compute_gamma_robustness(row):\n",
    "#     rho = row[\"rho\"]\n",
    "#     tau = row[\"tau\"]\n",
    "\n",
    "\n",
    "#     rho = float(rho)\n",
    "#     tau = float(tau)\n",
    "\n",
    "#     return np.sqrt(2 / np.pi) * tau / np.sqrt( rho + tau**2 )\n",
    "\n",
    "# def compute_mu_usefulness_ratio(row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the difference between the adversarial_generalization_error and the generalization_error for both the state evolution and the erm\n",
    "state_evolution[\"difference_adv_gen\"] = state_evolution[\"adversarial_generalization_error\"] - state_evolution[\"generalization_error\"]\n",
    "gd[\"difference_adv_gen\"] = gd[\"adversarial_generalization_error\"] - gd[\"generalization_error_erm\"]\n",
    "\n",
    "\n",
    "state_evolution[\"ratio_adv_gen\"] = state_evolution[\"adversarial_generalization_error\"] / state_evolution[\"generalization_error\"]\n",
    "gd[\"ratio_adv_gen\"] = gd[\"adversarial_generalization_error\"] / gd[\"generalization_error_erm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the noise contribution\n",
    "def noise_contribution(rho: float, tau: float) -> float:\n",
    "    if tau == 0:\n",
    "        tau = 1e-10\n",
    "    return 0.5 - np.arctan( np.sqrt( rho / tau**2 ) ) / np.pi\n",
    "\n",
    "state_evolution[\"noise_contribution\"] = state_evolution.apply(lambda x: noise_contribution(x[\"rho\"], x[\"tau\"]), axis=1)\n",
    "gd[\"noise_contribution\"] = gd.apply(lambda x: noise_contribution(x[\"rho\"], x[\"tau\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the noiseless generalization error without the noise contribution\n",
    "state_evolution[\"noiseless_generalization_error\"] = state_evolution[\"generalization_error\"] - state_evolution[\"noise_contribution\"]\n",
    "gd[\"noiseless_generalization_error_erm\"] = gd[\"generalization_error_erm\"] - gd[\"noise_contribution\"]\n",
    "\n",
    "# create a column for the adversarial noiseless generalization error without the noise contribution\n",
    "state_evolution[\"noiseless_adversarial_generalization_error\"] = state_evolution[\"adversarial_generalization_error\"] - state_evolution[\"noise_contribution\"]\n",
    "gd[\"noiseless_adversarial_generalization_error\"] = gd[\"adversarial_generalization_error\"] - gd[\"noise_contribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseless_angle_to_generalisation(angle):\n",
    "    return np.arccos(angle) / np.pi\n",
    "\n",
    "state_evolution[\"noiseless_angle_to_generalisation\"] = state_evolution.apply(lambda x: noiseless_angle_to_generalisation(x[\"angle\"]), axis=1)\n",
    "gd[\"noiseless_angle_to_generalisation\"] = gd.apply(lambda x: noiseless_angle_to_generalisation(x[\"angle\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the A/sqrt(q*N) for both the state evolution and the erm\n",
    "state_evolution[\"A_over_sqrt_qN\"] = state_evolution[\"A\"] / np.sqrt(state_evolution[\"q\"] * state_evolution[\"N\"])\n",
    "gd[\"A_over_sqrt_qN\"] = gd[\"A\"] / np.sqrt(gd[\"q\"] * gd[\"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for m/sqrt( rho*q - m**2 ) vs A/sqrt(q*N) for both the state evolution and the erm\n",
    "state_evolution[\"m_over_sqrt_rhoq_minus_m2\"] = state_evolution[\"m\"] / np.sqrt(state_evolution[\"rho\"] * state_evolution[\"q\"] - state_evolution[\"m\"]**2)\n",
    "gd[\"m_over_sqrt_rhoq_minus_m2\"] = gd[\"m\"] / np.sqrt(gd[\"rho\"] * gd[\"q\"] - gd[\"m\"]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the data_model_name if there is a sequence ___text at the end\n",
    "def strip_data_model_name(data_model_name):\n",
    "    return re.sub(r\"___.*\",\"\",data_model_name)\n",
    "state_evolution[\"data_model_name\"] = state_evolution[\"data_model_name\"].apply(strip_data_model_name)\n",
    "gd[\"data_model_name\"] = gd[\"data_model_name\"].apply(strip_data_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the beta coefficient from the data_model_name\n",
    "def extract_beta(row):\n",
    "    data_model_name = row[\"data_model_name\"]\n",
    "    split = data_model_name.split(\"_\")\n",
    "    # extract the coefficient\n",
    "    beta = split[-1]\n",
    "    # convert the beta to a float\n",
    "    beta = float(beta)\n",
    "    return beta\n",
    "\n",
    "state_evolution[\"beta\"] = state_evolution.apply(lambda x: extract_beta(x), axis=1)\n",
    "if len(gd) > 0:\n",
    "    gd[\"beta\"] = gd.apply(lambda x: extract_beta(x), axis=1)\n",
    "else:\n",
    "    gd[\"beta\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = gd.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"beta\"]) #,\"p_calibration\"\n",
    "state_evolution = state_evolution.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"beta\"]) #,\"p_calibration\"\n",
    "\n",
    "# drop id, code_version, experiment_id, date, initial_condition, test_against_epsilons, calibrations, abs_tol, min_iter,max_iter,blend_fpe,int_lims,subspace_overlaps\n",
    "state_evolution = state_evolution.drop(columns=[\"id\",\"code_version\",\"experiment_id\",\"date\",\"initial_condition\",\"test_against_epsilons\",\"calibrations\",\"abs_tol\",\"min_iter\",\"max_iter\",\"blend_fpe\",\"int_lims\",\"subspace_overlaps\",\"data_model_type\",\"data_model_description\",\"data_model_name\"])\n",
    "# drop id, code_version, experiment_id, test_against_epsilons, date, subspace_overlaps, analytical_calibrations, erm_calibrations, \n",
    "gd = gd.drop(columns=[\"id\",\"code_version\",\"experiment_id\",\"test_against_epsilons\",\"date\",\"subspace_overlaps\",\"analytical_calibrations\",\"erm_calibrations\",\"data_model_type\",\"data_model_description\",\"data_model_name\"])\n",
    "\n",
    "state_evolution.columns = [col+\"_state_evolution\" for col in state_evolution.columns]\n",
    "gd.columns = [col+\"_erm\" for col in gd.columns]\n",
    "\n",
    "state_evolution = state_evolution.groupby(level=[0,1,2,3,4,5,6]).agg([\"mean\",\"std\"]) #,4\n",
    "gd = gd.groupby(level=[0,1,2,3,4,5,6]).agg([\"mean\",\"std\"]) #,4\n",
    "df = state_evolution.join(gd, how=\"outer\")\n",
    "df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataframe as a pickle file\n",
    "if not os.path.exists(\"Pickles\"):\n",
    "    os.makedirs(\"Pickles\")\n",
    "if not os.path.exists(\"Pickles/powerlaw_beta.pkl\"):\n",
    "    df.to_pickle(\"Pickles/powerlaw_beta.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle\n",
    "df = pd.read_pickle(\"Pickles/powerlaw_beta.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique epsilons\n",
    "epsilons = df.index.get_level_values(\"epsilon\").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = df.index.get_level_values(\"alpha\").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of dataframes for each data_model_name\n",
    "df_dict = {}\n",
    "for epsilon in epsilons:\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    eps_df = df.xs(epsilon, level=\"epsilon\")\n",
    "\n",
    "\n",
    "\n",
    "    # for each data_model_name, create a dictionary\n",
    "    eps_dict = {}\n",
    "\n",
    "\n",
    "    betas = eps_df.index.get_level_values(\"beta\").unique()\n",
    "    adversarial_error_0 = eps_df[\"adversarial_generalization_error_state_evolution\"][\"mean\"].values\n",
    "    generalization_error_0 = eps_df[\"generalization_error_state_evolution\"][\"mean\"].values\n",
    "    boundary_error_0 = eps_df[\"difference_adv_gen_state_evolution\"][\"mean\"].values\n",
    "    class_preserving = eps_df[\"fair_adversarial_error_state_evolution\"][\"mean\"].values\n",
    "\n",
    "    adversarial_error_erm = eps_df[\"adversarial_generalization_error_erm\"][\"mean\"].values\n",
    "    generalization_error_erm = eps_df[\"generalization_error_erm_erm\"][\"mean\"].values\n",
    "    boundary_error_erm = eps_df[\"difference_adv_gen_erm\"][\"mean\"].values\n",
    "    class_preserving_erm = eps_df[\"fair_adversarial_error_erm\"][\"mean\"].values\n",
    "\n",
    "    adversarial_error_erm_std = eps_df[\"adversarial_generalization_error_erm\"][\"std\"].values\n",
    "    generalization_error_erm_std = eps_df[\"generalization_error_erm_erm\"][\"std\"].values\n",
    "    boundary_error_erm_std = eps_df[\"difference_adv_gen_erm\"][\"std\"].values\n",
    "    class_preserving_erm_std = eps_df[\"fair_adversarial_error_erm\"][\"std\"].values\n",
    "\n",
    "    betas = np.array(betas)\n",
    "    adversarial_error_0 = np.array(adversarial_error_0)\n",
    "    generalization_error_0 = np.array(generalization_error_0)\n",
    "    boundary_error_0 = np.array(boundary_error_0)\n",
    "    class_preserving = np.array(class_preserving)\n",
    "\n",
    "    adversarial_error_erm = np.array(adversarial_error_erm)\n",
    "    generalization_error_erm = np.array(generalization_error_erm)\n",
    "    boundary_error_erm = np.array(boundary_error_erm)\n",
    "    class_preserving_erm = np.array(class_preserving_erm)\n",
    "\n",
    "    adversarial_error_erm_std = np.array(adversarial_error_erm_std)\n",
    "    generalization_error_erm_std = np.array(generalization_error_erm_std)\n",
    "    boundary_error_erm_std = np.array(boundary_error_erm_std)\n",
    "    class_preserving_erm_std = np.array(class_preserving_erm_std)\n",
    "\n",
    "    eps_0_dict = {}\n",
    "    eps_0_dict[\"betas\"] = betas\n",
    "    eps_0_dict[\"adversarial_error\"] = adversarial_error_0\n",
    "    eps_0_dict[\"generalization_error\"] = generalization_error_0\n",
    "    eps_0_dict[\"boundary_error\"] = boundary_error_0\n",
    "    eps_0_dict[\"class_preserving\"] = class_preserving\n",
    "\n",
    "    eps_0_dict[\"adversarial_error_erm\"] = adversarial_error_erm\n",
    "    eps_0_dict[\"generalization_error_erm\"] = generalization_error_erm\n",
    "    eps_0_dict[\"boundary_error_erm\"] = boundary_error_erm\n",
    "    eps_0_dict[\"class_preserving_erm\"] = class_preserving_erm\n",
    "\n",
    "    eps_0_dict[\"adversarial_error_erm_std\"] = adversarial_error_erm_std\n",
    "    eps_0_dict[\"generalization_error_erm_std\"] = generalization_error_erm_std\n",
    "    eps_0_dict[\"boundary_error_erm_std\"] = boundary_error_erm_std\n",
    "    eps_0_dict[\"class_preserving_erm_std\"] = class_preserving_erm_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_dict[epsilon] = eps_0_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_DIRECTORY = \"./Assets/powerlawBeta\"\n",
    "if not os.path.exists(IMG_DIRECTORY):\n",
    "    os.makedirs(IMG_DIRECTORY)\n",
    "\n",
    "\n",
    "def save_plot(fig, name, formats=[\"pdf\",\"jpg\"], date=False):\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for f in formats:        \n",
    "        path = \"{}\".format(name) + \"_{}\".format(current_date) + \".\" + f\n",
    "        if not date:\n",
    "            path = \"{}\".format(name) + \".\" + f\n",
    "        fig.savefig(            \n",
    "            os.path.join(IMG_DIRECTORY, path),\n",
    "            format=f,\n",
    "        )\n",
    "\n",
    "\n",
    "def set_size(width, fraction=1, subplots=(1, 1)):\n",
    "    if width == \"thesis\":\n",
    "        width_pt = 426.79135\n",
    "    elif width == \"beamer\":\n",
    "        width_pt = 307.28987\n",
    "    else:\n",
    "        width_pt = width\n",
    "\n",
    "    fig_width_pt = width_pt * fraction\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    golden_ratio = (5**0.5 - 1) / 2\n",
    "\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    fig_height_in = fig_width_in * (golden_ratio) * (subplots[0] / subplots[1])\n",
    "\n",
    "    return (fig_width_in, fig_height_in)\n",
    "\n",
    "\n",
    "width = 1.25 * 458.63788\n",
    "\n",
    "plt.style.use(\"../latex_ready.mplstyle\")\n",
    "\n",
    "tuple_size = set_size(width, fraction=0.50)\n",
    "\n",
    "tuple_size = (4.25,2.7)\n",
    "\n",
    "multiplier = 1.2\n",
    "second_multiplier = 0.7\n",
    "\n",
    "\n",
    "# import Line2D for custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=1,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    figsize=(tuple_size[0], tuple_size[1]),\n",
    "    gridspec_kw={\"hspace\": 0,\"wspace\": 0},\n",
    ")\n",
    "\n",
    "\n",
    "# ICML adjustments\n",
    "fig.subplots_adjust(left=0.11)\n",
    "fig.subplots_adjust(bottom=0.12)\n",
    "fig.subplots_adjust(top=0.83)\n",
    "fig.subplots_adjust(right=0.97)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a custom legend\n",
    "custom_legend = []\n",
    "\n",
    "linestyles = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for df_idx, (epsilon, value) in enumerate(df_dict.items()):\n",
    "\n",
    "    eps_dict = value\n",
    "\n",
    "    \n",
    "\n",
    "    betas = eps_dict[\"betas\"]\n",
    "    adversarial_error = eps_dict[\"adversarial_error\"]\n",
    "    generalization_error = eps_dict[\"generalization_error\"]\n",
    "    boundary_error = eps_dict[\"boundary_error\"]\n",
    "    class_preserving = eps_dict[\"class_preserving\"]\n",
    "\n",
    "    adversarial_error_erm = eps_dict[\"adversarial_error_erm\"]\n",
    "    generalization_error_erm = eps_dict[\"generalization_error_erm\"]\n",
    "    boundary_error_erm = eps_dict[\"boundary_error_erm\"]\n",
    "    class_preserving_erm = eps_dict[\"class_preserving_erm\"]\n",
    "\n",
    "    adversarial_error_erm_std = eps_dict[\"adversarial_error_erm_std\"]\n",
    "    generalization_error_erm_std = eps_dict[\"generalization_error_erm_std\"]\n",
    "    boundary_error_erm_std = eps_dict[\"boundary_error_erm_std\"]\n",
    "    class_preserving_erm_std = eps_dict[\"class_preserving_erm_std\"]\n",
    "\n",
    "\n",
    "    adversarial_lines = axes.plot(betas, adversarial_error, linestyle=linestyles[df_idx],color=\"C0\")\n",
    "    axes.plot(betas, generalization_error, linestyle=linestyles[df_idx],color=\"C1\")\n",
    "    axes.plot(betas, boundary_error,linestyle=linestyles[df_idx], color=\"C2\")\n",
    "    # axes.plot(betas, class_preserving,linestyle=linestyles[df_idx], color=\"C3\")\n",
    "\n",
    "    # custom_legend.append(Line2D([0],[0],color=\"C0\", linestyle=linestyles[df_idx], label=r\"$E_{{\\mathrm{{adv}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "    # custom_legend.append(Line2D([0],[0],color=\"C1\", linestyle=linestyles[df_idx], label=r\"$E_{{\\mathrm{{gen}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "    # custom_legend.append(Line2D([0],[0],color=\"C2\", linestyle=linestyles[df_idx], label=r\"$E_{{\\mathrm{{bound}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "    # custom_legend.append(Line2D([0],[0],color=\"C3\", linestyle=linestyles[df_idx], label=r\"$E_{{\\mathrm{{CP}}}}, \\beta={}$\".format(beta)))\n",
    "\n",
    "    # axs.errorbar(\n",
    "    #     betas,\n",
    "    #     adversarial_error_erm,\n",
    "    #     yerr=adversarial_error_erm_std,\n",
    "    #     fmt=\".\",\n",
    "    #     markersize=1,\n",
    "    #     color=\"C0\"\n",
    "    # )\n",
    "    # axs.errorbar(\n",
    "    #     betas,\n",
    "    #     generalization_error_erm,\n",
    "    #     yerr=generalization_error_erm_std,\n",
    "    #     fmt=\".\",\n",
    "    #     markersize=1,\n",
    "    #     color=\"C1\"\n",
    "    # )\n",
    "    # axs.errorbar(\n",
    "    #     betas,\n",
    "    #     boundary_error_erm,\n",
    "    #     yerr=boundary_error_erm_std,\n",
    "    #     fmt=\".\",\n",
    "    #     markersize=1,\n",
    "    #     color=\"C2\"\n",
    "    # )\n",
    "    # axs.errorbar(\n",
    "    #     betas,\n",
    "    #     class_preserving_erm,\n",
    "    #     yerr=class_preserving_erm_std,\n",
    "    #     fmt=\".\",\n",
    "    #     markersize=1,\n",
    "    #     color=\"C3\"\n",
    "    # )\n",
    "\n",
    "\n",
    "# axes.set_xscale(\"log\")\n",
    "axes.set_yscale(\"log\")\n",
    "\n",
    "\n",
    "\n",
    "axes.set_ylabel(r\"$E$\", labelpad=2.0)\n",
    "\n",
    "\n",
    "axes.set_xlabel(r\"$\\beta$\", labelpad=2.0)\n",
    "axes.grid(which=\"both\", axis=\"both\", alpha=0.5)\n",
    "# Set the major ticks to face inwards\n",
    "axes.tick_params(axis='both', which='major', direction='in')\n",
    "\n",
    "# Set the minor ticks to face inwards\n",
    "axes.tick_params(axis='both', which='minor', direction='in')\n",
    "\n",
    "error_legend = []\n",
    "\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{adv}}$\",color=\"C0\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{gen}}$\",color=\"C1\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{bound}}$\",color=\"C2\"))\n",
    "\n",
    "epsilon_legend = []\n",
    "\n",
    "for idx, epsilon in enumerate(epsilons):\n",
    "    epsilon_legend.append(Line2D([0],[0],color=\"black\", linestyle=linestyles[idx], label=r\"$\\varepsilon_t={}$\".format(epsilons[idx]))) \n",
    "\n",
    "custom_legend = []\n",
    "\n",
    "for idx in range(len(error_legend)-1):\n",
    "    custom_legend.append(error_legend[idx])\n",
    "    custom_legend.append(epsilon_legend[idx])\n",
    "\n",
    "custom_legend.append(error_legend[-1])\n",
    "\n",
    "\n",
    "# Place the legend at the bottom of the figure\n",
    "fig.legend(handles=custom_legend, loc='upper center', ncol=3)\n",
    "\n",
    "save = True\n",
    "if save:\n",
    "    save_plot(\n",
    "        fig,\n",
    "        f\"powerlawbeta\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
