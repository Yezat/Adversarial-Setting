{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"../experiments\")\n",
    "\n",
    "\n",
    "from experiment_information import *\n",
    "from experiment_setup import *\n",
    "from data import *\n",
    "from helpers import *\n",
    "from data_loading import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code assumes that you have defined and run an experiment before using `define_experiment.ipynb` in the `experiments` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiments, df_state_evolution, df_erm = obtain_dataframes(logger)\n",
    "df_experiments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_loc = 0\n",
    "\n",
    "# extract experiment by id \"250fa46e-55cb-40d2-b503-11b38823235d\"\n",
    "# experiment_id = \"d43ae27a-cf92-4261-a7d1-de5c9717f775\"\n",
    "\n",
    "\n",
    "# extract and print the top experiment_id\n",
    "experiment_id = df_experiments.iloc[experiment_loc][\"experiment_id\"]\n",
    "print(experiment_id)\n",
    "\n",
    "# extract and print the experiment type\n",
    "experiment_type = df_experiments.iloc[experiment_loc][\"experiment_type\"]\n",
    "print(experiment_type)\n",
    "\n",
    "# extract and print the data model type and data_model name used\n",
    "data_model_types = df_experiments.iloc[experiment_loc][\"data_model_types\"]\n",
    "# convert it to the enum\n",
    "data_model_types = [DataModelType[data_model_type] for data_model_type in json.loads(data_model_types)]\n",
    "data_model_names = [name for name in json.loads(df_experiments.iloc[experiment_loc][\"data_model_names\"])]\n",
    "data_model_descriptions = df_experiments.iloc[experiment_loc][\"data_model_descriptions\"]\n",
    "for data_model_type in data_model_types:\n",
    "    print(data_model_type.name)\n",
    "print(data_model_names)\n",
    "print(data_model_descriptions)\n",
    "\n",
    "# print the experiment name\n",
    "experiment_name = df_experiments.iloc[experiment_loc][\"experiment_name\"]\n",
    "print(experiment_name)\n",
    "\n",
    "# print the experiment problem types\n",
    "experiment_problem_types = df_experiments.iloc[experiment_loc][\"problem_types\"]\n",
    "experiment_problem_types = json.loads(experiment_problem_types)\n",
    "print(experiment_problem_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_name_dict = {}\n",
    "data_model_name_dict[\"VanillaGaussian\"] = \"Vanilla Gaussian\"\n",
    "data_model_name_dict[\"2_VanillaGaussian\"] = \"Vanilla Gaussian\"\n",
    "data_model_name_dict[\"VanillaGaussianThetaFirst\"] = \"Vanilla Gaussian - Teacher 10:1\"\n",
    "data_model_name_dict[\"VanillaGaussianTimes10\"] = \"Vanilla Gaussian x10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[1 1]\"] = \"Strong Weak 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1 1]\"] = \"Strong Weak 5:1\"\n",
    "data_model_name_dict[\"2_KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1 1]\"] = \"Strong Weak 5:1\"\n",
    "data_model_name_dict[\"2_KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[1 1]\"] = \"Strong Weak 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[10  1]\"] = \"Strong Weak 5:1 - Teacher 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[10  1]\"] = \"Strong Weak 10:1 - Teacher 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[ 1 10]\"] = \"Strong Weak 5:1 - Teacher 1:10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[ 1 10]\"] = \"Strong Weak 10:1 - Teacher 1:10\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_evolution = df_state_evolution[df_state_evolution[\"experiment_id\" ] == experiment_id]\n",
    "gd = df_erm[df_erm[\"experiment_id\" ] == experiment_id]\n",
    "# make the column subspace_overlaps to string\n",
    "state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: str(x))\n",
    "gd[\"subspace_overlaps\"] = gd[\"subspace_overlaps\"].apply(lambda x: str(x))\n",
    "\n",
    "# create a json colum\n",
    "state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: json.loads(x))\n",
    "gd[\"subspace_overlaps\"] = gd[\"subspace_overlaps\"].apply(lambda x: json.loads(x))\n",
    "from pandas import json_normalize\n",
    "# normalize the json column\n",
    "normalized = json_normalize(state_evolution[\"subspace_overlaps\"])\n",
    "normalized_gd = json_normalize(gd[\"subspace_overlaps\"])\n",
    "\n",
    "\n",
    "\n",
    "def explode_array_column(row, col):\n",
    "    return pd.Series(row[col])\n",
    "\n",
    "# reset the index of the original dataframe\n",
    "state_evolution = state_evolution.reset_index(drop=True)\n",
    "gd = gd.reset_index(drop=True)\n",
    "\n",
    "for col in normalized.columns:\n",
    "    expanded_cols = normalized.apply(lambda x: explode_array_column(x,col), axis=1)\n",
    "    col = col[:-1]\n",
    "    expanded_cols.columns = [col+'_{}'.format(i) for i in range(expanded_cols.shape[1])]\n",
    "    # reset the index of the expanded columns\n",
    "    expanded_cols = expanded_cols.reset_index(drop=True)\n",
    "    state_evolution = pd.concat([state_evolution, expanded_cols], axis=1)\n",
    "\n",
    "for col in normalized_gd.columns:\n",
    "    expanded_cols = normalized_gd.apply(lambda x: explode_array_column(x,col), axis=1)\n",
    "    col = col[:-1]\n",
    "    expanded_cols.columns = [col+'_{}'.format(i) for i in range(expanded_cols.shape[1])]\n",
    "    # reset the index of the expanded columns\n",
    "    expanded_cols = expanded_cols.reset_index(drop=True)\n",
    "\n",
    "    gd = pd.concat([gd, expanded_cols], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_evolution[\"subspace_overlaps_ratio\"] = state_evolution[\"subspace_overlaps_ratio\"].apply(lambda x: str(x))\n",
    "state_evolution[\"subspace_overlaps_ratio\"] = state_evolution[\"subspace_overlaps_ratio\"].apply(lambda x: json.loads(x))\n",
    "normalized = json_normalize(state_evolution[\"subspace_overlaps_ratio\"])\n",
    "# rename the columns of the normalized dataframe\n",
    "for column in normalized.columns:\n",
    "    normalized = normalized.rename(columns={column:column+\"_ratio\"})\n",
    "# merge the normalized dataframe with the original dataframe\n",
    "state_evolution = pd.concat([state_evolution, normalized], axis=1)\n",
    "# drop the original subspace_overlaps column\n",
    "state_evolution = state_evolution.drop(columns=[\"subspace_overlaps_ratio\"])\n",
    "\n",
    "gd[\"subspace_overlaps_ratio\"] = gd[\"subspace_overlaps_ratio\"].apply(lambda x: str(x))\n",
    "gd[\"subspace_overlaps_ratio\"] = gd[\"subspace_overlaps_ratio\"].apply(lambda x: json.loads(x))\n",
    "normalized = json_normalize(gd[\"subspace_overlaps_ratio\"])\n",
    "# rename the columns of the normalized dataframe\n",
    "for column in normalized.columns:\n",
    "    normalized = normalized.rename(columns={column:column+\"_ratio\"})\n",
    "# merge the normalized dataframe with the original dataframe\n",
    "gd = pd.concat([gd, normalized], axis=1)\n",
    "# drop the original subspace_overlaps column\n",
    "gd = gd.drop(columns=[\"subspace_overlaps_ratio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the difference between the adversarial_generalization_error and the generalization_error for both the state evolution and the erm\n",
    "state_evolution[\"difference_adv_gen\"] = state_evolution[\"adversarial_generalization_error\"] - state_evolution[\"generalization_error\"]\n",
    "gd[\"difference_adv_gen\"] = gd[\"adversarial_generalization_error\"] - gd[\"generalization_error_erm\"]\n",
    "\n",
    "\n",
    "state_evolution[\"ratio_adv_gen\"] = state_evolution[\"adversarial_generalization_error\"] / state_evolution[\"generalization_error\"]\n",
    "gd[\"ratio_adv_gen\"] = gd[\"adversarial_generalization_error\"] / gd[\"generalization_error_erm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the noise contribution\n",
    "def noise_contribution(rho: float, tau: float) -> float:\n",
    "    if tau == 0:\n",
    "        tau = 1e-10\n",
    "    return 0.5 - np.arctan( np.sqrt( rho / tau**2 ) ) / np.pi\n",
    "\n",
    "state_evolution[\"noise_contribution\"] = state_evolution.apply(lambda x: noise_contribution(x[\"rho\"], x[\"tau\"]), axis=1)\n",
    "gd[\"noise_contribution\"] = gd.apply(lambda x: noise_contribution(x[\"rho\"], x[\"tau\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the noiseless generalization error without the noise contribution\n",
    "state_evolution[\"noiseless_generalization_error\"] = state_evolution[\"generalization_error\"] - state_evolution[\"noise_contribution\"]\n",
    "gd[\"noiseless_generalization_error_erm\"] = gd[\"generalization_error_erm\"] - gd[\"noise_contribution\"]\n",
    "\n",
    "# create a column for the adversarial noiseless generalization error without the noise contribution\n",
    "state_evolution[\"noiseless_adversarial_generalization_error\"] = state_evolution[\"adversarial_generalization_error\"] - state_evolution[\"noise_contribution\"]\n",
    "gd[\"noiseless_adversarial_generalization_error\"] = gd[\"adversarial_generalization_error\"] - gd[\"noise_contribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the A/sqrt(q*N) for both the state evolution and the erm\n",
    "state_evolution[\"A_over_sqrt_qN\"] = state_evolution[\"A\"] / np.sqrt(state_evolution[\"q\"] * state_evolution[\"N\"])\n",
    "gd[\"A_over_sqrt_qN\"] = gd[\"A\"] / np.sqrt(gd[\"q\"] * gd[\"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for m/sqrt( rho*q - m**2 ) vs A/sqrt(q*N) for both the state evolution and the erm\n",
    "state_evolution[\"m_over_sqrt_rhoq_minus_m2\"] = state_evolution[\"m\"] / np.sqrt(state_evolution[\"rho\"] * state_evolution[\"q\"] - state_evolution[\"m\"]**2)\n",
    "gd[\"m_over_sqrt_rhoq_minus_m2\"] = gd[\"m\"] / np.sqrt(gd[\"rho\"] * gd[\"q\"] - gd[\"m\"]**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the data_model_name if there is a sequence ___text at the end\n",
    "def strip_data_model_name(data_model_name):\n",
    "    return re.sub(r\"___.*\",\"\",data_model_name)\n",
    "state_evolution[\"data_model_name\"] = state_evolution[\"data_model_name\"].apply(strip_data_model_name)\n",
    "gd[\"data_model_name\"] = gd[\"data_model_name\"].apply(strip_data_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = gd.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"data_model_name\"]) #,\"p_calibration\"\n",
    "state_evolution = state_evolution.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"data_model_name\"]) #,\"p_calibration\"\n",
    "\n",
    "# drop id, code_version, experiment_id, date, initial_condition, test_against_epsilons, calibrations, abs_tol, min_iter,max_iter,blend_fpe,int_lims,subspace_overlaps\n",
    "state_evolution = state_evolution.drop(columns=[\"id\",\"code_version\",\"experiment_id\",\"date\",\"initial_condition\",\"test_against_epsilons\",\"calibrations\",\"abs_tol\",\"min_iter\",\"max_iter\",\"blend_fpe\",\"int_lims\",\"subspace_overlaps\",\"data_model_type\",\"data_model_description\"])\n",
    "# drop id, code_version, experiment_id, test_against_epsilons, date, subspace_overlaps, analytical_calibrations, erm_calibrations, \n",
    "gd = gd.drop(columns=[\"id\",\"code_version\",\"experiment_id\",\"test_against_epsilons\",\"date\",\"subspace_overlaps\",\"analytical_calibrations\",\"erm_calibrations\",\"data_model_type\",\"data_model_description\"])\n",
    "\n",
    "state_evolution.columns = [col+\"_state_evolution\" for col in state_evolution.columns]\n",
    "gd.columns = [col+\"_erm\" for col in gd.columns]\n",
    "\n",
    "state_evolution = state_evolution.groupby(level=[0,1,2,3,4,5,6]).agg([\"mean\",\"std\"]) #,4\n",
    "gd = gd.groupby(level=[0,1,2,3,4,5,6]).agg([\"mean\",\"std\"]) #,4\n",
    "df = state_evolution.join(gd, how=\"outer\")\n",
    "df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataframe as a pickle file\n",
    "if not os.path.exists(\"Pickles\"):\n",
    "    os.makedirs(\"Pickles\")\n",
    "if not os.path.exists(\"Pickles/effective_regularisation.pkl\"):\n",
    "    df.to_pickle(\"Pickles/effective_regularisation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_pickle(\"Pickles/effective_regularisation.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique epsilons\n",
    "epsilons = df.index.get_level_values(1).unique()\n",
    "epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique attack_epsilons\n",
    "attack_epsilons = df.index.get_level_values(5).unique()\n",
    "attack_epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of dataframes for each data_model_name\n",
    "epsilon_dict = {}\n",
    "for epsilon in epsilons:\n",
    "\n",
    "    eps_df = df.xs(epsilon, level=\"epsilon\")\n",
    "    \n",
    "\n",
    "    alphas = eps_df.index.get_level_values(\"alpha\").unique()\n",
    "    adversarial_error_0 = eps_df[\"adversarial_generalization_error_state_evolution\"][\"mean\"].values\n",
    "    generalization_error_0 = eps_df[\"generalization_error_state_evolution\"][\"mean\"].values\n",
    "    boundary_error_0 = eps_df[\"difference_adv_gen_state_evolution\"][\"mean\"].values\n",
    "    class_preserving = eps_df[\"fair_adversarial_error_state_evolution\"][\"mean\"].values\n",
    "\n",
    "    adversarial_error_erm = eps_df[\"adversarial_generalization_error_erm\"][\"mean\"].values\n",
    "    generalization_error_erm = eps_df[\"generalization_error_erm_erm\"][\"mean\"].values\n",
    "    boundary_error_erm = eps_df[\"difference_adv_gen_erm\"][\"mean\"].values\n",
    "    class_preserving_erm = eps_df[\"fair_adversarial_error_erm\"][\"mean\"].values\n",
    "\n",
    "    adversarial_error_erm_std = eps_df[\"adversarial_generalization_error_erm\"][\"std\"].values\n",
    "    generalization_error_erm_std = eps_df[\"generalization_error_erm_erm\"][\"std\"].values\n",
    "    boundary_error_erm_std = eps_df[\"difference_adv_gen_erm\"][\"std\"].values\n",
    "    class_preserving_erm_std = eps_df[\"fair_adversarial_error_erm\"][\"std\"].values\n",
    "\n",
    "\n",
    "    alphas = np.array(alphas)\n",
    "    adversarial_error_0 = np.array(adversarial_error_0)\n",
    "    generalization_error_0 = np.array(generalization_error_0)\n",
    "    boundary_error_0 = np.array(boundary_error_0)\n",
    "    class_preserving = np.array(class_preserving)\n",
    "\n",
    "\n",
    "    adversarial_error_erm = np.array(adversarial_error_erm)\n",
    "    generalization_error_erm = np.array(generalization_error_erm)\n",
    "    boundary_error_erm = np.array(boundary_error_erm)\n",
    "    class_preserving_erm = np.array(class_preserving_erm)\n",
    "\n",
    "    adversarial_error_erm_std = np.array(adversarial_error_erm_std)\n",
    "    generalization_error_erm_std = np.array(generalization_error_erm_std)\n",
    "    boundary_error_erm_std = np.array(boundary_error_erm_std)\n",
    "    class_preserving_erm_std = np.array(class_preserving_erm_std)\n",
    "\n",
    "\n",
    "    eps_0_dict = {}\n",
    "    eps_0_dict[\"alphas\"] = alphas\n",
    "    eps_0_dict[\"adversarial_error\"] = adversarial_error_0\n",
    "    eps_0_dict[\"generalization_error\"] = generalization_error_0\n",
    "    eps_0_dict[\"boundary_error\"] = boundary_error_0\n",
    "    eps_0_dict[\"class_preserving\"] = class_preserving\n",
    "\n",
    "    eps_0_dict[\"adversarial_error_erm\"] = adversarial_error_erm\n",
    "    eps_0_dict[\"generalization_error_erm\"] = generalization_error_erm\n",
    "    eps_0_dict[\"boundary_error_erm\"] = boundary_error_erm\n",
    "    eps_0_dict[\"class_preserving_erm\"] = class_preserving_erm\n",
    "\n",
    "    eps_0_dict[\"adversarial_error_erm_std\"] = adversarial_error_erm_std\n",
    "    eps_0_dict[\"generalization_error_erm_std\"] = generalization_error_erm_std\n",
    "    eps_0_dict[\"boundary_error_erm_std\"] = boundary_error_erm_std\n",
    "    eps_0_dict[\"class_preserving_erm_std\"] = class_preserving_erm_std\n",
    "\n",
    "\n",
    "\n",
    "    epsilon_dict[epsilon] = eps_0_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_DIRECTORY = \"./Assets/effective_regularisation\"\n",
    "if not os.path.exists(IMG_DIRECTORY):\n",
    "    os.makedirs(IMG_DIRECTORY)\n",
    "\n",
    "\n",
    "def save_plot(fig, name, formats=[\"pdf\",\"jpg\"], date=False):\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for f in formats:        \n",
    "        path = \"{}\".format(name) + \"_{}\".format(current_date) + \".\" + f\n",
    "        if not date:\n",
    "            path = \"{}\".format(name) + \".\" + f\n",
    "        fig.savefig(            \n",
    "            os.path.join(IMG_DIRECTORY, path),\n",
    "            format=f,\n",
    "        )\n",
    "\n",
    "\n",
    "def set_size(width, fraction=1, subplots=(1, 1)):\n",
    "    if width == \"thesis\":\n",
    "        width_pt = 426.79135\n",
    "    elif width == \"beamer\":\n",
    "        width_pt = 307.28987\n",
    "    else:\n",
    "        width_pt = width\n",
    "\n",
    "    fig_width_pt = width_pt * fraction\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    golden_ratio = (5**0.5 - 1) / 2\n",
    "\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    fig_height_in = fig_width_in * (golden_ratio) * (subplots[0] / subplots[1])\n",
    "\n",
    "    return (fig_width_in, fig_height_in)\n",
    "\n",
    "\n",
    "width = 1.75 * 458.63788\n",
    "\n",
    "multiplier = 1.25\n",
    "width = multiplier * 234.8775\n",
    "\n",
    "plt.style.use(\"../latex_ready.mplstyle\")\n",
    "\n",
    "tuple_size = set_size(width, fraction=0.50)\n",
    "tuple_size = ( 2.5 , 2.4 )\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=1,\n",
    "    sharex=True,\n",
    "    figsize=( tuple_size[0], tuple_size[1]),\n",
    "    gridspec_kw={\"hspace\": 0},\n",
    ")\n",
    "\n",
    "# print the figure size\n",
    "print(\"Figure size: \", tuple_size)\n",
    "# print the figure size\n",
    "print(\"Figure size: \", fig.get_size_inches())\n",
    "\n",
    "from matplotlib.pyplot import Line2D\n",
    "\n",
    "# ICML adjustments\n",
    "fig.subplots_adjust(left=0.2)\n",
    "fig.subplots_adjust(bottom=0.12)\n",
    "fig.subplots_adjust(top=0.99)\n",
    "fig.subplots_adjust(right=0.99)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "linestyles = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "\n",
    "for idx, (epsilon, eps_dict) in enumerate(epsilon_dict.items()):\n",
    "\n",
    "\n",
    "    alphas = eps_dict[\"alphas\"]\n",
    "    adversarial_error = eps_dict[\"adversarial_error\"]\n",
    "    generalization_error = eps_dict[\"generalization_error\"]\n",
    "    boundary_error = eps_dict[\"boundary_error\"]\n",
    "    class_preserving = eps_dict[\"class_preserving\"]\n",
    "\n",
    "    adversarial_error_erm = eps_dict[\"adversarial_error_erm\"]\n",
    "    generalization_error_erm = eps_dict[\"generalization_error_erm\"]\n",
    "    boundary_error_erm = eps_dict[\"boundary_error_erm\"]\n",
    "    class_preserving_erm = eps_dict[\"class_preserving_erm\"]\n",
    "\n",
    "    adversarial_error_erm_std = eps_dict[\"adversarial_error_erm_std\"]\n",
    "    generalization_error_erm_std = eps_dict[\"generalization_error_erm_std\"]\n",
    "    boundary_error_erm_std = eps_dict[\"boundary_error_erm_std\"]\n",
    "    class_preserving_erm_std = eps_dict[\"class_preserving_erm_std\"]\n",
    "\n",
    "\n",
    "    axs[0].plot(alphas, adversarial_error,  color=\"C0\", linestyle=linestyles[idx])\n",
    "    axs[0].plot(alphas, generalization_error, color=\"C1\", linestyle=linestyles[idx])\n",
    "    axs[0].plot(alphas, boundary_error,  color=\"C2\", linestyle=linestyles[idx])\n",
    "    # axs[0].plot(alphas, class_preserving,  color=\"C3\", linestyle=linestyles[idx])\n",
    "\n",
    "\n",
    "\n",
    "    axs[1].plot(\n",
    "        alphas,\n",
    "        adversarial_error_erm,        \n",
    "        linestyle=linestyles[idx],\n",
    "        color=\"C0\"\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        alphas,\n",
    "        generalization_error_erm, \n",
    "        linestyle=linestyles[idx],\n",
    "        color=\"C1\"\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        alphas,\n",
    "        boundary_error_erm,        \n",
    "        linestyle=linestyles[idx],\n",
    "        color=\"C2\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # axs[1].errorbar(\n",
    "    #     alphas,\n",
    "    #     adversarial_error_erm,\n",
    "    #     yerr=adversarial_error_erm_std,\n",
    "        \n",
    "    #     markersize=1,\n",
    "    #     linestyle=linestyles[idx],\n",
    "    #     color=\"C0\"\n",
    "    # )\n",
    "    # axs[1].errorbar(\n",
    "    #     alphas,\n",
    "    #     generalization_error_erm,\n",
    "    #     yerr=generalization_error_erm_std,\n",
    "    #     markersize=1,\n",
    "    #     linestyle=linestyles[idx],\n",
    "    #     color=\"C1\"\n",
    "    # )\n",
    "    # axs[1].errorbar(\n",
    "    #     alphas,\n",
    "    #     boundary_error_erm,\n",
    "    #     yerr=boundary_error_erm_std,\n",
    "        \n",
    "    #     markersize=1,\n",
    "    #     linestyle=linestyles[idx],\n",
    "    #     color=\"C2\"\n",
    "    # )\n",
    "    # axs[1].errorbar(\n",
    "    #     alphas,\n",
    "    #     class_preserving_erm,\n",
    "    #     yerr=class_preserving_erm_std,\n",
    "    #     fmt=\"*\",\n",
    "    #     markersize=1,\n",
    "    #     color=\"C3\"\n",
    "    # )\n",
    "\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].set_ylabel(r\"$E$\", labelpad=2.0)\n",
    "# axs[0].set_xlabel(r\"$\\alpha$\", labelpad=2.0)\n",
    "axs[0].grid(which=\"both\", axis=\"both\", alpha=0.5)\n",
    "axs[0].tick_params(axis='both', which='major', direction='in')\n",
    "axs[0].tick_params(axis='both', which='minor', direction='in')\n",
    "# axs[0].legend(title=\"State Evolution\", loc=\"lower left\")\n",
    "\n",
    "axs[1].set_xscale(\"log\")\n",
    "axs[1].set_yscale(\"log\")\n",
    "axs[1].set_ylabel(r\"$E$\", labelpad=2.0)\n",
    "axs[1].set_xlabel(r\"$\\alpha$\", labelpad=2.0)\n",
    "axs[1].grid(which=\"both\", axis=\"both\", alpha=0.5)\n",
    "axs[1].tick_params(axis='both', which='major', direction='in')\n",
    "axs[1].tick_params(axis='both', which='minor', direction='in')\n",
    "# axs[1].legend(title=\"$\\\\boldsymbol{b}$\", loc=\"lower left\")\n",
    "\n",
    "\n",
    "\n",
    "error_legend = []\n",
    "\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{adv}}$\",color=\"C0\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{gen}}$\",color=\"C1\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{bound}}$\",color=\"C2\"))\n",
    "error_legend.append(Line2D([],[], color=\"white\"))\n",
    "\n",
    "epsilon_legend = []\n",
    "\n",
    "for idx, epsilon in enumerate(epsilons):\n",
    "    epsilon_legend.append(Line2D([0],[0],color=\"black\", linestyle=linestyles[idx], label=r\"$\\varepsilon_t={}$\".format(epsilons[idx]))) \n",
    "\n",
    "custom_legend = []\n",
    "\n",
    "for idx in range(len(error_legend)):\n",
    "    custom_legend.append(error_legend[idx])\n",
    "    custom_legend.append(epsilon_legend[idx])\n",
    "\n",
    "# fig.legend(handles=custom_legend, loc=\"upper center\", ncol=4, handlelength=1.5)\n",
    "\n",
    "save = True\n",
    "if save:\n",
    "    save_plot(\n",
    "        fig,\n",
    "        \"effective_regularisation\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the legend separately, put the custom legend into a figure\n",
    "figlegend = plt.figure(figsize=( 4.25, 0.15*tuple_size[1]))\n",
    "plt.style.use(\"../latex_ready.mplstyle\")\n",
    "legend_ax = figlegend.add_axes([0, 0, 1, 1])\n",
    "legend_ax.axis('off')  # Turn off the axes for the legend figure\n",
    "\n",
    "figlegend.legend(\n",
    "    handles=custom_legend,\n",
    "    handlelength=1,\n",
    "    loc=\"center\",\n",
    "    ncol=4,\n",
    ")\n",
    "\n",
    "figlegend.savefig(\n",
    "    os.path.join(IMG_DIRECTORY, \"legend.pdf\"),\n",
    "    format=\"pdf\",\n",
    "    # bbox_inches=\"tight\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_legend = []\n",
    "\n",
    "custom_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{adv}}$\",color=\"C0\"))\n",
    "custom_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{gen}}$\",color=\"C1\"))\n",
    "custom_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{bound}}$\",color=\"C2\"))\n",
    "# custom_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{CP}}$\",color=\"C0\", marker='o'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save the legend separately, put the custom legend into a figure\n",
    "figlegend = plt.figure(figsize=( tuple_size[0], 0.1 * tuple_size[0]))\n",
    "plt.style.use(\"../latex_ready.mplstyle\")\n",
    "legend_ax = figlegend.add_axes([0, 0, 1, 1])\n",
    "legend_ax.axis('off')  # Turn off the axes for the legend figure\n",
    "\n",
    "figlegend.legend(\n",
    "    handles=custom_legend,\n",
    "    handlelength=1,\n",
    "    loc=\"center\",\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "figlegend.savefig(\n",
    "    os.path.join(IMG_DIRECTORY, \"effective_regularisation_legend_error.pdf\"),\n",
    "    format=\"pdf\",\n",
    "    # bbox_inches=\"tight\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_legend = []\n",
    "\n",
    "for idx, epsilon in enumerate(epsilons):\n",
    "    custom_legend.append(Line2D([0],[0],color=\"black\", linestyle=linestyles[idx], label=r\"$\\varepsilon_t={}$\".format(epsilons[idx]))) \n",
    "\n",
    "\n",
    "# save the legend separately, put the custom legend into a figure\n",
    "figlegend = plt.figure(figsize=( 1.2 * tuple_size[0],0.1 * tuple_size[0]))\n",
    "plt.style.use(\"../latex_ready.mplstyle\")\n",
    "legend_ax = figlegend.add_axes([0, 0, 1, 1])\n",
    "legend_ax.axis('off')  # Turn off the axes for the legend figure\n",
    "\n",
    "figlegend.legend(\n",
    "    handles=custom_legend,\n",
    "    handlelength=2,\n",
    "    loc=\"center\",\n",
    "    ncol=5,\n",
    ")\n",
    "\n",
    "figlegend.savefig(\n",
    "    os.path.join(IMG_DIRECTORY, \"effective_regularisation_legend_epsilon.pdf\"),\n",
    "    format=\"pdf\",\n",
    "    # bbox_inches=\"tight\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
