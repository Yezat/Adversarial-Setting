{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"../experiments\")\n",
    "\n",
    "\n",
    "from experiment_information import *\n",
    "from experiment_setup import *\n",
    "from data import *\n",
    "from helpers import *\n",
    "from data_loading import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code assumes that you have defined and run an experiment before using `define_experiment.ipynb` in the `experiments` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiments, df_state_evolution, df_erm = obtain_dataframes(logger)\n",
    "df_experiments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_loc = 0\n",
    "\n",
    "# extract and print the top experiment_id\n",
    "experiment_id = df_experiments.iloc[experiment_loc][\"experiment_id\"]\n",
    "print(experiment_id)\n",
    "\n",
    "# extract and print the experiment type\n",
    "experiment_type = df_experiments.iloc[experiment_loc][\"experiment_type\"]\n",
    "print(experiment_type)\n",
    "\n",
    "# extract and print the data model type and data_model name used\n",
    "data_model_types = df_experiments.iloc[experiment_loc][\"data_model_types\"]\n",
    "# convert it to the enum\n",
    "data_model_types = [DataModelType[data_model_type] for data_model_type in json.loads(data_model_types)]\n",
    "data_model_names = [name for name in json.loads(df_experiments.iloc[experiment_loc][\"data_model_names\"])]\n",
    "data_model_descriptions = df_experiments.iloc[experiment_loc][\"data_model_descriptions\"]\n",
    "for data_model_type in data_model_types:\n",
    "    print(data_model_type.name)\n",
    "print(data_model_names)\n",
    "print(data_model_descriptions)\n",
    "\n",
    "# print the experiment name\n",
    "experiment_name = df_experiments.iloc[experiment_loc][\"experiment_name\"]\n",
    "print(experiment_name)\n",
    "\n",
    "# print the experiment problem types\n",
    "experiment_problem_types = df_experiments.iloc[experiment_loc][\"problem_types\"]\n",
    "experiment_problem_types = json.loads(experiment_problem_types)\n",
    "print(experiment_problem_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_name_dict = {}\n",
    "data_model_name_dict[\"VanillaGaussian\"] = \"Vanilla Gaussian\"\n",
    "data_model_name_dict[\"2_VanillaGaussian\"] = \"Vanilla Gaussian\"\n",
    "data_model_name_dict[\"VanillaGaussianThetaFirst\"] = \"Vanilla Gaussian - Teacher 10:1\"\n",
    "data_model_name_dict[\"VanillaGaussianTimes10\"] = \"Vanilla Gaussian x10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[1 1]\"] = \"Strong Weak 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1 1]\"] = \"Strong Weak 5:1\"\n",
    "data_model_name_dict[\"2_KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1 1]\"] = \"Strong Weak 5:1\"\n",
    "data_model_name_dict[\"2_KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[1 1]\"] = \"Strong Weak 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[10  1]\"] = \"Strong Weak 5:1 - Teacher 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[10  1]\"] = \"Strong Weak 10:1 - Teacher 10:1\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[ 1 10]\"] = \"Strong Weak 5:1 - Teacher 1:10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[10  1]_[ 1 10]\"] = \"Strong Weak 10:1 - Teacher 1:10\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[1 1]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Non-Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[5 5]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[5 5]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Non-Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[1 1]_SD_1_1_SU_1_1___RhoNormalisationSweepAllFeatureCombinations\"] = \"Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[1 1]_SD_1_1_SU_1_1\"] = \"Non-Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[5 5]_SD_1_1_SU_1_1\"] = \"Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[5 5]_SD_1_1_SU_1_1\"] = \"Non-Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5 5]_[1 1]_SD_1_1_SU_1_1\"] = \"Robust Non-Useful\"\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[2 2]_[2 2]_SD_1_1_SU_1_1\"] = \"Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[1 1]_[4 4]_SD_1_1_SU_1_1\"] = \"Non-Robust Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[2 2]_[0.5 0.5]_SD_1_1_SU_1_1\"] = \"Robust Non-Useful\"\n",
    "\n",
    "\n",
    "data_model_name_dict['KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[0.5 0.5]_[2 2]_SD_1_1_SU_1_1'] = \"Non-Robust Non-Useful\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[0.5 0.5]_[8 8]_SD_1_1_SU_1_1\"] = \"Non-Robust Useful\"\n",
    "\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[2.  0.5]_[1 1]_SD_1_1_SU_1_1\"] = \"Invariant Defence\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingFirstStronger_AttackingIdentity_[0.5 0.5]_[2.  0.5]_[1 1]_SD_2_1_SU_1_1\"] = \"Protecting Robust\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[2.  0.5]_[1 1]_SD_1_2_SU_1_1\"] = \"Protecting Non-Robust\"\n",
    "\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[1 1]_SD_1_1_SU_1_1\"] = \"Invariant Defence\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingFirstStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[1 1]_SD_2_1_SU_1_1\"] = \"Protecting Robust\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[1 1]_SD_1_2_SU_1_1\"] = \"Protecting Non-Robust\"\n",
    "\n",
    "\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[0.2 5. ]_SD_1_1_SU_1_1\"] = \"Invariant Defence\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingFirstStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[0.2 5. ]_SD_2_1_SU_1_1\"] = \"Protecting Robust\"\n",
    "data_model_name_dict[\"KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[0.2 5. ]_SD_1_2_SU_1_1\"] = \"Protecting Non-Robust\"\n",
    "\n",
    "# 'KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[0.2 5. ]_SD_1_1_SU_1_1___Defence Sweep', 'KFeaturesModel_TwoFeatures_ProtectingFirstStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[0.2 5. ]_SD_2_1_SU_1_1___Defence Sweep', 'KFeaturesModel_TwoFeatures_ProtectingSecondStronger_AttackingIdentity_[0.5 0.5]_[5.  0.2]_[0.2 5. ]_SD_1_2_SU_1_1___Defence Sweep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_evolution = df_state_evolution[df_state_evolution[\"experiment_id\" ] == experiment_id]\n",
    "gd = df_erm[df_erm[\"experiment_id\" ] == experiment_id]\n",
    "# make the column subspace_overlaps to string\n",
    "state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: str(x))\n",
    "gd[\"subspace_overlaps\"] = gd[\"subspace_overlaps\"].apply(lambda x: str(x))\n",
    "\n",
    "# create a json colum\n",
    "state_evolution[\"subspace_overlaps\"] = state_evolution[\"subspace_overlaps\"].apply(lambda x: json.loads(x))\n",
    "gd[\"subspace_overlaps\"] = gd[\"subspace_overlaps\"].apply(lambda x: json.loads(x))\n",
    "from pandas import json_normalize\n",
    "# normalize the json column\n",
    "normalized = json_normalize(state_evolution[\"subspace_overlaps\"])\n",
    "normalized_gd = json_normalize(gd[\"subspace_overlaps\"])\n",
    "\n",
    "\n",
    "\n",
    "def explode_array_column(row, col):\n",
    "    return pd.Series(row[col])\n",
    "\n",
    "# reset the index of the original dataframe\n",
    "state_evolution = state_evolution.reset_index(drop=True)\n",
    "gd = gd.reset_index(drop=True)\n",
    "\n",
    "for col in normalized.columns:\n",
    "    expanded_cols = normalized.apply(lambda x: explode_array_column(x,col), axis=1)\n",
    "    col = col[:-1]\n",
    "    expanded_cols.columns = [col+'_{}'.format(i) for i in range(expanded_cols.shape[1])]\n",
    "    # reset the index of the expanded columns\n",
    "    expanded_cols = expanded_cols.reset_index(drop=True)\n",
    "    state_evolution = pd.concat([state_evolution, expanded_cols], axis=1)\n",
    "\n",
    "for col in normalized_gd.columns:\n",
    "    expanded_cols = normalized_gd.apply(lambda x: explode_array_column(x,col), axis=1)\n",
    "    col = col[:-1]\n",
    "    expanded_cols.columns = [col+'_{}'.format(i) for i in range(expanded_cols.shape[1])]\n",
    "    # reset the index of the expanded columns\n",
    "    expanded_cols = expanded_cols.reset_index(drop=True)\n",
    "\n",
    "    gd = pd.concat([gd, expanded_cols], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_evolution[\"subspace_overlaps_ratio\"] = state_evolution[\"subspace_overlaps_ratio\"].apply(lambda x: str(x))\n",
    "state_evolution[\"subspace_overlaps_ratio\"] = state_evolution[\"subspace_overlaps_ratio\"].apply(lambda x: json.loads(x))\n",
    "normalized = json_normalize(state_evolution[\"subspace_overlaps_ratio\"])\n",
    "# rename the columns of the normalized dataframe\n",
    "for column in normalized.columns:\n",
    "    normalized = normalized.rename(columns={column:column+\"_ratio\"})\n",
    "# merge the normalized dataframe with the original dataframe\n",
    "state_evolution = pd.concat([state_evolution, normalized], axis=1)\n",
    "# drop the original subspace_overlaps column\n",
    "state_evolution = state_evolution.drop(columns=[\"subspace_overlaps_ratio\"])\n",
    "\n",
    "gd[\"subspace_overlaps_ratio\"] = gd[\"subspace_overlaps_ratio\"].apply(lambda x: str(x))\n",
    "gd[\"subspace_overlaps_ratio\"] = gd[\"subspace_overlaps_ratio\"].apply(lambda x: json.loads(x))\n",
    "normalized = json_normalize(gd[\"subspace_overlaps_ratio\"])\n",
    "# rename the columns of the normalized dataframe\n",
    "for column in normalized.columns:\n",
    "    normalized = normalized.rename(columns={column:column+\"_ratio\"})\n",
    "# merge the normalized dataframe with the original dataframe\n",
    "gd = pd.concat([gd, normalized], axis=1)\n",
    "# drop the original subspace_overlaps column\n",
    "gd = gd.drop(columns=[\"subspace_overlaps_ratio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mu_usefulness(row):\n",
    "#     rho = row[\"rho\"]\n",
    "#     tau = row[\"tau\"]\n",
    "\n",
    "#     rho = float(rho)\n",
    "#     tau = float(tau)\n",
    "\n",
    "#     return np.sqrt(2 / np.pi) * rho / np.sqrt( rho + tau**2 )\n",
    "\n",
    "# def compute_gamma_robustness(row):\n",
    "#     rho = row[\"rho\"]\n",
    "#     tau = row[\"tau\"]\n",
    "\n",
    "\n",
    "#     rho = float(rho)\n",
    "#     tau = float(tau)\n",
    "\n",
    "#     return np.sqrt(2 / np.pi) * tau / np.sqrt( rho + tau**2 )\n",
    "\n",
    "# def compute_mu_usefulness_ratio(row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the difference between the adversarial_generalization_error and the generalization_error for both the state evolution and the erm\n",
    "state_evolution[\"difference_adv_gen\"] = state_evolution[\"adversarial_generalization_error\"] - state_evolution[\"generalization_error\"]\n",
    "gd[\"difference_adv_gen\"] = gd[\"adversarial_generalization_error\"] - gd[\"generalization_error_erm\"]\n",
    "\n",
    "\n",
    "state_evolution[\"ratio_adv_gen\"] = state_evolution[\"adversarial_generalization_error\"] / state_evolution[\"generalization_error\"]\n",
    "gd[\"ratio_adv_gen\"] = gd[\"adversarial_generalization_error\"] / gd[\"generalization_error_erm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the noise contribution\n",
    "def noise_contribution(rho: float, tau: float) -> float:\n",
    "    if tau == 0:\n",
    "        tau = 1e-10\n",
    "    return 0.5 - np.arctan( np.sqrt( rho / tau**2 ) ) / np.pi\n",
    "\n",
    "state_evolution[\"noise_contribution\"] = state_evolution.apply(lambda x: noise_contribution(x[\"rho\"], x[\"tau\"]), axis=1)\n",
    "gd[\"noise_contribution\"] = gd.apply(lambda x: noise_contribution(x[\"rho\"], x[\"tau\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the noiseless generalization error without the noise contribution\n",
    "state_evolution[\"noiseless_generalization_error\"] = state_evolution[\"generalization_error\"] - state_evolution[\"noise_contribution\"]\n",
    "gd[\"noiseless_generalization_error_erm\"] = gd[\"generalization_error_erm\"] - gd[\"noise_contribution\"]\n",
    "\n",
    "# create a column for the adversarial noiseless generalization error without the noise contribution\n",
    "state_evolution[\"noiseless_adversarial_generalization_error\"] = state_evolution[\"adversarial_generalization_error\"] - state_evolution[\"noise_contribution\"]\n",
    "gd[\"noiseless_adversarial_generalization_error\"] = gd[\"adversarial_generalization_error\"] - gd[\"noise_contribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseless_angle_to_generalisation(angle):\n",
    "    return np.arccos(angle) / np.pi\n",
    "\n",
    "state_evolution[\"noiseless_angle_to_generalisation\"] = state_evolution.apply(lambda x: noiseless_angle_to_generalisation(x[\"angle\"]), axis=1)\n",
    "gd[\"noiseless_angle_to_generalisation\"] = gd.apply(lambda x: noiseless_angle_to_generalisation(x[\"angle\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for the A/sqrt(q*N) for both the state evolution and the erm\n",
    "state_evolution[\"A_over_sqrt_qN\"] = state_evolution[\"A\"] / np.sqrt(state_evolution[\"q\"] * state_evolution[\"N\"])\n",
    "gd[\"A_over_sqrt_qN\"] = gd[\"A\"] / np.sqrt(gd[\"q\"] * gd[\"N\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for m/sqrt( rho*q - m**2 ) vs A/sqrt(q*N) for both the state evolution and the erm\n",
    "state_evolution[\"m_over_sqrt_rhoq_minus_m2\"] = state_evolution[\"m\"] / np.sqrt(state_evolution[\"rho\"] * state_evolution[\"q\"] - state_evolution[\"m\"]**2)\n",
    "gd[\"m_over_sqrt_rhoq_minus_m2\"] = gd[\"m\"] / np.sqrt(gd[\"rho\"] * gd[\"q\"] - gd[\"m\"]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the data_model_name if there is a sequence ___text at the end\n",
    "def strip_data_model_name(data_model_name):\n",
    "    return re.sub(r\"___.*\",\"\",data_model_name)\n",
    "state_evolution[\"data_model_name\"] = state_evolution[\"data_model_name\"].apply(strip_data_model_name)\n",
    "gd[\"data_model_name\"] = gd[\"data_model_name\"].apply(strip_data_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_theta_ratio(data_model_name):\n",
    "    # a typical data_model_name looks like \"\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1. 1.]_SD_1_1_SU_1_1\"\"\n",
    "    # we want to extract the digits in within the last square brackets\n",
    "    theta_part = data_model_name.split(\"[\")[-1].split(\"]\")[0]\n",
    "    # remove any spaces in the end\n",
    "    theta_part = theta_part.strip()\n",
    "    \n",
    "\n",
    "    # concatenate multiple spaces into one space\n",
    "    theta_part = re.sub(r\"\\s+\",\" \",theta_part)\n",
    "\n",
    "    # determine whether the two values are split by \" \" or by \"  \"\n",
    "    if \"  \" in theta_part:\n",
    "        splitter = \"  \"\n",
    "    else:\n",
    "        splitter = \" \"\n",
    "\n",
    "    theta_first = float(theta_part.split(splitter)[0])\n",
    "    theta_second = float(theta_part.split(splitter)[1])\n",
    "    return theta_second # / theta_first  \n",
    "\n",
    "\n",
    "state_evolution[\"theta_ratio\"] = state_evolution[\"data_model_name\"].apply(extract_theta_ratio)\n",
    "gd[\"theta_ratio\"] = gd[\"data_model_name\"].apply(extract_theta_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sigmax_ratio(data_model_name):\n",
    "    # a typical data_model_name looks like \"\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1. 1.]_SD_1_1_SU_1_1\"\"\n",
    "    # we want to extract the digits in within the last square brackets\n",
    "    theta_part = data_model_name.split(\"[\")[-2].split(\"]\")[0]\n",
    "    # remove any spaces in the end\n",
    "    theta_part = theta_part.strip()\n",
    "\n",
    "    # concatenate multiple spaces into one space\n",
    "    theta_part = re.sub(r\"\\s+\",\" \",theta_part)\n",
    "\n",
    "    # determine whether the two values are split by \" \" or by \"  \"\n",
    "    if \"  \" in theta_part:\n",
    "        splitter = \"  \"\n",
    "    else:\n",
    "        splitter = \" \"\n",
    "\n",
    "    theta_first = float(theta_part.split(splitter)[0])\n",
    "    theta_second = float(theta_part.split(splitter)[1])\n",
    "    return theta_first # / theta_second\n",
    "\n",
    "\n",
    "state_evolution[\"sigmax_ratio\"] = state_evolution[\"data_model_name\"].apply(extract_sigmax_ratio)\n",
    "gd[\"sigmax_ratio\"] = gd[\"data_model_name\"].apply(extract_sigmax_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sigmadelta_ratio(data_model_name):\n",
    "    # a typical data_model_name looks like \"\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1. 1.]_SD_1_1_SU_1_1\"\"\n",
    "    # we want to extract the digits after the SD_ only\n",
    "    theta_part = data_model_name.split(\"SD\")[-1].split(\"SU\")[0]\n",
    "    # print(\"Post SD: \", theta_part)\n",
    "\n",
    "\n",
    "    # remove leading _ and ending _\n",
    "    theta_part = theta_part.strip(\"_\")\n",
    "    # print(\"Post strip: \", theta_part)\n",
    "\n",
    "    # concatenate multiple spaces into one space\n",
    "    theta_part = re.sub(r\"\\s+\",\" \",theta_part)\n",
    "\n",
    "    # define the spliter to be \"_\"\n",
    "    splitter = \"_\"\n",
    "\n",
    "    theta_first = float(theta_part.split(splitter)[0])\n",
    "    theta_second = float(theta_part.split(splitter)[1])\n",
    "    # print(\"Theta first: \", theta_first)\n",
    "    # print(\"Theta second: \", theta_second)\n",
    "    return theta_second  #/ theta_first\n",
    "\n",
    "def extract_sigmadelta_scale(data_model_name):\n",
    "    # a typical data_model_name looks like \"\"KFeaturesModel_TwoFeatures_ProtectingIdentity_AttackingIdentity_[0.5 0.5]_[5 1]_[1. 1.]_SD_1_1_SU_1_1\"\"\n",
    "    # we want to extract the digits after the SD_ only\n",
    "    theta_part = data_model_name.split(\"SD\")[-1].split(\"SU\")[0]\n",
    "    # print(\"Post SD: \", theta_part)\n",
    "\n",
    "\n",
    "    # remove leading _ and ending _\n",
    "    theta_part = theta_part.strip(\"_\")\n",
    "    # print(\"Post strip: \", theta_part)\n",
    "\n",
    "    # concatenate multiple spaces into one space\n",
    "    theta_part = re.sub(r\"\\s+\",\" \",theta_part)\n",
    "\n",
    "    # define the spliter to be \"_\"\n",
    "    splitter = \"_\"\n",
    "\n",
    "    theta_first = float(theta_part.split(splitter)[0])\n",
    "    theta_second = float(theta_part.split(splitter)[1])\n",
    "    # print(\"Theta first: \", theta_first)\n",
    "    # print(\"Theta second: \", theta_second)\n",
    "    return theta_first\n",
    "\n",
    "\n",
    "state_evolution[\"sigmadelta_ratio\"] = state_evolution[\"data_model_name\"].apply(extract_sigmadelta_ratio)\n",
    "gd[\"sigmadelta_ratio\"] = gd[\"data_model_name\"].apply(extract_sigmadelta_ratio)\n",
    "\n",
    "state_evolution[\"sigmadelta_scale\"] = state_evolution[\"data_model_name\"].apply(extract_sigmadelta_scale)\n",
    "gd[\"sigmadelta_scale\"] = gd[\"data_model_name\"].apply(extract_sigmadelta_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = gd.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"sigmax_ratio\",\"sigmadelta_ratio\",\"theta_ratio\",\"data_model_name\"]) #,\"p_calibration\"\n",
    "state_evolution = state_evolution.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"sigmax_ratio\",\"sigmadelta_ratio\",\"theta_ratio\",\"data_model_name\"]) #,\"p_calibration\"\n",
    "\n",
    "# drop id, code_version, experiment_id, date, initial_condition, test_against_epsilons, calibrations, abs_tol, min_iter,max_iter,blend_fpe,int_lims,subspace_overlaps\n",
    "state_evolution = state_evolution.drop(columns=[\"id\",\"code_version\",\"experiment_id\",\"date\",\"initial_condition\",\"test_against_epsilons\",\"calibrations\",\"abs_tol\",\"min_iter\",\"max_iter\",\"blend_fpe\",\"int_lims\",\"subspace_overlaps\",\"data_model_type\",\"data_model_description\"])\n",
    "# drop id, code_version, experiment_id, test_against_epsilons, date, subspace_overlaps, analytical_calibrations, erm_calibrations, \n",
    "gd = gd.drop(columns=[\"id\",\"code_version\",\"experiment_id\",\"test_against_epsilons\",\"date\",\"subspace_overlaps\",\"analytical_calibrations\",\"erm_calibrations\",\"data_model_type\",\"data_model_description\"])\n",
    "\n",
    "state_evolution.columns = [col+\"_state_evolution\" for col in state_evolution.columns]\n",
    "gd.columns = [col+\"_erm\" for col in gd.columns]\n",
    "\n",
    "state_evolution = state_evolution.groupby(level=[0,1,2,3,4,5,6,7,8,9]).agg([\"mean\",\"std\"]) #,4\n",
    "gd = gd.groupby(level=[0,1,2,3,4,5,6,7,8,9]).agg([\"mean\",\"std\"]) #,4\n",
    "df = state_evolution.join(gd, how=\"outer\")\n",
    "df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rho_norm_usefulness(row):\n",
    "    # extract sigmax_ratio from the index\n",
    "    sigmax_ratio = row[\"sigmax_ratio\"]\n",
    "    sigmatheta_ratio = row[\"theta_ratio\"]\n",
    "\n",
    "    sigmax_ratio = float(sigmax_ratio)\n",
    "    sigmatheta_ratio = float(sigmatheta_ratio)\n",
    "\n",
    "    norm = (sigmax_ratio + sigmatheta_ratio**2) * 0.5\n",
    "\n",
    "    subspace_rho_1 = sigmax_ratio / (2 * norm)\n",
    "\n",
    "    usefulness = subspace_rho_1 / ( 1 - subspace_rho_1 )\n",
    "\n",
    "    subspace_rho_2 = sigmatheta_ratio**2 / (2*norm)\n",
    "\n",
    "    assert subspace_rho_1 + subspace_rho_2 - 1 < 1e-5, \"Subspace rho 1 and 2 should sum to 1 but they are {} and {}\".format(subspace_rho_1, subspace_rho_2)\n",
    "\n",
    "\n",
    "    return usefulness\n",
    "\n",
    "def extract_trace_norm_usefulness(row):\n",
    "    sigmax_ratio = row[\"sigmax_ratio\"]\n",
    "    sigmatheta_ratio = row[\"theta_ratio\"]\n",
    "    rho = row[\"rho_state_evolution\"][\"mean\"]\n",
    "\n",
    "    sigmax_ratio = float(sigmax_ratio)\n",
    "    sigmatheta_ratio = float(sigmatheta_ratio)\n",
    "    rho = float(rho)\n",
    "\n",
    "    norm = (1/4) * (1 + sigmax_ratio) * ( 1 + sigmatheta_ratio**2)\n",
    "\n",
    "    subspace_1 = 0.5 * sigmax_ratio / norm\n",
    "    subspace_2 = 0.5 * sigmatheta_ratio**2 / norm\n",
    "\n",
    "    # assert subspace_1 + subspace_2 - rho < 1e-5, \"Subspace rho 1 and 2 should sum to 1 but they are {} and {}\".format(subspace_1, subspace_2)\n",
    "\n",
    "    usefulness = subspace_1 / ( rho - subspace_1 )\n",
    "\n",
    "    return usefulness\n",
    "\n",
    "def extract_relative_usefulness(row):\n",
    "    sigmax_ratio = row[\"sigmax_ratio\"]\n",
    "    sigmatheta_ratio = row[\"theta_ratio\"]\n",
    "\n",
    "    sigmax_ratio = float(sigmax_ratio)\n",
    "    sigmatheta_ratio = float(sigmatheta_ratio)\n",
    "\n",
    "    return sigmax_ratio / sigmatheta_ratio**2\n",
    "\n",
    "def extract_usefulness(row):\n",
    "    rho = row[\"rho_state_evolution\"][\"mean\"]\n",
    "    rho = float(rho)\n",
    "    return rho\n",
    "\n",
    "\n",
    "df2 = df.reset_index()\n",
    "\n",
    "\n",
    "df2[\"total_usefulness\"] = df2.apply(extract_usefulness, axis=1)\n",
    "\n",
    "# # apply mean and std to the new column\n",
    "# df_result = df2[\"total_usefulness\"].apply(lambda x: pd.Series({\"mean\":x, \"std\":0}))\n",
    "# multiindex = pd.MultiIndex.from_product([['total_usefulness'], ['mean', 'std']], names=['', ''])\n",
    "# df_result.columns = multiindex\n",
    "# # Concatenate the original DataFrame with the computed values DataFrame\n",
    "# df2 = pd.concat([df2, df_result], axis=1)\n",
    "\n",
    "\n",
    "df2[\"rho_usefulness\"] = df2.apply(extract_rho_norm_usefulness, axis=1)\n",
    "\n",
    "\n",
    "# apply mean and std to the new column\n",
    "df_result = df2[\"rho_usefulness\"].apply(lambda x: pd.Series({\"mean\":x, \"std\":0}))\n",
    "multiindex = pd.MultiIndex.from_product([['rho_usefulness'], ['mean', 'std']], names=['', ''])\n",
    "df_result.columns = multiindex\n",
    "# Concatenate the original DataFrame with the computed values DataFrame\n",
    "df2 = pd.concat([df2, df_result], axis=1)\n",
    "\n",
    "df2[\"trace_usefulness\"] = df2.apply(extract_trace_norm_usefulness, axis=1)\n",
    "\n",
    "# apply mean and std to the new column\n",
    "df_result = df2[\"trace_usefulness\"].apply(lambda x: pd.Series({\"mean\":x, \"std\":0}))\n",
    "multiindex = pd.MultiIndex.from_product([['trace_usefulness'], ['mean', 'std']], names=['', ''])\n",
    "df_result.columns = multiindex\n",
    "# Concatenate the original DataFrame with the computed values DataFrame\n",
    "df2 = pd.concat([df2, df_result], axis=1)\n",
    "\n",
    "df2[\"relative_usefulness\"] = df2.apply(extract_relative_usefulness, axis=1)\n",
    "\n",
    "# apply mean and std to the new column\n",
    "# df_result = df2[\"relative_usefulness\"].apply(lambda x: pd.Series({\"mean\":x, \"std\":0}))\n",
    "# multiindex = pd.MultiIndex.from_product([['relative_usefulness'], ['mean', 'std']], names=['', ''])\n",
    "# df_result.columns = multiindex\n",
    "# Concatenate the original DataFrame with the computed values DataFrame\n",
    "df2 = pd.concat([df2, df_result], axis=1)\n",
    "\n",
    "# # apply mean and std to the column \"theta_ratio\"\n",
    "# df_result = df2[\"theta_ratio\"].apply(lambda x: pd.Series({\"mean\":x, \"std\":0}))\n",
    "# multiindex = pd.MultiIndex.from_product([['theta_ratio'], ['mean', 'std']], names=['', ''])\n",
    "# df_result.columns = multiindex\n",
    "# # Concatenate the original DataFrame with the computed values DataFrame\n",
    "# df2 = pd.concat([df2, df_result], axis=1)\n",
    "\n",
    "# df_result = df2[\"sigmax_ratio\"].apply(lambda x: pd.Series({\"mean\":x, \"std\":0}))\n",
    "# multiindex = pd.MultiIndex.from_product([['sigmax_ratio'], ['mean', 'std']], names=['', ''])\n",
    "# df_result.columns = multiindex\n",
    "# # Concatenate the original DataFrame with the computed values DataFrame\n",
    "# df2 = pd.concat([df2, df_result], axis=1)\n",
    "\n",
    "\n",
    "# set all quantities from the df index in df2 as index\n",
    "# df = df2.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"sigmax_ratio\",\"sigmadelta_ratio\",\"relative_usefulness\",\"data_model_name\"]) #,\"p_calibration\"\n",
    "# df = df2.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"sigmax_ratio\",\"sigmadelta_ratio\",\"total_usefulness\",\"data_model_name\"]) \n",
    "df = df2.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"theta_ratio\",\"sigmax_ratio\",\"sigmadelta_ratio\",\"total_usefulness\",\"data_model_name\"]) #,\"p_calibration\"\n",
    "\n",
    "\n",
    "# sort index\n",
    "df.sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try replacing data_model_name in df with data_model_name_dict\n",
    "df = df.reset_index()\n",
    "df[\"data_model_name\"] = df[\"data_model_name\"].apply(lambda x: data_model_name_dict[x])\n",
    "df = df.set_index([\"alpha\",\"epsilon\",\"tau\",\"lam\",\"problem_type\",\"attack_epsilon\",\"theta_ratio\",\"sigmax_ratio\",\"sigmadelta_ratio\",\"total_usefulness\",\"data_model_name\"]) #,\"p_calibration\"\n",
    "df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lambdas = load_csv_to_object_dictionary(OptimalLambdaResult(0,0,0,0,None,None,None),path=\"experiments/\")\n",
    "def extract_optimal_lambda(alpha, epsilon, tau, data_model_type, data_model_name, problem_type):\n",
    "    # extract or compute optimal lamba and plot it as a vertical line\n",
    "    optimal_result = OptimalLambdaResult(alpha,epsilon,tau,1,data_model_type,data_model_name, problem_type)\n",
    "    if optimal_result.get_key() in optimal_lambdas:\n",
    "        optimal_lambda = optimal_lambdas[optimal_result.get_key()]\n",
    "        return optimal_lambda\n",
    "    else:   \n",
    "        print(\"Optimal lambda not found for \", optimal_result.get_key())\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique data_model_name from index\n",
    "data_model_names = df.index.get_level_values(\"data_model_name\").unique()\n",
    "data_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataframe as a pickle file\n",
    "if not os.path.exists(\"Pickles\"):\n",
    "    os.makedirs(\"Pickles\")\n",
    "if not os.path.exists(\"Pickles/defence_sweep.pkl\"):\n",
    "    df.to_pickle(\"Pickles/defence_sweep.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle\n",
    "# df = pd.read_pickle(\"Pickles/defence_sweep.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique epsilons\n",
    "epsilons = df.index.get_level_values(\"epsilon\").unique()\n",
    "epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the data_model_name \"Invariant defence\" to \"Uniform defence\"\n",
    "df = df.rename(index={\"Invariant Defence\":\"Uniform Defence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_names = df.index.get_level_values(\"data_model_name\").unique()\n",
    "data_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of dataframes for each data_model_name\n",
    "df_dict = {}\n",
    "for data_model_name in data_model_names:\n",
    "\n",
    "\n",
    "    df_data_model = df.xs(data_model_name, level=\"data_model_name\")\n",
    "\n",
    "    # for each data_model_name, create a dictionary\n",
    "    eps_dict = {}\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "\n",
    "\n",
    "        eps_df = df_data_model.xs(epsilon, level=\"epsilon\")\n",
    "\n",
    "        alphas = eps_df.index.get_level_values(\"alpha\").unique()\n",
    "        adversarial_error_0 = eps_df[\"adversarial_generalization_error_state_evolution\"][\"mean\"].values\n",
    "        generalization_error_0 = eps_df[\"generalization_error_state_evolution\"][\"mean\"].values\n",
    "        boundary_error_0 = eps_df[\"difference_adv_gen_state_evolution\"][\"mean\"].values\n",
    "        class_preserving = eps_df[\"fair_adversarial_error_state_evolution\"][\"mean\"].values\n",
    "\n",
    "        adversarial_error_erm = eps_df[\"adversarial_generalization_error_erm\"][\"mean\"].values\n",
    "        generalization_error_erm = eps_df[\"generalization_error_erm_erm\"][\"mean\"].values\n",
    "        boundary_error_erm = eps_df[\"difference_adv_gen_erm\"][\"mean\"].values\n",
    "        class_preserving_erm = eps_df[\"fair_adversarial_error_erm\"][\"mean\"].values\n",
    "\n",
    "        adversarial_error_erm_std = eps_df[\"adversarial_generalization_error_erm\"][\"std\"].values\n",
    "        generalization_error_erm_std = eps_df[\"generalization_error_erm_erm\"][\"std\"].values\n",
    "        boundary_error_erm_std = eps_df[\"difference_adv_gen_erm\"][\"std\"].values\n",
    "        class_preserving_erm_std = eps_df[\"fair_adversarial_error_erm\"][\"std\"].values\n",
    "\n",
    "        alphas = np.array(alphas)\n",
    "        adversarial_error_0 = np.array(adversarial_error_0)\n",
    "        generalization_error_0 = np.array(generalization_error_0)\n",
    "        boundary_error_0 = np.array(boundary_error_0)\n",
    "        class_preserving = np.array(class_preserving)\n",
    "\n",
    "        adversarial_error_erm = np.array(adversarial_error_erm)\n",
    "        generalization_error_erm = np.array(generalization_error_erm)\n",
    "        boundary_error_erm = np.array(boundary_error_erm)\n",
    "        class_preserving_erm = np.array(class_preserving_erm)\n",
    "\n",
    "        adversarial_error_erm_std = np.array(adversarial_error_erm_std)\n",
    "        generalization_error_erm_std = np.array(generalization_error_erm_std)\n",
    "        boundary_error_erm_std = np.array(boundary_error_erm_std)\n",
    "        class_preserving_erm_std = np.array(class_preserving_erm_std)\n",
    "\n",
    "        eps_0_dict = {}\n",
    "        eps_0_dict[\"alphas\"] = alphas\n",
    "        eps_0_dict[\"adversarial_error\"] = adversarial_error_0\n",
    "        eps_0_dict[\"generalization_error\"] = generalization_error_0\n",
    "        eps_0_dict[\"boundary_error\"] = boundary_error_0\n",
    "        eps_0_dict[\"class_preserving\"] = class_preserving\n",
    "\n",
    "        eps_0_dict[\"adversarial_error_erm\"] = adversarial_error_erm\n",
    "        eps_0_dict[\"generalization_error_erm\"] = generalization_error_erm\n",
    "        eps_0_dict[\"boundary_error_erm\"] = boundary_error_erm\n",
    "        eps_0_dict[\"class_preserving_erm\"] = class_preserving_erm\n",
    "\n",
    "        eps_0_dict[\"adversarial_error_erm_std\"] = adversarial_error_erm_std\n",
    "        eps_0_dict[\"generalization_error_erm_std\"] = generalization_error_erm_std\n",
    "        eps_0_dict[\"boundary_error_erm_std\"] = boundary_error_erm_std\n",
    "        eps_0_dict[\"class_preserving_erm_std\"] = class_preserving_erm_std\n",
    "\n",
    "\n",
    "        eps_dict[epsilon] = eps_0_dict\n",
    "\n",
    "\n",
    "    df_dict[data_model_name] = eps_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_DIRECTORY = \"./Assets/defence_sweep\"\n",
    "if not os.path.exists(IMG_DIRECTORY):\n",
    "    os.makedirs(IMG_DIRECTORY)\n",
    "\n",
    "\n",
    "def save_plot(fig, name, formats=[\"pdf\",\"jpg\"], date=False):\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for f in formats:        \n",
    "        path = \"{}\".format(name) + \"_{}\".format(current_date) + \".\" + f\n",
    "        if not date:\n",
    "            path = \"{}\".format(name) + \".\" + f\n",
    "        fig.savefig(            \n",
    "            os.path.join(IMG_DIRECTORY, path),\n",
    "            format=f,\n",
    "        )\n",
    "\n",
    "\n",
    "def set_size(width, fraction=1, subplots=(1, 1)):\n",
    "    if width == \"thesis\":\n",
    "        width_pt = 426.79135\n",
    "    elif width == \"beamer\":\n",
    "        width_pt = 307.28987\n",
    "    else:\n",
    "        width_pt = width\n",
    "\n",
    "    fig_width_pt = width_pt * fraction\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    golden_ratio = (5**0.5 - 1) / 2\n",
    "\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    fig_height_in = fig_width_in * (golden_ratio) * (subplots[0] / subplots[1])\n",
    "\n",
    "    return (fig_width_in, fig_height_in)\n",
    "\n",
    "\n",
    "# width = 2.5 * 458.63788\n",
    "multiplier = 1.25\n",
    "width = multiplier * 1.5 * 234.8775\n",
    "\n",
    "plt.style.use(\"../latex_ready.mplstyle\")\n",
    "\n",
    "tuple_size = set_size(width, fraction=1.0, subplots=(1, 2))\n",
    "tuple_size = (3*8.5/4,2.4)\n",
    "\n",
    "multiplier = 1.25\n",
    "second_multiplier = 0.6\n",
    "\n",
    "\n",
    "# import Line2D for custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=3,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    figsize=( tuple_size[0], tuple_size[1]),\n",
    "    gridspec_kw={\"hspace\": 0,\"wspace\": 0},\n",
    ")\n",
    "\n",
    "\n",
    "# ICML adjustments\n",
    "fig.subplots_adjust(left=0.08)\n",
    "fig.subplots_adjust(bottom=0.12)\n",
    "fig.subplots_adjust(top=0.99)\n",
    "fig.subplots_adjust(right=0.97)\n",
    "\n",
    "print(\"Figure size in inches: \", fig.get_size_inches())\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create a custom legend\n",
    "custom_legend = []\n",
    "\n",
    "\n",
    "for df_idx, (key, value) in enumerate(df_dict.items()):\n",
    "\n",
    "    data_model_name = key\n",
    "\n",
    "    axs = axes[df_idx]\n",
    "    \n",
    "\n",
    "    linestyles = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "\n",
    "    for idx, (epsilon, eps_dict) in enumerate(value.items()):\n",
    "\n",
    "        ax = axes[idx]\n",
    "\n",
    "        alphas = eps_dict[\"alphas\"]\n",
    "        adversarial_error = eps_dict[\"adversarial_error\"]\n",
    "        generalization_error = eps_dict[\"generalization_error\"]\n",
    "        boundary_error = eps_dict[\"boundary_error\"]\n",
    "        class_preserving = eps_dict[\"class_preserving\"]\n",
    "\n",
    "        adversarial_error_erm = eps_dict[\"adversarial_error_erm\"]\n",
    "        generalization_error_erm = eps_dict[\"generalization_error_erm\"]\n",
    "        boundary_error_erm = eps_dict[\"boundary_error_erm\"]\n",
    "        class_preserving_erm = eps_dict[\"class_preserving_erm\"]\n",
    "\n",
    "        adversarial_error_erm_std = eps_dict[\"adversarial_error_erm_std\"]\n",
    "        generalization_error_erm_std = eps_dict[\"generalization_error_erm_std\"]\n",
    "        boundary_error_erm_std = eps_dict[\"boundary_error_erm_std\"]\n",
    "        class_preserving_erm_std = eps_dict[\"class_preserving_erm_std\"]\n",
    "\n",
    "        if df_idx == 0:\n",
    "\n",
    "            custom_legend.append(Line2D([0],[0],color=\"C0\", linestyle=linestyles[idx], label=r\"$E_{{\\mathrm{{adv}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "            custom_legend.append(Line2D([0],[0],color=\"C1\", linestyle=linestyles[idx], label=r\"$E_{{\\mathrm{{gen}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "            custom_legend.append(Line2D([0],[0],color=\"C2\", linestyle=linestyles[idx], label=r\"$E_{{\\mathrm{{bound}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "            custom_legend.append(Line2D([0],[0],color=\"C3\", linestyle=linestyles[idx], label=r\"$E_{{\\mathrm{{CP}}}}, \\varepsilon_t={}$\".format(epsilon)))\n",
    "        \n",
    "\n",
    "        adversarial_lines = axs.plot(alphas, adversarial_error, linestyle=linestyles[idx],color=\"C0\")\n",
    "        axs.plot(alphas, generalization_error, linestyle=linestyles[idx],color=\"C1\")\n",
    "        axs.plot(alphas, boundary_error,linestyle=linestyles[idx], color=\"C2\")\n",
    "        # axs.plot(alphas, class_preserving,linestyle=linestyles[idx], color=\"C3\")\n",
    "\n",
    "        axs.errorbar(\n",
    "            alphas,\n",
    "            adversarial_error_erm,\n",
    "            yerr=adversarial_error_erm_std,\n",
    "            fmt=\".\",\n",
    "            markersize=1,\n",
    "            color=\"C0\"\n",
    "        )\n",
    "        axs.errorbar(\n",
    "            alphas,\n",
    "            generalization_error_erm,\n",
    "            yerr=generalization_error_erm_std,\n",
    "            fmt=\".\",\n",
    "            markersize=1,\n",
    "            color=\"C1\"\n",
    "        )\n",
    "        axs.errorbar(\n",
    "            alphas,\n",
    "            boundary_error_erm,\n",
    "            yerr=boundary_error_erm_std,\n",
    "            fmt=\".\",\n",
    "            markersize=1,\n",
    "            color=\"C2\"\n",
    "        )\n",
    "        axs.errorbar(\n",
    "            alphas,\n",
    "            class_preserving_erm,\n",
    "            yerr=class_preserving_erm_std,\n",
    "            fmt=\".\",\n",
    "            markersize=1,\n",
    "            color=\"C3\"\n",
    "        )\n",
    "\n",
    "\n",
    "    axs.set_xscale(\"log\")\n",
    "    axs.set_yscale(\"log\")\n",
    "\n",
    "\n",
    "    if df_idx == 0:\n",
    "        axs.set_ylabel(r\"$E$\", labelpad=2.0)\n",
    "    # axs.set_ylabel(r\"$E$\", labelpad=2.0)\n",
    "\n",
    "\n",
    "    axs.set_xlabel(r\"$\\alpha$\", labelpad=2.0)\n",
    "    axs.grid(which=\"both\", axis=\"both\", alpha=0.5)\n",
    "    axs.legend(title=data_model_name, loc=\"lower left\")\n",
    "    # Set the major ticks to face inwards\n",
    "    axs.tick_params(axis='both', which='major', direction='in')\n",
    "\n",
    "    # Set the minor ticks to face inwards\n",
    "    axs.tick_params(axis='both', which='minor', direction='in')\n",
    "\n",
    "\n",
    "error_legend = []\n",
    "\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{adv}}$\",color=\"C0\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{gen}}$\",color=\"C1\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{bound}}$\",color=\"C2\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{CP}}$\",color=\"C3\"))\n",
    "\n",
    "epsilon_legend = []\n",
    "\n",
    "for idx, epsilon in enumerate(epsilons):\n",
    "    epsilon_legend.append(Line2D([0],[0],color=\"black\", linestyle=linestyles[idx], label=r\"$\\varepsilon_t={}$\".format(epsilons[idx]))) \n",
    "\n",
    "custom_legend = []\n",
    "\n",
    "for idx in range(3):\n",
    "    custom_legend.append(error_legend[idx])\n",
    "    custom_legend.append(epsilon_legend[idx])\n",
    "\n",
    "custom_legend.append(error_legend[-1])\n",
    "\n",
    "# Place the legend at the bottom of the figure\n",
    "# fig.legend(handles=custom_legend, loc='upper center', ncol=4)\n",
    "\n",
    "save = True\n",
    "if save:\n",
    "    save_plot(\n",
    "        fig,\n",
    "        f\"defence_sweep\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the legend separately, put the custom legend into a figure\n",
    "figlegend = plt.figure(figsize=( tuple_size[0], 0.1 * tuple_size[0]))\n",
    "plt.style.use(\"../latex_ready.mplstyle\")\n",
    "legend_ax = figlegend.add_axes([0, 0, 1, 1])\n",
    "legend_ax.axis('off')  # Turn off the axes for the legend figure\n",
    "\n",
    "figlegend.legend(\n",
    "    handles=custom_legend,\n",
    "    handlelength=1,\n",
    "    loc=\"center\",\n",
    "    ncol=4,\n",
    ")\n",
    "\n",
    "figlegend.savefig(\n",
    "    os.path.join(IMG_DIRECTORY, \"legend.pdf\"),\n",
    "    format=\"pdf\",\n",
    "    # bbox_inches=\"tight\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the legend separately, put the custom legend into a figure\n",
    "figlegend = plt.figure(figsize=( 0.1*tuple_size[0], 0.5*tuple_size[1]))\n",
    "plt.style.use(\"../latex_ready.mplstyle\")\n",
    "legend_ax = figlegend.add_axes([0, 0, 1, 1])\n",
    "legend_ax.axis('off')  # Turn off the axes for the legend figure\n",
    "\n",
    "# display figure size\n",
    "print(\"Figure size in inches: \", figlegend.get_size_inches())\n",
    "\n",
    "error_legend = []\n",
    "\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{adv}}$\",color=\"C0\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{gen}}$\",color=\"C1\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{bound}}$\",color=\"C2\"))\n",
    "error_legend.append(Line2D([0],[0],label=r\"$E_{\\mathrm{CP}}$\",color=\"C3\"))\n",
    "\n",
    "epsilon_legend = []\n",
    "\n",
    "for idx, epsilon in enumerate(epsilons):\n",
    "    epsilon_legend.append(Line2D([0],[0],color=\"black\", linestyle=linestyles[idx], label=r\"$\\varepsilon_t={}$\".format(epsilons[idx]))) \n",
    "\n",
    "\n",
    "# merge the two legends by concatenating the lists\n",
    "custom_legend = error_legend + epsilon_legend\n",
    "\n",
    "\n",
    "figlegend.legend(\n",
    "    handles=custom_legend,\n",
    "    handlelength=1,\n",
    "    loc=\"center\",\n",
    "    ncol=1,\n",
    ")\n",
    "\n",
    "figlegend.savefig(\n",
    "    os.path.join(IMG_DIRECTORY, \"vertical_legend.pdf\"),\n",
    "    format=\"pdf\",\n",
    "    # bbox_inches=\"tight\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
